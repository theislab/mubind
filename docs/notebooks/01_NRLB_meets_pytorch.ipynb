{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NRLB with Pytorch\n",
    "https://www.pnas.org/content/115/16/E3692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('here...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as tdata\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as tfunc\n",
    "import torch.optim as topti\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use a GPU if available, as it should be faster.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \" + str(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class for reading training/testing dataset files.\n",
    "class toyDataset(tdata.Dataset):\n",
    "    def __init__(self, dataFile, labelFile):\n",
    "        # Load data from files.\n",
    "        self.inputs = np.loadtxt(dataFile, dtype = np.float32).reshape(-1, 4, 1000)\n",
    "        self.labels = np.loadtxt(labelFile, dtype = np.float32)\n",
    "\n",
    "        self.length = len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return a single input/label pair from the dataset.\n",
    "        inputSample = self.inputs[index]\n",
    "        labelSample = self.labels[index]\n",
    "        sample = {\"input\": inputSample, \"label\": labelSample}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the functions to generate one hot encoded dna sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "class hot_dna:\n",
    "    def __init__(self,fasta):\n",
    "   \n",
    "        #check for and grab sequence name\n",
    "        if re.search(\">\",fasta):\n",
    "            name = re.split(\"\\n\",fasta)[0]\n",
    "            sequence = re.split(\"\\n\",fasta)[1]\n",
    "        else:\n",
    "            name = 'unknown_sequence'\n",
    "            sequence = fasta\n",
    "\n",
    "        #get sequence into an array\n",
    "        seq_array = array(list(sequence))\n",
    "\n",
    "        #integer encode the sequence\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded_seq = label_encoder.fit_transform(seq_array)\n",
    "\n",
    "        #one hot the sequence\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        #reshape because that's what OneHotEncoder likes\n",
    "        integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
    "        onehot_encoded_seq = onehot_encoder.fit_transform(integer_encoded_seq)\n",
    "\n",
    "        #add the attributes to self \n",
    "        self.name = name\n",
    "        self.sequence = fasta\n",
    "        self.integer = integer_encoded_seq\n",
    "        self.onehot = onehot_encoded_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity between DNA sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "import itertools\n",
    "\n",
    "def mismatch(word, letters, num_mismatches):\n",
    "    for locs in itertools.combinations(range(len(word)), num_mismatches):\n",
    "        this_word = [[char] for char in word]\n",
    "        for loc in locs:\n",
    "            orig_char = word[loc]\n",
    "            this_word[loc] = [l for l in letters if l != orig_char]\n",
    "        for poss in itertools.product(*this_word):\n",
    "            yield ''.join(poss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motif generation function. Not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# given a motif generate a seq of sequences with this seuquence embedded\n",
    "def get_x_and_y(motif, batch=100):\n",
    "    import random\n",
    "    random.seed(500)\n",
    "\n",
    "    import numpy as np\n",
    "    nseqs = 500\n",
    "    length = 15\n",
    "    fg_seqs = np.array([scr.gen.get_random_sequence(length) + 'ACGT' for k in range(nseqs)])\n",
    "    # bg_seqs = np.array([scr.gen.get_random_sequence(length) for k in range(nseqs)])\n",
    "    # bg_seqs = [scr.gen.randomize_sequence(fg_seqs[i]) + 'ACGT' for i in range(len(fg_seqs))]\n",
    "\n",
    "    options = []\n",
    "    for n_mismatches in range(0, min(len(motif), 5)):\n",
    "        next_options = np.random.choice(list(mismatch(motif, 'ACGT', n_mismatches)), 100)[:100]\n",
    "        print(n_mismatches, len(next_options), next_options[:10])\n",
    "        options += list(next_options)\n",
    "    y = []\n",
    "    for i, opt in zip(range(len(fg_seqs)), options):\n",
    "        p = np.random.choice(range(len(fg_seqs[0]) - 4 - len(opt)))\n",
    "        fg_seqs[i] = fg_seqs[i][:p] + opt + fg_seqs[i][p + len(opt):]\n",
    "        y.append(int(similar(motif, opt) * batch))\n",
    "    y_pos = np.array(y)\n",
    "    fg_seqs = fg_seqs[:len(y_pos)]\n",
    "    print(len(fg_seqs))\n",
    "    return fg_seqs, y_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read HT-SELEX data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrlb_data = 'https://www.dropbox.com/s/oib5lq23wck3gsh/GATA3_TGTCGT20NGA_AC_4.tsv.gz'\n",
    "!wget $nrlb_data -O GATA3_TGTCGT20NGA_AC_4.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "def get_seqs(fastq_path, round_key):\n",
    "    fastq = np.array([(s.strip()).decode('utf-8') for s in gzip.open(fastq_path)])\n",
    "    mask = np.array([((i + 3) % 4 == 0) for i in range(len(fastq))])\n",
    "    seqs = fastq[mask]\n",
    "    df = pd.DataFrame(seqs, columns=['seq'])\n",
    "    df = df[~df['seq'].str.contains('N')]\n",
    "    seq_counts = df['seq'].value_counts()\n",
    "    df['counts'] = [seq_counts.loc[s] for s in df['seq']]\n",
    "    df['round'] = round_key\n",
    "    df = df.drop_duplicates('seq').sort_values('counts', ascending=False)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/countTable.0.CTCF_r3.tsv.gz: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# download CTCF from motif central\n",
    "!wget http://pbdemo.x3dna.org/files/example_data/singleTF/countTable.0.CTCF_r3.tsv.gz -O data/countTable.0.CTCF_r3.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/countTable.0.CTCF_r3.tsv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mdata/countTable.0.CTCF_r3.tsv.gz\u001b[39;49m\u001b[39m'\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m) \u001b[39m# ['sequence', 'round.0', 'round.1']) #  header=False)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mseq\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/multibind/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/multibind/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/multibind/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/mambaforge/envs/multibind/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/mambaforge/envs/multibind/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/mambaforge/envs/multibind/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/mambaforge/envs/multibind/lib/python3.10/site-packages/pandas/io/common.py:750\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[39mif\u001b[39;00m compression \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    747\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    748\u001b[0m         \u001b[39m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[1;32m    749\u001b[0m         \u001b[39m# \"GzipFile\", variable has type \"Union[str, BaseBuffer]\")\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m         handle \u001b[39m=\u001b[39m gzip\u001b[39m.\u001b[39;49mGzipFile(  \u001b[39m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[1;32m    751\u001b[0m             filename\u001b[39m=\u001b[39;49mhandle,\n\u001b[1;32m    752\u001b[0m             mode\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    753\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcompression_args,\n\u001b[1;32m    754\u001b[0m         )\n\u001b[1;32m    755\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    756\u001b[0m         handle \u001b[39m=\u001b[39m gzip\u001b[39m.\u001b[39mGzipFile(\n\u001b[1;32m    757\u001b[0m             \u001b[39m# No overload variant of \"GzipFile\" matches argument types\u001b[39;00m\n\u001b[1;32m    758\u001b[0m             \u001b[39m# \"Union[str, BaseBuffer]\", \"str\", \"Dict[str, Any]\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcompression_args,\n\u001b[1;32m    762\u001b[0m         )\n",
      "File \u001b[0;32m~/mambaforge/envs/multibind/lib/python3.10/gzip.py:174\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    172\u001b[0m     mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m fileobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     fileobj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmyfileobj \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, mode \u001b[39mor\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m filename \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(fileobj, \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/countTable.0.CTCF_r3.tsv.gz'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/countTable.0.CTCF_r3.tsv.gz', sep='\\t', header=None) # ['sequence', 'round.0', 'round.1']) #  header=False)\n",
    "data.columns = ['seq', 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # r0 = get_seqs('MAX_R0.fastq.gz', 0)\n",
    "# # r1 = get_seqs('MAX_R1.fastq.gz', 1)\n",
    "# r0 = get_seqs('data/ZeroCycle_ES0_TGTCGT20NGA_0.txt.gz', 0)\n",
    "# r1 = get_seqs('data/GATA3_TGTCGT20NGA_AC_1.fastq.gz', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r1.sort_values('counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r0.shape, r1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r0 = r0.head(10000)\n",
    "# r1 = r1.head(10000)\n",
    "# # r1['seq'] = r1['seq'].str[:5] + 'GATA' + r1['seq'].str[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = data.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = 0\n",
    "if control:\n",
    "    seq_len = 9\n",
    "    r0 = []\n",
    "    for seq in itertools.product('ATCG', repeat=seq_len):\n",
    "        seq = ''.join(seq)\n",
    "        r0.append([seq, 1, 0])\n",
    "    r0 = pd.DataFrame(r0, columns=['seq', 'counts', 'round'])\n",
    "    r0['prob'] = r0['counts'] / sum(r0['counts'])\n",
    "    r0 = r0.sample(1000)\n",
    "\n",
    "    r1 = []\n",
    "    for seq in itertools.product('ATCG', repeat=seq_len):\n",
    "        seq = ''.join(seq)\n",
    "        if np.random.random() > .3:\n",
    "            seq = seq[:3] + 'GATA' + seq[7:]\n",
    "        if np.random.random() > .6:\n",
    "            seq = seq[:3] + 'CATA' + seq[7:]\n",
    "\n",
    "        r1.append([seq, 20 if 'GATA' in seq else 10 if 'CATA' in seq else 5, 0])\n",
    "    r1 = pd.DataFrame(r1, columns=['seq', 'counts', 'round'])\n",
    "    r1['prob'] = r1['counts'] / sum(r1['counts'])\n",
    "    r1 = r1.sample(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fastq_from_seqs_table(seqs, path):\n",
    "    # write fastq.gz from seqs\n",
    "    seqs = [r['seq'] for ri, r in seqs.iterrows() for i in range(r['counts'])]\n",
    "    writer = gzip.open(path, 'wt')\n",
    "    for s in seqs:\n",
    "        writer.write('@\\n')\n",
    "        writer.write(s + '\\n')\n",
    "        writer.write('+\\n')\n",
    "        writer.write('@\\n')\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_fastq_from_seqs_table(r1, 'r1.fastq.gz')\n",
    "# write_fastq_from_seqs_table(r0, 'r0.fastq.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste a GATA motif\n",
    "# r1['seq'] = np.where(r1['counts'] == 2, r1['seq'].str[:5] + 'GATA' + r1['seq'].str[9:], r1['seq'])\n",
    "# r1['seq'] = np.where(r1['counts'] == 2, 'AAAAAAAAAAAAAAAA', 'GGGGGGGGGGGGGGGG') # r1['seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmers_table = {} # round # k\n",
    "kstart = 13\n",
    "kstop = kstart + 1\n",
    "\n",
    "pseudocount = 1\n",
    "for selection_round, df in zip([0, 1], [r1, r1]):\n",
    "    kmer_table_by_k = {}\n",
    "    for k in range(kstart, kstop): # this is also the kernel size\n",
    "        print('selection round', selection_round)\n",
    "        print('generating a k-mers table for k=%i' % k)\n",
    "        unique_kmers = [[r['seq'][si: si + k], r[selection_round] + 1]  for ri, r in df.iterrows() for si in range(len(r['seq']) - k + 1)]\n",
    "        # kmer_table = pd.Series(unique_kmers).value_counts()\n",
    "        # kmer_prob = kmer_table / kmer_table.sum()\n",
    "        kmer_table = pd.DataFrame(unique_kmers, columns=['kmer', 'counts'])\n",
    "        kmer_table = kmer_table.groupby('kmer').sum().sort_values('counts', ascending=False)\n",
    "        kmer_table['prob'] = kmer_table['counts'] / kmer_table['counts'].sum()\n",
    "        kmer_table['round'] = selection_round\n",
    "        kmer_table_by_k[k] = kmer_table\n",
    "    kmers_table[selection_round] = kmer_table_by_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksel = kstart\n",
    "kmers = pd.concat([kmers_table[sel_round][ksel] for sel_round in kmers_table])\n",
    "kmers\n",
    "kmers.sort_values('prob', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1.sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.displot(np.log(r1[0] + 1))\n",
    "plt.xlabel('counts [log]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_Z(kmers_r0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wi(seq, kmers_r0, k):\n",
    "    wi = 0\n",
    "    for si in range(0, len(seq) - k + 1):\n",
    "        kmer = seq[si: si + k]\n",
    "        wi += sum([kmers_r0.loc[kmer]['prob']]) if kmer in kmers_r0.index else 0\n",
    "    return wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmers_r1 = kmers_table[1][ksel]\n",
    "kmers_r0 = kmers_table[0][ksel]\n",
    "\n",
    "print('preparing p_i0 for r0')\n",
    "w = []\n",
    "for ri, r in r1.iterrows():\n",
    "    if len(w) % 1000 == 0:\n",
    "        print(len(w), 'out of', r1.shape[0])\n",
    "    seq = r['seq']\n",
    "    # print(seq)\n",
    "    wi = get_wi(seq, kmers_r1, ksel)\n",
    "    # print(seq, wi)\n",
    "    w.append(wi)\n",
    "    # break    \n",
    "r1['p_i0'] = w\n",
    "\n",
    "\n",
    "print('preparing p_i0 for r1')\n",
    "# do the same for r1\n",
    "w = []\n",
    "for ri, r in r1.iterrows():\n",
    "    if len(w) % 1000 == 0:\n",
    "        print(len(w), 'out of', r1.shape[0])\n",
    "    seq = r['seq']\n",
    "    # print(seq)\n",
    "    wi = get_wi(seq, kmers_r0, ksel)\n",
    "    # print(seq, wi)\n",
    "    w.append(wi)\n",
    "    # break    \n",
    "r1['p_i1'] = w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z0 is a constant so arguably we do not need it\n",
    "def calc_Z(kmers_table, seq_len):\n",
    "    for seq in itertools.product('ATCG', repeat=seq_len):\n",
    "        print(''.join(seq))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define models\n",
    "\n",
    "# Class for creating the neural network.\n",
    "class nrlb(nn.Module):\n",
    "    def __init__(self, n_bp, kernel_sizes=[10], init_scale=1e-3, initial_conv1d=None,\n",
    "                random_init=0):\n",
    "        super(nrlb, self).__init__()\n",
    "\n",
    "        # Create and initialise weights and biases for the layers.\n",
    "        self.conv1d_set = nn.ModuleList()    \n",
    "        for k in kernel_sizes:\n",
    "            conv1d = nn.Conv1d(4, 1, k)\n",
    "            if initial_conv1d is not None:\n",
    "                print(conv1d.weight.data)\n",
    "                conv1d.weight.data = initial_conv1d\n",
    "                print(conv1d.weight.data)\n",
    "            else:\n",
    "                if not random_init:\n",
    "                    print('zetting weights to zero')\n",
    "                    conv1d.weight.data.fill_(0.0)\n",
    "            self.conv1d_set.append(conv1d)\n",
    "        # self.scale_set = [nn.Parameter(torch.FloatTensor([init_scale])) for k in kernel_sizes]\n",
    "  \n",
    "        # self.batchnorm = torch.nn.BatchNorm1d(1)\n",
    "\n",
    "        # self.softmax_out = nn.Softmax(1)\n",
    "        self.linear1 = nn.Linear(1, 1)\n",
    "#         self.linear2 = nn.Linear(1, 1)\n",
    "#         self.linear3 = nn.Linear(1, 1)\n",
    "#         self.linear4 = nn.Linear(1, 1)\n",
    "#         self.linear5 = nn.Linear(1, 1)\n",
    "\n",
    "        # self.fc2 = nn.Linear(n_bp - kernel_size + 1, 1)\n",
    "\n",
    "    def forward(self, x1_fwd, x1_rev, x2_fwd, x2_rev):\n",
    "        # Create the forward pass through the network.\n",
    "        \n",
    "        res = None # np.zeros(x1.shape[0])\n",
    "        for i, conv1d in enumerate(self.conv1d_set):\n",
    "            \n",
    "            out1 = conv1d(x1_fwd)\n",
    "            out2 = conv1d(x1_rev)\n",
    "\n",
    "            out1 = out1.squeeze()\n",
    "            out2 = out2.squeeze()\n",
    "            \n",
    "            out1 = out1 * x2_fwd\n",
    "            out2 = out2 * x2_rev\n",
    "\n",
    "            # print(out1.shape)\n",
    "            # print(sum(out))\n",
    "            out = torch.sum(out1, axis=1) + torch.sum(out2, axis=1)\n",
    "            \n",
    "            # out = out * self.scale_set[i]\n",
    "\n",
    "            if res is None:\n",
    "                res = out\n",
    "            else:\n",
    "                res += out\n",
    "\n",
    "        # print(res.unsqueeze(-1))\n",
    "        # print(res.shape)\n",
    "        # res = self.softmax_out(res.unsqueeze(-1))\n",
    "\n",
    "        # res = torch.log(res + 1e-15)\n",
    "        # res = self.batchnorm(res.unsqueeze(-1))\n",
    "\n",
    "        res = self.linear1(res.unsqueeze(-1))\n",
    "        # res = self.linear2(res)\n",
    "        # res = self.linear3(res)\n",
    "        # res = self.linear4(res)\n",
    "        # res = self.linear5(res)\n",
    "\n",
    "        # print(res.shape)\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1[[1, 0]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## x_test = torch.rand(1).to(device='cuda')\n",
    "# x_test = torch.rand([5, 4, 20]).to(device='cuda')\n",
    "# # print(x_test)\n",
    "# out_test = torch.sum(model.conv1d_set[0](x_test), axis=2)\n",
    "# # print(model.softmax_out(out_test))\n",
    "# # print(model.linear_out(out_test))\n",
    "\n",
    "# model.linear3(out_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # r1_sel['probe.enrichment'] = r1_sel['p_i1'] / r1_sel['p_i0']\n",
    "# # r1_sel = r1_sel.sort_values('probe.enrichment', ascending=False)\n",
    "# r1_sel[r1_sel['seq'].str.contains('GATA') | r1_sel['seq'].str.contains('TATC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n######\\nQuery %i' % k)\n",
    "print('preparing data...')\n",
    "x = np.array(r1['seq'])\n",
    "\n",
    "r1['p_i1'] = r1[1] / sum(r1[1])\n",
    "# plt.scatter(r1['p_i0'], r1['p_i1'])\n",
    "\n",
    "y = np.array(r1['p_i1'], dtype=np.float32)\n",
    "\n",
    "\n",
    "print(r1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = r1.sort_values(1, ascending=False)\n",
    "r1['ratio_1_over_0'] = np.log((r1['p_i1'] + 1) / (r1['p_i0'] + 1))\n",
    "r1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1.sort_values('ratio_1_over_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(r1['ratio_1_over_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(r1['ratio_1_over_0'], dtype=np.float32)\n",
    "# y = np.array(r1_sel['probe.enrichment'], dtype=np.float32)\n",
    "# y = np.array(r1['counts'], dtype=np.float32)\n",
    "# r1['ratio_1_over_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "# str(seq.reverse_complement())\n",
    "\n",
    "print('preparing one encoding representations...')\n",
    "# create the one hot encoding representations. This needs to be appended by the dictionary size to avoid smaller matrices when some nucleotides not found.\n",
    "x1_fwd = np.array([hot_dna(x[i] + 'ACGT').onehot.T[:,:-4].astype(np.float32) for i in range(len(x))])\n",
    "x1_rev = np.array([hot_dna(str(Seq(x[i]).reverse_complement()) + 'ACGT').onehot.T[:,:-4].astype(np.float32) for i in range(len(x))])\n",
    "print('done...')\n",
    "\n",
    "\n",
    "\n",
    "pmin = min(kmers_table[0][ksel]['prob'])\n",
    "\n",
    "x2_fwd = np.array([np.array([kmers_table[0][ksel].loc[xi[si: si + k]]['prob'] if xi[si: si + k] in kmers_table[0][ksel] else pmin\n",
    "                             for si in range(len(xi) - k + 1)], dtype=np.float32)\n",
    "                   for xi in x], dtype=np.float32)\n",
    "\n",
    "x2_rev = np.array([np.array([kmers_table[0][ksel].loc[str(Seq(xi[si: si + k]).reverse_complement())]['prob']\n",
    "                             if str(Seq(xi[si: si + k]).reverse_complement()) in kmers_table[0][ksel] else pmin\n",
    "                             for si in range(len(xi) - k + 1)], dtype=np.float32)\n",
    "                   for xi in x], dtype=np.float32)\n",
    "\n",
    "\n",
    "x2_fwd = x2_fwd.reshape(x2_fwd.shape[:2])\n",
    "print(x2_fwd.shape)\n",
    "\n",
    "x2_rev = x2_rev.reshape(x2_rev.shape[:2])\n",
    "print(x2_rev.shape)\n",
    "\n",
    "scale_01 = False\n",
    "if scale_01:\n",
    "    y = (y - y.min()) / (y.max() - y.min())\n",
    "    \n",
    "# std norm \n",
    "y = (y - y.mean()) / y.std()\n",
    "\n",
    "\n",
    "# x_train -= np.mean(x_train)\n",
    "y += np.abs(np.min(y))\n",
    "\n",
    "# y *= 1e-10\n",
    "\n",
    "# tensors\n",
    "x1_fwd_tensor = torch.FloatTensor(x1_fwd)\n",
    "x1_rev_tensor = torch.FloatTensor(x1_rev)\n",
    "\n",
    "x2_fwd_tensor = torch.FloatTensor(x2_fwd)\n",
    "x2_rev_tensor = torch.FloatTensor(x2_rev)\n",
    "\n",
    "y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "n_bp = len(x[0])\n",
    "print(n_bp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(r1['seq'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = conv1d(xi)\n",
    "# out_df = pd.DataFrame(out.squeeze().detach().numpy())\n",
    "# # print(out_df.sum(axis=1))\n",
    "# torch.sum(out.squeeze(), axis=1) * p_i0\n",
    "# # out = out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x2_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "print('# of base pairs', n_bp)\n",
    "print('kernel size (also k-mer length)', k)\n",
    "init_scale = 1\n",
    "\n",
    "K = torch.Tensor([[[0, 0, 0, 0],\n",
    "                   [0, 0, 0, 0],\n",
    "                   [0, 0, 0, 0],\n",
    "                   [0, 0, 0, 0]]])\n",
    "\n",
    "K = torch.Tensor([[[0] * k,\n",
    "                   [0] * k,\n",
    "                   [0] * k,\n",
    "                   [0] * k]])\n",
    "\n",
    "model = nrlb(n_bp, kernel_sizes=[k], init_scale=init_scale, # initial_conv1d=K,\n",
    "            random_init=1).to(device) # Create an instance of the network in memory (potentially GPU memory).\n",
    "# net = netsimple(n_bp, kernel_sizes=[k], init_scale=init_scale).to(device) # Create an instance of the network in memory (potentially GPU memory).\n",
    "criterion = nn.MSELoss() # Add a sigmoid activation function to the output.  Use a binary cross entropy\n",
    "                                    # loss function.\n",
    "optimiser = topti.Adam(model.parameters(), lr = 0.001) # Minimise the loss using the Adam algorithm.\n",
    "# optimiser = topti.LBFGS(net.parameters(), lr = 0.01) # Minimise the loss using the Adam algorithm.\n",
    "\n",
    "\n",
    "ppms = []\n",
    "for ppm in model.conv1d_set:\n",
    "    ppm = ppm.weight.data.cpu().numpy()\n",
    "    ppm = pd.DataFrame(ppm[0])    \n",
    "    ppms.append(ppm)\n",
    "for i, ppm in enumerate(ppms):\n",
    "    plt.subplot(len(ppms), 1, i + 1)\n",
    "    sns.heatmap(ppm, cmap='Reds')\n",
    "plt.show()\n",
    "\n",
    "log_each = int(n_epochs / 10)\n",
    "## TRAIN\n",
    "print('Training...')\n",
    "# print('shapes', x1_tensor.shape, x2_tensor.shape, y.shape)\n",
    "\n",
    "labels = y_tensor\n",
    "\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    x1_fwd_tensor = x1_fwd_tensor.to(device='cuda')\n",
    "    x1_rev_tensor = x1_rev_tensor.to(device='cuda')\n",
    "    x2_fwd_tensor = x2_fwd_tensor.to(device='cuda')\n",
    "    x2_rev_tensor = x2_rev_tensor.to(device='cuda')\n",
    "    labels = labels.to(device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_fwd_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1d_set[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_fwd_tensor.shape, x1_rev_tensor.shape, x2_fwd_tensor.shape, x2_rev_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # print(inputs.shape, labels.shape)\n",
    "    outputs = model(x1_fwd_tensor, x1_rev_tensor, x2_fwd_tensor, x2_rev_tensor) # Forward pass through the network.\n",
    "    # outputs = net(inputs) # Forward pass through the network.\n",
    "\n",
    "    # loss = -torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    # def f():\n",
    "    #     return loss\n",
    "    # optimiser.step(f) # Step to minimise the loss according to the gradient.\n",
    "\n",
    "    loss.backward() # Calculate gradients.\n",
    "    optimiser.step() # Step to minimise the loss according to the gradient.\n",
    "\n",
    "    if epoch % log_each == 0:\n",
    "        print(\"Epoch: %2d, Loss: %.12f\" % (epoch + 1, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import logomaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1.sort_values('ratio_1_over_0', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_fwd_tensor[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions\n",
    "ytrue = []\n",
    "ypred = []\n",
    "with torch.no_grad():\n",
    "    # Get a batch and potentially send it to GPU memory.\n",
    "    # inputs, labels = batch[\"input\"].to(device), batch[\"label\"].to(device)\n",
    "    # inputs = x1_tensor\n",
    "    labels = y_tensor\n",
    "\n",
    "    # print(inputs.shape)\n",
    "    # outputs = torch.sigmoid(net(x1_tensor, x2_tensor))\n",
    "    outputs = model(x1_fwd_tensor, x1_rev_tensor, x2_fwd_tensor, x2_rev_tensor).cpu() # Forward pass through the network.\n",
    "    # predicted = torch.round(outputs)\n",
    "    predicted = outputs\n",
    "\n",
    "    ytrue += list(labels)\n",
    "    ypred += list(outputs)\n",
    "\n",
    "print('true', ytrue[:5])\n",
    "print('pred', ypred[:5])\n",
    "\n",
    "# print('MSE default', metrics.mean_squared_error(y, .sum(axis=1)))\n",
    "print('MSE final', metrics.mean_squared_error(ytrue, ypred))\n",
    "\n",
    "print(len(ytrue), len(ypred))\n",
    "\n",
    "## downstream plots: compare before and after optimization effects\n",
    "# prior relationship between kmer counts and observed \n",
    "# plt.subplot(1, 2, 1)\n",
    "\n",
    "# ypred_init = np.array(x2)\n",
    "# ypred_init = np.array(x2.sum(axis=1))\n",
    "\n",
    "# plt.scatter(np.array(ytrue), ypred_init, s=10, c='red')\n",
    "# plt.xlabel('counts [norm]')\n",
    "# plt.ylabel('sum of k-mer prob')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(np.array(ytrue), np.array(ypred), s=10)\n",
    "plt.xlabel('observed value')\n",
    "plt.ylabel('predicted value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ppms = []\n",
    "for ppm in model.conv1d_set:\n",
    "    ppm = ppm.weight.data.cpu().numpy()\n",
    "    print(ppm)\n",
    "    ppm = pd.DataFrame(ppm[0])\n",
    "    ppm.index = 'A', 'C', 'G', 'T'\n",
    "    ppms.append(ppm)\n",
    "for i, ppm in enumerate(ppms):\n",
    "    plt.subplot(len(ppms), 1, i + 1)\n",
    "    \n",
    "    cmax, cmin = max(ppm.max()), min(ppm.min())\n",
    "    \n",
    "    sns.heatmap(ppm, cmap='Reds', vmin=cmin, vmax=cmax)\n",
    "    print(ppm)\n",
    "    # create Logo object\n",
    "    crp_logo = logomaker.Logo(ppm.T,\n",
    "                          shade_below=.5,\n",
    "                          fade_below=.5,\n",
    "                          font_name='Arial Rounded MT Bold')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# plt.show()\n",
    "\n",
    "# print(model.scale_set)\n",
    "# print(net, ytrue, ypred)\n",
    "ppm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(ypred), max(ytrue), min(ypred), min(ytrue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('here...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.array(ytrue), np.array(ypred))\n",
    "# plt.autoscale(enable=False, axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.autoscale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(ypred.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "make_dot(outputs.mean(), params=dict(list(model.named_parameters()))).render(\"attached\") # rnn_torchviz\", format=\"png\")\n",
    "# make_dot(r).render(\"attached\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!readlink -f ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.conv1d_set:\n",
    "    print(layer.weight.data)\n",
    "model.linear1.bias, model.linear1.weight\n",
    "# print(layer.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fastq = np.array([(s.strip()).decode('utf-8') for s in gzip.open(nrlb_data)])\n",
    "# # print(len(fastq))\n",
    "# # mask = np.array([((i + 3) % 4 == 0) for i in range(len(fastq))])\n",
    "# # seqs = fastq[mask] \n",
    "# df = pd.read_csv('GATA3_TGTCGT20NGA_AC_4.tsv.gz', index_col=0)\n",
    "# # df['seq'] += df['seq'] + ''\n",
    "# # df = df[~df['seq'].str.contains('N')]\n",
    "# # seq_counts = df['seq'].value_counts()\n",
    "# # df['counts'] = [seq_counts.loc[s] for s in df['seq']]\n",
    "# # df = df.drop_duplicates('seq').sort_values('counts', ascending=False)\n",
    "\n",
    "# # selex_path = '/mnt/znas/icb_zstore01/groups/ml01/workspace/ignacio.ibarra/screg/notebooks/protein_dna_landscape/ELK1_EVX1_3_AAD_TGGAAT40NAAT.fastq.tsv'\n",
    "# # df = pd.read_csv(selex_path, sep='\\t', header=None)\n",
    "# # df.columns = ['kmer', 'counts']\n",
    "\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Class for reading training/testing dataset files.\n",
    "# class toyDataset(tdata.Dataset):\n",
    "#     def __init__(self, dataFile=None, labelFile=None, data_x=None, data_y=None):\n",
    "#         # Load data from files.\n",
    "#         self.inputs = [np.loadtxt(dataFile, dtype = np.float32).reshape(-1, 4, 1000) if dataFile is not None else data_x[0],\n",
    "#                        np.loadtxt(dataFile, dtype = np.float32).reshape(-1, 1, 1000) if dataFile is not None else data_x[1]]\n",
    "        \n",
    "#         self.labels = np.loadtxt(labelFile, dtype = np.float32) if dataFile is not None else data_y\n",
    "\n",
    "#         self.length = len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # Return a single input/label pair from the dataset.\n",
    "#         inputSample = self.inputs[index]\n",
    "#         labelSample = self.labels[index]\n",
    "#         sample = {\"input\": inputSample, \"label\": labelSample}\n",
    "\n",
    "#         return sample\n",
    "\n",
    "#     def __len__(self):\n",
    "\n",
    "#         return self.length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counts distribution for the observed data (log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial parms for ppms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_model(df, kmer_table_by_k, k, n_epochs=10, nsel=2000, log_each=250, init_scale=10000):\n",
    "    \n",
    "    print('\\n######\\nQuery %i' % k)\n",
    "    print('preparing data...')\n",
    "    x = np.array(df['seq'].head(nsel))\n",
    "    y = np.array(df['counts'].head(nsel), dtype=np.float32)\n",
    "    # create the one hot encoding representations. This needs to be appended by the dictionary size to avoid smaller matrices when some nucleotides not found.\n",
    "    x_one_hot = np.array([hot_dna(x[i] + 'ACGT').onehot.T[:,:-4].astype(np.float32) for i in range(len(x))])\n",
    "    x1 = x_one_hot\n",
    "    x2 = np.array([np.array([kmer_table_by_k[k].loc[xi[si: si + k]]['prob'] for si in range(len(xi) - k + 1)], dtype=np.float32)\n",
    "                   for xi in x], dtype=np.float32)\n",
    "    x2 = x2.reshape(x2.shape[:2])\n",
    "    y = (y - y.min()) / (y.max() - y.min())\n",
    "    \n",
    "    # tensors\n",
    "    x1_tensor = torch.FloatTensor(x1)\n",
    "    x2_tensor = torch.FloatTensor(x2)\n",
    "    y_tensor = torch.FloatTensor(y)\n",
    "    \n",
    "    n_bp = len(x[0])\n",
    "    print(n_bp)\n",
    "\n",
    "    \n",
    "    print('# of base pairs', n_bp)\n",
    "    print('kernel size (also k-mer length)', k)\n",
    "    net = netsimple(n_bp, kernel_sizes=[k], init_scale=init_scale).to(device) # Create an instance of the network in memory (potentially GPU memory).\n",
    "    criterion = nn.MSELoss() # Add a sigmoid activation function to the output.  Use a binary cross entropy\n",
    "                                        # loss function.\n",
    "    optimiser = topti.Adam(net.parameters(), lr = 0.01) # Minimise the loss using the Adam algorithm.\n",
    "    \n",
    "    ppms = []\n",
    "    for ppm in net.conv1d_set:\n",
    "        ppm = ppm.weight.data.numpy()\n",
    "        ppm = pd.DataFrame(ppm[0])    \n",
    "        ppms.append(ppm)\n",
    "    for i, ppm in enumerate(ppms):\n",
    "        plt.subplot(len(ppms), 1, i + 1)\n",
    "        sns.heatmap(ppm, cmap='Reds')\n",
    "\n",
    "    ## TRAIN\n",
    "    print('Training...')\n",
    "    print('shapes', x1_tensor.shape, x2_tensor.shape, y.shape)\n",
    "    for epoch in range(n_epochs):\n",
    "        inputs, labels = x1_tensor, y_tensor\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # print(inputs.shape, labels.shape)\n",
    "        outputs = net(x1_tensor, x2_tensor) # Forward pass through the network.\n",
    "        # outputs = net(inputs) # Forward pass through the network.\n",
    "\n",
    "        # loss = -torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward() # Calculate gradients.\n",
    "        optimiser.step() # Step to minimise the loss according to the gradient.\n",
    "\n",
    "        if epoch % log_each == 0:\n",
    "            print(\"Epoch: %2d, Loss: %.3f\" % (epoch + 1, loss))\n",
    "         \n",
    "    # calculate predictions\n",
    "    ytrue = []\n",
    "    ypred = []\n",
    "    with torch.no_grad():\n",
    "        # Get a batch and potentially send it to GPU memory.\n",
    "        # inputs, labels = batch[\"input\"].to(device), batch[\"label\"].to(device)\n",
    "        inputs, labels = x1_tensor, y_tensor\n",
    "\n",
    "        # print(inputs.shape)\n",
    "        # outputs = torch.sigmoid(net(x1_tensor, x2_tensor))\n",
    "        outputs = net(x1_tensor, x2_tensor)\n",
    "        # predicted = torch.round(outputs)\n",
    "        predicted = outputs\n",
    "\n",
    "        ytrue += list(labels)\n",
    "        ypred += list(outputs)\n",
    "\n",
    "    print('MSE default', metrics.mean_squared_error(y, x2.sum(axis=1)))\n",
    "    print('MSE final', metrics.mean_squared_error(ytrue, ypred))\n",
    "    \n",
    "    print(len(ytrue), len(ypred))\n",
    "\n",
    "    ## downstream plots: compare before and after optimization effects\n",
    "    # prior relationship between kmer counts and observed \n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    ypred_init = np.array(x2.sum(axis=1))\n",
    "    \n",
    "    plt.scatter(np.array(ytrue), ypred_init, s=10, c='red')\n",
    "    plt.xlabel('counts [norm]')\n",
    "    plt.ylabel('sum of k-mer prob')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(np.array(ytrue), np.array(ypred), s=10)\n",
    "    plt.xlabel('observed')\n",
    "    plt.ylabel('predicted')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return net, ytrue, ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ki in range(kstart, kstop):\n",
    "    \n",
    "    # this is a function call to the whole pipeline routine.\n",
    "    nsel = 10000 # df.shape[0] # 10000\n",
    "    net, ytrue, ypred = get_trained_model(df, kmer_table_by_k, ki, n_epochs=2000, log_each=250, nsel=nsel)\n",
    "    \n",
    "    ppms = []\n",
    "    for ppm in net.conv1d_set:\n",
    "        ppm = ppm.weight.data.numpy()\n",
    "        ppm = pd.DataFrame(ppm[0])    \n",
    "        ppms.append(ppm)\n",
    "    for i, ppm in enumerate(ppms):\n",
    "        plt.subplot(len(ppms), 1, i + 1)\n",
    "        sns.heatmap(ppm, cmap='Reds')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print(net.scale_set)\n",
    "    \n",
    "    ### Visualize results (not working without screg)\n",
    "    plot_logo = False\n",
    "    if plot_logo:\n",
    "        import screg as scr\n",
    "        scr.constants.ANNOTATIONS_DIRECTORY = '/mnt/znas/icb_zstore01/groups/ml01/datasets/annotations'\n",
    "        scr.constants.SCREG_DATA_DIRECTORY = '/mnt/znas/icb_zstore01/groups/ml01/workspace/ignacio.ibarra/screg/data'\n",
    "\n",
    "        for mi, motif in enumerate(ppms):\n",
    "            ppm = pd.DataFrame(motif)\n",
    "            norm = True\n",
    "            if norm:\n",
    "                for c in ppm:\n",
    "                    # print(c)\n",
    "                    ppm[c] = (ppm[c] - min(ppm[c])) / (max(ppm[c]) - min(ppm[c]))\n",
    "                    ppm[c] = ppm[c] / sum(ppm[c])\n",
    "            ppm.index = 'A', 'C', 'G', 'T'\n",
    "            print(mi * 2 + 1)\n",
    "            ax = plt.subplot(len(ppms), 2, mi * 2 + 1)\n",
    "            sns.heatmap(ppm, cmap='Reds',)\n",
    "            print(mi * 2 + 2)\n",
    "            ax = plt.subplot(len(ppms), 2, mi * 2 + 2)\n",
    "            \n",
    "            scr.pl.plot_pwm_model('learned motif %i' % (mi + 1), ppm=ppm, ax=ax)\n",
    "            plt.ylim([0, 1.3])\n",
    "            plt.ylabel('Bits')\n",
    "            ppm\n",
    "            plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking log conversions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# create dummy data for training\n",
    "x_values = [i for i in range(11)]\n",
    "\n",
    "# log conversion\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_values = np.power(x_train, 10)\n",
    "x_train = np.log10(x_values + 1)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = np.log10(y_train + 1)\n",
    "\n",
    "x_train += 100\n",
    "\n",
    "x_train -= np.mean(x_train)\n",
    "x_train += np.abs(np.min(x_train))\n",
    "\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 1        # takes variable 'x' \n",
    "outputDim = 1       # takes variable 'y'\n",
    "learningRate = 0.001\n",
    "epochs = 10000\n",
    "\n",
    "model = linearRegression(inputDim, outputDim)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    # print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % int(epochs / 10) == 0:\n",
    "        print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear.weight.data.cpu().numpy(), model.linear.bias.data.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    print(predicted)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('multibind')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "21beeb3f351c8e886a898db8b4cccf8d4f8a70210033cb08b23469d9f8df079c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
