{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "004d2a17-2952-4822-83ae-09dd177136d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27c59d94-fd4a-4a3c-9abf-5109ef97c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import multibind as mb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import bindome as bd\n",
    "bd.constants.ANNOTATIONS_DIRECTORY = '/mnt/c/Users/ignacio/Dropbox/annotations'\n",
    "# mb.models.MultiBind\n",
    "import torch.optim as topti\n",
    "import torch.utils.data as tdata\n",
    "import matplotlib.pyplot as plt\n",
    "import logomaker\n",
    "\n",
    "# Use a GPU if available, as it should be faster.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e226d21e-d360-4316-872a-777b6895b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = mb.bindome.datasets.scATAC.PBMCs_10x_v2(datadir='../../atac_poisson_study/data/')\n",
    "peak_ids = adata.var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "200a9729-9538-4622-a8f6-98e6c9d6f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c6414ce-fd5d-4022-8366-af929eb2d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var['summit'] = ((adata.var['end'] + adata.var['start']) / 2).astype(int)\n",
    "adata.var['summit.start'] = adata.var['summit'] - 100\n",
    "adata.var['summit.end'] = adata.var['summit'] + 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f6301bc-467a-4b74-a736-3e2613aab729",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var['k.summit'] = adata.var['chr'] + ':' + adata.var['summit.start'].astype(str) + '-' + adata.var['summit.end'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7effe82e-e5d9-4eb2-810d-8aa4cd6b0963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp8jq207e4\n",
      "genome hg38 False\n",
      "options\n",
      "/mnt/c/Users/ignacio/Dropbox/annotations/hg38/genome/hg38.fa\n",
      "True /mnt/c/Users/ignacio/Dropbox/annotations/hg38/genome/hg38.fa\n",
      "running bedtools...\n",
      "bedtools getfasta -fi /mnt/c/Users/ignacio/Dropbox/annotations/hg38/genome/hg38.fa -bed /tmp/tmp8jq207e4 -fo /tmp/tmpbi2fvy_f\n"
     ]
    }
   ],
   "source": [
    "seqs = mb.bindome.tl.get_sequences_from_bed(adata.var[['chr', 'summit.start', 'summit.end']], genome='hg38', uppercase=True)\n",
    "keys = set([s[0] for s in seqs])\n",
    "adata = adata[:,adata.var['k.summit'].isin(keys)]\n",
    "# seqs = [[s[0], s[1].upper()] for s in seqs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0f698-3282-43bd-835e-af784c80bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee1199-34d9-495d-a17a-c9a757e63bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43712938-90b9-43f4-8a1d-14c40766ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Ns\n",
    "seqs = [[s[0], s[1].replace('N', '')] for s in seqs]\n",
    "counts = adata.X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8babeabf-0b8f-478a-b479-121fa62c8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_data = pd.DataFrame.sparse.from_spmatrix(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d579216-c739-4e93-b686-dda243b07310",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_data['seq'] = [s[1] for s in seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97306ac-efa3-4f4f-a15d-164788fe5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083e53a-9928-4d00-a243-9c40283b4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_data = next_data.head(10000)\n",
    "next_data = next_data.reindex(next_data[next_data.columns[:-1]].sum(axis=1).sort_values(ascending=False).index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c73bb6-0571-4c88-8048-9dd72ca09d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_data_sel = next_data[[i for i in range(13)] + ['seq']]\n",
    "next_data_sel = next_data_sel[next_data_sel[next_data_sel.columns[:-1]].sum(axis=1) > 10]\n",
    "next_data_sel.shape\n",
    "# for testing purposes\n",
    "# next_data['seq'] = next_data['seq'].astype(str).str[:min(next_data['seq'].str.len())]\n",
    "next_data_sel = next_data_sel[next_data_sel['seq'].str.len() == 200].reset_index(drop=True)\n",
    "next_data_sel.sum(axis=1)\n",
    "n_rounds = next_data_sel.shape[1] - 2\n",
    "n_rounds\n",
    "# # n_rounds\n",
    "dataset = mb.datasets.SelexDataset(next_data_sel, n_rounds=n_rounds)\n",
    "train = tdata.DataLoader(dataset=dataset, batch_size=100, shuffle=True)\n",
    "# s = [mb.tl.onehot_mononuc(row['seq'], le, oe) for index, row in next_data.iterrows()]\n",
    "# model = mb.models.DinucSelex(use_dinuc=True, kernels=[0] + [w] * (n_kernels - 1),  n_rounds=n_rounds).to(device)\n",
    "\n",
    "\n",
    "import copy\n",
    "model_by_k = {}\n",
    "optimize_motif_shift = True\n",
    "n_kernels = 3\n",
    "num_epochs = 500\n",
    "early_stopping = 5\n",
    "log_each = 25\n",
    "seqlen = list(next_data['seq'].str.len())[0]\n",
    "\n",
    "for w in range(12, seqlen, 2):\n",
    "    # step 1) freeze everything before the current binding mode\n",
    "    print('next seqlen', w)\n",
    "    model = mb.models.DinucSelex(use_dinuc=True, kernels=[0] + [w] * (n_kernels - 1),  n_rounds=n_rounds).to(device)\n",
    "\n",
    "    for i in range(0, n_kernels):\n",
    "        print('kernel to optimize %i' % i)\n",
    "\n",
    "        for ki in range(n_kernels):\n",
    "            mb.tl.update_grad(model, ki, ki == i)\n",
    "\n",
    "        optimiser = topti.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "        criterion = mb.tl.PoissonLoss()\n",
    "        mb.tl.train_network(model, train, device, optimiser, criterion, num_epochs=num_epochs,\n",
    "                            early_stopping=early_stopping, log_each=log_each)\n",
    "        model.load_state_dict(model.best_model_state) # probably here load the state of the best epoch and save \n",
    "        k_parms = 'model_%i' % w\n",
    "        model_by_k[k_parms] = copy.deepcopy(model) # store model parameters and fit for later visualization\n",
    "        # optimizer for left / right flanks\n",
    "        best_loss = model_by_k[k_parms].best_loss\n",
    "\n",
    "        print('before shift optim.')\n",
    "        mb.pl.conv_mono(model)\n",
    "\n",
    "        #######\n",
    "        # optimize the flanks through +1/-1 shifts\n",
    "        #######\n",
    "        if optimize_motif_shift and i != 0:\n",
    "            next_loss = None\n",
    "            while next_loss is None or next_loss < best_loss:\n",
    "                print('optimize_motif_shift (%s)...' % ('once' if next_loss is None else 'again'), end='')\n",
    "                model = model_by_k[k_parms]\n",
    "                best_loss = model.best_loss\n",
    "\n",
    "                model_left = mb.tl.train_shift(copy.deepcopy(model), train, kernel_i=i, shift=1,\n",
    "                                               device=device, num_epochs=num_epochs,\n",
    "                                               early_stopping=early_stopping, log_each=log_each,\n",
    "                                               update_grad_i=i)\n",
    "                model_right = mb.tl.train_shift(copy.deepcopy(model), train, kernel_i=i, shift=-1,\n",
    "                                                device=device, num_epochs=num_epochs,\n",
    "                                                early_stopping=early_stopping, log_each=log_each,\n",
    "                                                update_grad_i=i)\n",
    "                print(best_loss, model_left.best_loss, model_right.best_loss)\n",
    "                best = sorted([[model, best_loss],\n",
    "                               [model_left, model_left.best_loss],\n",
    "                               [model_right, model_right.best_loss]], key= lambda x: x[-1])\n",
    "                next_model, next_loss = best[0]\n",
    "                model_by_k[k_parms] = copy.deepcopy(next_model)\n",
    "\n",
    "        model = model_by_k[k_parms]\n",
    "        n_feat = sum([np.prod(layer.kernel_size) for conv in [model.conv_mono, model.conv_di] for layer in conv if layer is not None])\n",
    "        l_best = model.best_loss\n",
    "\n",
    "        print('after shift optimz model')\n",
    "        mb.pl.conv_mono(model_by_k[k_parms])\n",
    "        print('')\n",
    "\n",
    "    assert False\n",
    "\n",
    "    r = [k_parms, w, n_feat, l_best]\n",
    "    # print(r)\n",
    "    res.append(r)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aaf30f-d0fb-4a02-b76a-6b31c68df439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d286cbbd-3a4b-4fde-9df6-da04687c1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_data_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788de75-056c-4b7d-b9da-12a68ea2f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "rcParams['figure.figsize'] = 5, 3\n",
    "\n",
    "activity = np.exp(model.log_activity.weight.cpu().detach().numpy())\n",
    "rel_activity = activity / np.sum(activity)\n",
    "sns.heatmap(rel_activity.T, cmap='Reds')\n",
    "plt.title('rel contrib.')\n",
    "plt.ylabel('selection round')\n",
    "plt.xlabel('binding mode rel activity')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibind",
   "language": "python",
   "name": "multibind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
