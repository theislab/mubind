{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9b91376-6d9c-4b0a-9302-88600cca8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c78083b8-3d92-4531-8434-51ee2c20cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johanna/anaconda3/envs/multibind/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import multibind as mb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import bindome as bd\n",
    "bd.constants.ANNOTATIONS_DIRECTORY = 'annotations'\n",
    "# mb.models.MultiBind\n",
    "import torch.optim as topti\n",
    "import torch.utils.data as tdata\n",
    "import matplotlib.pyplot as plt\n",
    "import logomaker\n",
    "import os\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "# Use a GPU if available, as it should be faster.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "798d94a7-2ba5-4943-93c9-245b6564fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f8e7e-37d2-45bb-be02-75868ec61652",
   "metadata": {},
   "source": [
    "# SELEX (one dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c3f5428-a57e-430b-987d-c8172d789d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rounds = 1\n",
    "data = pd.read_csv('../data/countTable.0.CTCF_r3.tsv.gz', sep='\\t', header=None)\n",
    "data.columns = ['seq'] + [i for i in range(n_rounds+1)]\n",
    "data.index = data['seq']\n",
    "del data['seq']\n",
    "data = data.sample(n=1000)\n",
    "labels = list(data.columns[:n_rounds + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "410aae35-b5c0-463c-8a2e-69058fa9283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = mb.datasets.SelexDataset(data, n_rounds=n_rounds, labels=labels)\n",
    "train = tdata.DataLoader(dataset=dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2c08380-4a30-4552-aab3-a927038d99a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next w 15 <class 'int'>\n",
      "# rounds 1\n",
      "# batches 1\n",
      "# enr_series True\n",
      "\n",
      "Kernel to optimize 0\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 1\n",
      "setting grad status of kernel at 1 to 0\n",
      "setting grad status of kernel at 2 to 0\n",
      "setting grad status of kernel at 3 to 0\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 1.074707 , best epoch: 49 secs per epoch: 0.057 s\n",
      "Epoch: 101, Loss: 0.849744 , best epoch: 99 secs per epoch: 0.055 s\n",
      "Epoch: 151, Loss: 0.847395 , best epoch: 139 secs per epoch: 0.054 s\n",
      "Epoch: 201, Loss: 0.847329 , best epoch: 178 secs per epoch: 0.053 s\n",
      "Epoch: 229, Loss: 0.8473 , best epoch: 178 secs per epoch: 0.053 s\n",
      "early stop!\n",
      "total time: 12.017 s\n",
      "secs per epoch: 0.053 s\n",
      "\n",
      "Kernel to optimize 1\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 0\n",
      "setting grad status of kernel at 1 to 1\n",
      "setting grad status of kernel at 2 to 0\n",
      "setting grad status of kernel at 3 to 0\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847112 , best epoch: 27 secs per epoch: 0.060 s\n",
      "Epoch: 78, Loss: 0.8472 , best epoch: 27 secs per epoch: 0.060 s\n",
      "early stop!\n",
      "total time: 4.601 s\n",
      "secs per epoch: 0.060 s\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847238 , best epoch: 7 secs per epoch: 0.045 s\n",
      "Epoch: 58, Loss: 0.8473 , best epoch: 7 secs per epoch: 0.045 s\n",
      "early stop!\n",
      "total time: 2.550 s\n",
      "secs per epoch: 0.045 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847212 , best epoch: 2 secs per epoch: 0.044 s\n",
      "Epoch: 53, Loss: 0.8473 , best epoch: 2 secs per epoch: 0.044 s\n",
      "early stop!\n",
      "total time: 2.306 s\n",
      "secs per epoch: 0.044 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847193 , best epoch: 47 secs per epoch: 0.045 s\n",
      "Epoch: 101, Loss: 0.847225 , best epoch: 91 secs per epoch: 0.044 s\n",
      "Epoch: 142, Loss: 0.8472 , best epoch: 91 secs per epoch: 0.044 s\n",
      "early stop!\n",
      "total time: 6.173 s\n",
      "secs per epoch: 0.044 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847181 , best epoch: 30 secs per epoch: 0.044 s\n",
      "Epoch: 81, Loss: 0.8471 , best epoch: 30 secs per epoch: 0.044 s\n",
      "early stop!\n",
      "total time: 3.488 s\n",
      "secs per epoch: 0.044 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            2             2      0  0.846960\n",
      "1            1             2      0  0.847021\n",
      "2            2             1      0  0.847032\n",
      "3            0             0      0  0.847043\n",
      "4            1             1      0  0.847048\n",
      "action: (2, 2, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (again)...\n",
      "sorted\n",
      "   expand.left  expand.right  shift     loss\n",
      "0            0             0      0  0.84696\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "sorted\n",
      "   expand.left  expand.right  shift     loss\n",
      "0            0             0      0  0.84696\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "\n",
      "final refinement step (after shift)...\n",
      "\n",
      "unfreezing all layers for final refinement\n",
      "kernel grad (0) = 1 \n",
      "kernel grad (1) = 1 \n",
      "kernel grad (2) = 1 \n",
      "kernel grad (3) = 1 \n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847104 , best epoch: 4 secs per epoch: 0.044 s\n",
      "Epoch: 55, Loss: 0.8471 , best epoch: 4 secs per epoch: 0.044 s\n",
      "early stop!\n",
      "total time: 2.395 s\n",
      "secs per epoch: 0.044 s\n",
      "best loss 0.8470168262720108\n",
      "\n",
      "Kernel to optimize 2\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 0\n",
      "setting grad status of kernel at 1 to 0\n",
      "setting grad status of kernel at 2 to 1\n",
      "setting grad status of kernel at 3 to 0\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847151 , best epoch: 33 secs per epoch: 0.049 s\n",
      "Epoch: 101, Loss: 0.847012 , best epoch: 54 secs per epoch: 0.049 s\n",
      "Epoch: 105, Loss: 0.8471 , best epoch: 54 secs per epoch: 0.050 s\n",
      "early stop!\n",
      "total time: 5.155 s\n",
      "secs per epoch: 0.050 s\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847178 , best epoch: 28 secs per epoch: 0.037 s\n",
      "Epoch: 79, Loss: 0.8471 , best epoch: 28 secs per epoch: 0.035 s\n",
      "early stop!\n",
      "total time: 2.768 s\n",
      "secs per epoch: 0.035 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847144 , best epoch: 45 secs per epoch: 0.034 s\n",
      "Epoch: 101, Loss: 0.847062 , best epoch: 55 secs per epoch: 0.034 s\n",
      "Epoch: 106, Loss: 0.8471 , best epoch: 55 secs per epoch: 0.034 s\n",
      "early stop!\n",
      "total time: 3.578 s\n",
      "secs per epoch: 0.034 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847066 , best epoch: 27 secs per epoch: 0.034 s\n",
      "Epoch: 78, Loss: 0.8472 , best epoch: 27 secs per epoch: 0.034 s\n",
      "early stop!\n",
      "total time: 2.646 s\n",
      "secs per epoch: 0.034 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847167 , best epoch: 23 secs per epoch: 0.035 s\n",
      "Epoch: 74, Loss: 0.8472 , best epoch: 23 secs per epoch: 0.035 s\n",
      "early stop!\n",
      "total time: 2.545 s\n",
      "secs per epoch: 0.035 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            2             1      0  0.846900\n",
      "1            0             0      0  0.846903\n",
      "2            1             2      0  0.846938\n",
      "3            2             2      0  0.846968\n",
      "4            1             1      0  0.846974\n",
      "action: (2, 1, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (again)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847222 , best epoch: 23 secs per epoch: 0.035 s\n",
      "Epoch: 101, Loss: 0.847078 , best epoch: 99 secs per epoch: 0.034 s\n",
      "Epoch: 150, Loss: 0.8472 , best epoch: 99 secs per epoch: 0.034 s\n",
      "early stop!\n",
      "total time: 5.104 s\n",
      "secs per epoch: 0.034 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            1             1      0  0.846888\n",
      "1            0             0      0  0.846900\n",
      "action: (1, 1, 0)\n",
      "\n",
      "stop. Reached maximum w...\n",
      "stop. Reached maximum w...\n",
      "\n",
      "\n",
      "final refinement step (after shift)...\n",
      "\n",
      "unfreezing all layers for final refinement\n",
      "kernel grad (0) = 1 \n",
      "kernel grad (1) = 1 \n",
      "kernel grad (2) = 1 \n",
      "kernel grad (3) = 1 \n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847274 , best epoch: 28 secs per epoch: 0.035 s\n",
      "Epoch: 101, Loss: 0.847089 , best epoch: 73 secs per epoch: 0.034 s\n",
      "Epoch: 124, Loss: 0.8470 , best epoch: 73 secs per epoch: 0.034 s\n",
      "early stop!\n",
      "total time: 4.192 s\n",
      "secs per epoch: 0.034 s\n",
      "best loss 0.8469457179307938\n",
      "\n",
      "Kernel to optimize 3\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 0\n",
      "setting grad status of kernel at 1 to 0\n",
      "setting grad status of kernel at 2 to 0\n",
      "setting grad status of kernel at 3 to 1\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847022 , best epoch: 37 secs per epoch: 0.041 s\n",
      "Epoch: 101, Loss: 0.847040 , best epoch: 67 secs per epoch: 0.040 s\n",
      "Epoch: 151, Loss: 0.847305 , best epoch: 112 secs per epoch: 0.040 s\n",
      "Epoch: 163, Loss: 0.8469 , best epoch: 112 secs per epoch: 0.040 s\n",
      "early stop!\n",
      "total time: 6.517 s\n",
      "secs per epoch: 0.040 s\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847012 , best epoch: 10 secs per epoch: 0.019 s\n",
      "Epoch: 61, Loss: 0.8471 , best epoch: 10 secs per epoch: 0.019 s\n",
      "early stop!\n",
      "total time: 1.127 s\n",
      "secs per epoch: 0.019 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.846945 , best epoch: 32 secs per epoch: 0.020 s\n",
      "Epoch: 101, Loss: 0.846823 , best epoch: 58 secs per epoch: 0.020 s\n",
      "Epoch: 109, Loss: 0.8470 , best epoch: 58 secs per epoch: 0.020 s\n",
      "early stop!\n",
      "total time: 2.113 s\n",
      "secs per epoch: 0.020 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847191 , best epoch: 47 secs per epoch: 0.019 s\n",
      "Epoch: 98, Loss: 0.8470 , best epoch: 47 secs per epoch: 0.019 s\n",
      "early stop!\n",
      "total time: 1.815 s\n",
      "secs per epoch: 0.019 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.847105 , best epoch: 48 secs per epoch: 0.019 s\n",
      "Epoch: 99, Loss: 0.8469 , best epoch: 48 secs per epoch: 0.019 s\n",
      "early stop!\n",
      "total time: 1.876 s\n",
      "secs per epoch: 0.019 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            2             2      0  0.846682\n",
      "1            1             2      0  0.846797\n",
      "2            1             1      0  0.846824\n",
      "3            2             1      0  0.846835\n",
      "4            0             0      0  0.846844\n",
      "action: (2, 2, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (again)...\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            0             0      0  0.846682\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            0             0      0  0.846682\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "\n",
      "final refinement step (after shift)...\n",
      "\n",
      "unfreezing all layers for final refinement\n",
      "kernel grad (0) = 1 \n",
      "kernel grad (1) = 1 \n",
      "kernel grad (2) = 1 \n",
      "kernel grad (3) = 1 \n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.PoissonLoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.846911 , best epoch: 5 secs per epoch: 0.020 s\n",
      "Epoch: 56, Loss: 0.8469 , best epoch: 5 secs per epoch: 0.020 s\n",
      "early stop!\n",
      "total time: 1.073 s\n",
      "secs per epoch: 0.020 s\n",
      "best loss 0.8468190729618073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 74.3266 s\n",
       "File: /home/johanna/ICB/multibind/multibind/tl/prediction.py\n",
       "Function: train_iterative at line 232\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   232                                           def train_iterative(\n",
       "   233                                               train,\n",
       "   234                                               device,\n",
       "   235                                               n_kernels=4,\n",
       "   236                                               w=15,\n",
       "   237                                               # min_w=10,\n",
       "   238                                               max_w=20,\n",
       "   239                                               num_epochs=100,\n",
       "   240                                               early_stopping=15,\n",
       "   241                                               log_each=10,\n",
       "   242                                               opt_kernel_shift=True,\n",
       "   243                                               opt_kernel_length=True,\n",
       "   244                                               expand_length_max=3,\n",
       "   245                                               expand_length_step=1,\n",
       "   246                                               show_logo=False,\n",
       "   247                                               optimiser=None,\n",
       "   248                                               criterion=None,\n",
       "   249                                               seed=None,\n",
       "   250                                               init_random=False,\n",
       "   251                                               lr=0.01,\n",
       "   252                                               joint_learning=False,\n",
       "   253                                               ignore_kernel=False,\n",
       "   254                                               weight_decay=0.001,\n",
       "   255                                               stop_at_kernel=None,\n",
       "   256                                               dirichlet_regularization=0,\n",
       "   257                                               verbose=2,\n",
       "   258                                               exp_max=40,\n",
       "   259                                               shift_max=3,\n",
       "   260                                               shift_step=2,\n",
       "   261                                               **kwargs,\n",
       "   262                                           ):\n",
       "   263                                               # color for visualization of history\n",
       "   264         1          2.0      2.0      0.0      colors = [\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#984ea3\", \"#ff7f00\", \"#ffff33\", \"#a65628\"]\n",
       "   265         1          3.0      3.0      0.0      if verbose != 0:\n",
       "   266         1        136.0    136.0      0.0          print(\"next w\", w, type(w))\n",
       "   267                                           \n",
       "   268         1          4.0      4.0      0.0      if isinstance(train.dataset, mb.datasets.SelexDataset):\n",
       "   269         1          2.0      2.0      0.0          if criterion is None:\n",
       "   270         1         77.0     77.0      0.0              criterion = mb.tl.PoissonLoss()\n",
       "   271                                           \n",
       "   272         1          3.0      3.0      0.0          n_rounds = train.dataset.n_rounds\n",
       "   273         1          2.0      2.0      0.0          n_batches = train.dataset.n_batches\n",
       "   274         1          2.0      2.0      0.0          enr_series = train.dataset.enr_series\n",
       "   275         1          2.0      2.0      0.0          if verbose != 0:\n",
       "   276         1         16.0     16.0      0.0              print(\"# rounds\", n_rounds)\n",
       "   277         1         14.0     14.0      0.0              print(\"# batches\", n_batches)\n",
       "   278         1         15.0     15.0      0.0              print(\"# enr_series\", enr_series)\n",
       "   279                                           \n",
       "   280         4       7588.0   1897.0      0.0          model = mb.models.Multibind(\n",
       "   281         1          2.0      2.0      0.0              datatype=\"selex\",\n",
       "   282         1          3.0      3.0      0.0              kernels=[0] + [w] * (n_kernels - 1),\n",
       "   283         1          2.0      2.0      0.0              n_rounds=n_rounds,\n",
       "   284         1          2.0      2.0      0.0              init_random=init_random,\n",
       "   285         1          2.0      2.0      0.0              n_batches=n_batches,\n",
       "   286         1          2.0      2.0      0.0              enr_series=enr_series,\n",
       "   287         1          2.0      2.0      0.0              **kwargs,\n",
       "   288         1        780.0    780.0      0.0          ).to(device)\n",
       "   289                                               elif isinstance(train.dataset, mb.datasets.PBMDataset):\n",
       "   290                                                   if criterion is None:\n",
       "   291                                                       criterion = mb.tl.MSELoss()\n",
       "   292                                           \n",
       "   293                                                   n_proteins = train.dataset.n_proteins\n",
       "   294                                                   if verbose != 0:\n",
       "   295                                                       print(\"# proteins\", n_proteins)\n",
       "   296                                           \n",
       "   297                                                   if joint_learning:\n",
       "   298                                                       model = mb.models.Multibind(\n",
       "   299                                                           datatype=\"pbm\",\n",
       "   300                                                           kernels=[0] + [w] * (n_kernels - 1),\n",
       "   301                                                           init_random=init_random,\n",
       "   302                                                           n_batches=n_proteins,\n",
       "   303                                                           **kwargs,\n",
       "   304                                                       ).to(device)\n",
       "   305                                                   else:\n",
       "   306                                                       bm_generator = mb.models.BMCollection(n_proteins=n_proteins, n_kernels=n_kernels, init_random=init_random)\n",
       "   307                                                       model = mb.models.Multibind(\n",
       "   308                                                           datatype=\"pbm\",\n",
       "   309                                                           init_random=init_random,\n",
       "   310                                                           n_proteins=n_proteins,\n",
       "   311                                                           bm_generator=bm_generator,\n",
       "   312                                                           n_kernels=n_kernels,\n",
       "   313                                                           **kwargs,\n",
       "   314                                                       ).to(device)\n",
       "   315                                               elif isinstance(train.dataset, mb.datasets.GenomicsDataset):\n",
       "   316                                                   if criterion is None:\n",
       "   317                                                       criterion = mb.tl.MSELoss()\n",
       "   318                                           \n",
       "   319                                                   n_proteins = train.dataset.n_cells\n",
       "   320                                                   if verbose != 0:\n",
       "   321                                                       print(\"# cells\", n_proteins)\n",
       "   322                                           \n",
       "   323                                                   if joint_learning:\n",
       "   324                                                       model = mb.models.Multibind(\n",
       "   325                                                           datatype=\"pbm\",\n",
       "   326                                                           kernels=[0] + [w] * (n_kernels - 1),\n",
       "   327                                                           init_random=init_random,\n",
       "   328                                                           n_batches=n_proteins,\n",
       "   329                                                           **kwargs,\n",
       "   330                                                       ).to(device)\n",
       "   331                                                   else:\n",
       "   332                                                       bm_generator = mb.models.BMCollection(n_proteins=n_proteins, n_kernels=n_kernels, init_random=init_random)\n",
       "   333                                                       model = mb.models.Multibind(\n",
       "   334                                                           datatype=\"pbm\",\n",
       "   335                                                           init_random=init_random,\n",
       "   336                                                           n_proteins=n_proteins,\n",
       "   337                                                           bm_generator=bm_generator,\n",
       "   338                                                           n_kernels=n_kernels,\n",
       "   339                                                           **kwargs,\n",
       "   340                                                       ).to(device)\n",
       "   341                                               elif isinstance(train.dataset, mb.datasets.ResiduePBMDataset):\n",
       "   342                                                   model = mb.models.Multibind(\n",
       "   343                                                       datatype=\"pbm\",\n",
       "   344                                                       init_random=init_random,\n",
       "   345                                                       bm_generator=mb.models.BMPrediction(num_classes=1, input_size=21, hidden_size=2, num_layers=1,\n",
       "   346                                                                                           seq_length=train.dataset.get_max_residue_length()),\n",
       "   347                                                       **kwargs,\n",
       "   348                                                   ).to(device)\n",
       "   349                                               else:\n",
       "   350                                                   assert False  # not implemented yet\n",
       "   351                                           \n",
       "   352                                               # this sets up the seed at the first position\n",
       "   353         1          3.0      3.0      0.0      if seed is not None:\n",
       "   354                                                   # this sets up the seed at the first position\n",
       "   355                                                   for i, s, min_w, max_w in seed:\n",
       "   356                                                       if s is not None:\n",
       "   357                                                           print(i, s)\n",
       "   358                                                           model.set_seed(s, i, min=min_w, max=max_w)\n",
       "   359                                                   model = model.to(device)\n",
       "   360                                           \n",
       "   361                                               # step 1) freeze everything before the current binding mode\n",
       "   362         5         12.0      2.4      0.0      for i in range(0, n_kernels):\n",
       "   363         4         12.0      3.0      0.0          if verbose != 0:\n",
       "   364         4         64.0     16.0      0.0              print(\"\\nKernel to optimize %i\" % i)\n",
       "   365         4         44.0     11.0      0.0              print(\"\\nFreezing kernels\")\n",
       "   366        20         50.0      2.5      0.0          for ki in range(n_kernels):\n",
       "   367        16         38.0      2.4      0.0              if verbose != 0:\n",
       "   368        16        197.0     12.3      0.0                  print(\"setting grad status of kernel at %i to %i\" % (ki, ki == i))\n",
       "   369        16        850.0     53.1      0.0              model.update_grad(ki, ki == i)\n",
       "   370         4         50.0     12.5      0.0          print(\"\\n\")\n",
       "   371                                           \n",
       "   372         4         10.0      2.5      0.0          if show_logo:\n",
       "   373                                                       if verbose != 0:\n",
       "   374                                                           print(\"before kernel optimization.\")\n",
       "   375                                                       mb.pl.plot_activities(model, train)\n",
       "   376                                                       mb.pl.conv_mono(model)\n",
       "   377                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   378                                           \n",
       "   379         4         10.0      2.5      0.0          next_lr = lr if not isinstance(lr, list) else lr[i]\n",
       "   380         4         11.0      2.8      0.0          next_weight_decay = weight_decay if not isinstance(weight_decay, list) else weight_decay[i]\n",
       "   381                                           \n",
       "   382         4         29.0      7.2      0.0          next_optimiser = (\n",
       "   383         4        852.0    213.0      0.0              topti.Adam(model.parameters(), lr=next_lr, weight_decay=next_weight_decay)\n",
       "   384         4          9.0      2.2      0.0              if optimiser is None\n",
       "   385                                                       else optimiser(model.parameters(), lr=next_lr)\n",
       "   386                                                   )\n",
       "   387                                           \n",
       "   388                                                   # mask kernels to avoid using weights from further steps into early ones.\n",
       "   389         4         11.0      2.8      0.0          if ignore_kernel:\n",
       "   390                                                       model.set_ignore_kernel(np.array([0 for i in range(i + 1)] + [1 for i in range(i + 1, n_kernels)]))\n",
       "   391                                           \n",
       "   392         4         12.0      3.0      0.0          if verbose != 0:\n",
       "   393         4        125.0     31.2      0.0              print(\"kernels mask\", model.get_ignore_kernel())\n",
       "   394                                           \n",
       "   395                                                   # assert False\n",
       "   396         8   28291516.0 3536439.5     38.1          mb.tl.train_network(\n",
       "   397         4         10.0      2.5      0.0              model,\n",
       "   398         4         10.0      2.5      0.0              train,\n",
       "   399         4         11.0      2.8      0.0              device,\n",
       "   400         4         10.0      2.5      0.0              next_optimiser,\n",
       "   401         4         11.0      2.8      0.0              criterion,\n",
       "   402         4         12.0      3.0      0.0              num_epochs=num_epochs,\n",
       "   403         4         14.0      3.5      0.0              early_stopping=early_stopping,\n",
       "   404         4         11.0      2.8      0.0              log_each=log_each,\n",
       "   405         4         10.0      2.5      0.0              dirichlet_regularization=dirichlet_regularization,\n",
       "   406         4         10.0      2.5      0.0              exp_max=exp_max,\n",
       "   407         4         10.0      2.5      0.0              verbose=verbose,\n",
       "   408                                                   )\n",
       "   409                                                   # print('next color', colors[i])\n",
       "   410         4        394.0     98.5      0.0          model.loss_color += list(np.repeat(colors[i], len(model.loss_history) - len(model.loss_color)))\n",
       "   411                                                   # probably here load the state of the best epoch and save\n",
       "   412         4       2006.0    501.5      0.0          model.load_state_dict(model.best_model_state)\n",
       "   413         4         12.0      3.0      0.0          \"%i\" % w\n",
       "   414                                                   # store model parameters and fit for later visualization\n",
       "   415         4      27276.0   6819.0      0.0          model = copy.deepcopy(model)\n",
       "   416                                                   # optimizer for left / right flanks\n",
       "   417         4         13.0      3.2      0.0          best_loss = model.best_loss\n",
       "   418                                           \n",
       "   419         4          7.0      1.8      0.0          if show_logo:\n",
       "   420                                                       print(\"\\n##After kernel opt / before shift optim.\")\n",
       "   421                                                       mb.pl.plot_activities(model, train)\n",
       "   422                                                       mb.pl.conv_mono(model)\n",
       "   423                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   424                                                       mb.pl.plot_loss(model)\n",
       "   425                                           \n",
       "   426                                                   # print(model_by_k[k_parms].loss_color)\n",
       "   427                                                   #######\n",
       "   428                                                   # optimize the flanks through +1/-1 shifts\n",
       "   429                                                   #######\n",
       "   430         4          8.0      2.0      0.0          n_attempts = 0\n",
       "   431                                           \n",
       "   432         4         10.0      2.5      0.0          if (opt_kernel_shift or opt_kernel_length) and i != 0:\n",
       "   433                                           \n",
       "   434         3          9.0      3.0      0.0              opt_expand_left = range(1, expand_length_max, expand_length_step)\n",
       "   435         3          8.0      2.7      0.0              opt_expand_right = range(1, expand_length_max, expand_length_step)\n",
       "   436         3         10.0      3.3      0.0              opt_shift = [0] + list(range(-shift_max, shift_max + 1, shift_step))\n",
       "   437                                           \n",
       "   438        12         29.0      2.4      0.0              for opt_option_text, opt_option_next in zip(\n",
       "   439         3          8.0      2.7      0.0                  [\"FLANKS\", \"SHIFT\"], [[opt_expand_left, opt_expand_right, [0]], [[0], [0], opt_shift]]\n",
       "   440                                                       ):\n",
       "   441                                           \n",
       "   442                                                           # print(opt_option_text, opt_option_next)\n",
       "   443                                                           # assert False\n",
       "   444                                           \n",
       "   445         6         14.0      2.3      0.0                  next_loss = None\n",
       "   446        14         41.0      2.9      0.0                  while next_loss is None or next_loss < best_loss:\n",
       "   447        10         22.0      2.2      0.0                      n_attempts += 1\n",
       "   448                                           \n",
       "   449        10        226.0     22.6      0.0                      curr_w = model.get_kernel_width(i)\n",
       "   450        10         24.0      2.4      0.0                      if curr_w >= max_w:\n",
       "   451         2         27.0     13.5      0.0                          print(\"stop. Reached maximum w...\")\n",
       "   452         2          6.0      3.0      0.0                          break\n",
       "   453                                           \n",
       "   454         8        205.0     25.6      0.0                      if verbose != 0:\n",
       "   455        16        199.0     12.4      0.0                          print(\n",
       "   456         8         22.0      2.8      0.0                              \"\\noptimize_motif_shift (%s)...\" % (\"first\" if next_loss is None else \"again\"),\n",
       "   457         8         17.0      2.1      0.0                              end=\"\",\n",
       "   458                                                                   )\n",
       "   459         8         83.0     10.4      0.0                          print(\"\")\n",
       "   460         8      44892.0   5611.5      0.1                      model = copy.deepcopy(model)\n",
       "   461         8         25.0      3.1      0.0                      best_loss = model.best_loss\n",
       "   462         8         23.0      2.9      0.0                      next_color = colors[-(1 if n_attempts % 2 == 0 else -2)]\n",
       "   463                                           \n",
       "   464         8         17.0      2.1      0.0                      all_options = []\n",
       "   465                                           \n",
       "   466        16         74.0      4.6      0.0                      options = [\n",
       "   467                                                                   [expand_left, expand_right, shift]\n",
       "   468         8         18.0      2.2      0.0                          for expand_left in opt_option_next[0]\n",
       "   469                                                                   for expand_right in opt_option_next[1]\n",
       "   470                                                                   for shift in opt_option_next[2]\n",
       "   471                                                               ]\n",
       "   472                                           \n",
       "   473                                                               # print(options)\n",
       "   474                                           \n",
       "   475        42         93.0      2.2      0.0                      for expand_left, expand_right, shift in options:\n",
       "   476                                           \n",
       "   477        34         87.0      2.6      0.0                          if abs(expand_left) + abs(expand_right) + abs(shift) == 0:\n",
       "   478         2          4.0      2.0      0.0                              continue\n",
       "   479        32         69.0      2.2      0.0                          if abs(shift) > 0:  # skip shift for now.\n",
       "   480         8         15.0      1.9      0.0                              continue\n",
       "   481        24         56.0      2.3      0.0                          if curr_w + expand_left + expand_right > max_w:\n",
       "   482        11         23.0      2.1      0.0                              continue\n",
       "   483                                           \n",
       "   484                                                                   # print(expand_left, expand_right, shift)\n",
       "   485                                                                   # assert False\n",
       "   486                                           \n",
       "   487        13         28.0      2.2      0.0                          if verbose != 0:\n",
       "   488        26        170.0      6.5      0.0                              print(\n",
       "   489        26         67.0      2.6      0.0                                  \"next expand left: %i, next expand right: %i, shift: %i\"\n",
       "   490        13         28.0      2.2      0.0                                  % (expand_left, expand_right, shift)\n",
       "   491                                                                       )\n",
       "   492                                           \n",
       "   493        13      96721.0   7440.1      0.1                          model_shift = copy.deepcopy(model)\n",
       "   494        13        124.0      9.5      0.0                          model_shift.loss_history = []\n",
       "   495        13        145.0     11.2      0.0                          model_shift.loss_color = []\n",
       "   496                                           \n",
       "   497       234   38105227.0 162842.9     51.3                          mb.tl.train_modified_kernel(\n",
       "   498        13         28.0      2.2      0.0                              model_shift,\n",
       "   499        13         29.0      2.2      0.0                              train,\n",
       "   500        13         29.0      2.2      0.0                              kernel_i=i,\n",
       "   501        13         30.0      2.3      0.0                              shift=shift,\n",
       "   502        13         29.0      2.2      0.0                              expand_left=expand_left,\n",
       "   503        13         26.0      2.0      0.0                              expand_right=expand_right,\n",
       "   504        13         25.0      1.9      0.0                              device=device,\n",
       "   505        13         30.0      2.3      0.0                              num_epochs=num_epochs,\n",
       "   506        13         27.0      2.1      0.0                              early_stopping=early_stopping,\n",
       "   507        13         28.0      2.2      0.0                              log_each=log_each,\n",
       "   508        13         29.0      2.2      0.0                              update_grad_i=i,\n",
       "   509        13         31.0      2.4      0.0                              lr=next_lr,\n",
       "   510        13         32.0      2.5      0.0                              weight_decay=next_weight_decay,\n",
       "   511        13         32.0      2.5      0.0                              optimiser=optimiser,\n",
       "   512        13         29.0      2.2      0.0                              criterion=criterion,\n",
       "   513        13         27.0      2.1      0.0                              dirichlet_regularization=dirichlet_regularization,\n",
       "   514        13         27.0      2.1      0.0                              exp_max=exp_max,\n",
       "   515        13         26.0      2.0      0.0                              verbose=verbose,\n",
       "   516        13         26.0      2.0      0.0                              **kwargs,\n",
       "   517                                                                   )\n",
       "   518        13       1251.0     96.2      0.0                          model_shift.loss_color += list(np.repeat(next_color, len(model_shift.loss_history)))\n",
       "   519                                                                   # print('history left', len(model_left.loss_history))\n",
       "   520        13         42.0      3.2      0.0                          all_options.append([expand_left, expand_right, shift, model_shift, model_shift.best_loss])\n",
       "   521                                                                   # print('\\n')\n",
       "   522                                           \n",
       "   523        13         29.0      2.2      0.0                          if verbose != 0:\n",
       "   524        13        163.0     12.5      0.0                              print(\"after opt.\")\n",
       "   525        13         34.0      2.6      0.0                              if show_logo:\n",
       "   526                                                                           mb.pl.conv_mono(model_shift)\n",
       "   527                                           \n",
       "   528                                                               # for shift, model_shift, loss in all_shifts:\n",
       "   529                                                               #     print('shift=%i' % shift, 'loss=%.4f' % loss)\n",
       "   530        16        718.0     44.9      0.0                      best = sorted(\n",
       "   531         8         22.0      2.8      0.0                          all_options + [[0, 0, 0, model, best_loss]],\n",
       "   532         8         17.0      2.1      0.0                          key=lambda x: x[-1],\n",
       "   533                                                               )\n",
       "   534         8         18.0      2.2      0.0                      if verbose != 0:\n",
       "   535         8         93.0     11.6      0.0                          print(\"sorted\")\n",
       "   536        16       4658.0    291.1      0.0                      best_df = pd.DataFrame(\n",
       "   537        16         51.0      3.2      0.0                          [\n",
       "   538                                                                       [expand_left, expand_right, shift, loss]\n",
       "   539         8         15.0      1.9      0.0                              for expand_left, expand_right, shift, model_shift, loss in best\n",
       "   540                                                                   ],\n",
       "   541         8         18.0      2.2      0.0                          columns=[\"expand.left\", \"expand.right\", \"shift\", \"loss\"],\n",
       "   542                                                               )\n",
       "   543         8         20.0      2.5      0.0                      if verbose != 0:\n",
       "   544         8      32611.0   4076.4      0.0                          print(best_df.sort_values(\"loss\"))\n",
       "   545                                                               # for shift, model_shift, loss in best:\n",
       "   546                                                               #     print('shift=%i' % shift, 'loss=%.4f' % loss)\n",
       "   547                                           \n",
       "   548                                                               # print('\\n history len')\n",
       "   549         8        165.0     20.6      0.0                      next_expand_left, next_expand_right, next_position, next_model, next_loss = best[0]\n",
       "   550         8         16.0      2.0      0.0                      if verbose != 0:\n",
       "   551         8         98.0     12.2      0.0                          print(\"action: %s\\n\" % str((next_expand_left, next_expand_right, next_position)))\n",
       "   552                                           \n",
       "   553         8         18.0      2.2      0.0                      if next_position != 0:\n",
       "   554                                                                   next_model.loss_history = model.loss_history + next_model.loss_history\n",
       "   555                                                                   next_model.loss_color = model.loss_color + next_model.loss_color\n",
       "   556                                           \n",
       "   557         8      39315.0   4914.4      0.1                      model = copy.deepcopy(next_model)\n",
       "   558                                           \n",
       "   559         4          9.0      2.2      0.0          if show_logo:\n",
       "   560                                                       if verbose != 0:\n",
       "   561                                                           print(\"after shift optimz model\")\n",
       "   562                                                       mb.pl.plot_activities(model, train)\n",
       "   563                                                       mb.pl.conv_mono(model)\n",
       "   564                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   565                                                       mb.pl.plot_loss(model)\n",
       "   566                                                       print(\"\")\n",
       "   567                                           \n",
       "   568                                                   # the first kernel does not require an additional fit.\n",
       "   569         4          8.0      2.0      0.0          if i == 0:\n",
       "   570         1          4.0      4.0      0.0              continue\n",
       "   571                                           \n",
       "   572         3          7.0      2.3      0.0          if verbose != 0:\n",
       "   573         3         33.0     11.0      0.0              print(\"\\n\\nfinal refinement step (after shift)...\")\n",
       "   574         3         28.0      9.3      0.0              print(\"\\nunfreezing all layers for final refinement\")\n",
       "   575                                           \n",
       "   576        15         34.0      2.3      0.0          for ki in range(n_kernels):\n",
       "   577        12         23.0      1.9      0.0              if verbose != 0:\n",
       "   578        12        123.0     10.2      0.0                  print(\"kernel grad (%i) = %i \\n\" % (ki, True), sep=\", \", end=\"\")\n",
       "   579        12        510.0     42.5      0.0              model.update_grad(ki, ki == i)\n",
       "   580         3          6.0      2.0      0.0          if verbose != 0:\n",
       "   581         3         30.0     10.0      0.0              print(\"\")\n",
       "   582                                           \n",
       "   583         3         39.0     13.0      0.0          next_optimiser = (\n",
       "   584         3        479.0    159.7      0.0              topti.Adam(model.parameters(), lr=next_lr, weight_decay=next_weight_decay)\n",
       "   585         3          6.0      2.0      0.0              if optimiser is None\n",
       "   586                                                       else optimiser(model.parameters(), lr=next_lr)\n",
       "   587                                                   )\n",
       "   588                                           \n",
       "   589                                                   # mask kernels to avoid using weights from further steps into early ones.\n",
       "   590         3          6.0      2.0      0.0          if ignore_kernel:\n",
       "   591                                                       model.set_ignore_kernel(np.array([0 for i in range(i + 1)] + [1 for i in range(i + 1, n_kernels)]))\n",
       "   592         3          5.0      1.7      0.0          if verbose != 0:\n",
       "   593         3         66.0     22.0      0.0              print(\"kernels mask\", model.get_ignore_kernel())\n",
       "   594                                                   # assert False\n",
       "   595         6    7662182.0 1277030.3     10.3          mb.tl.train_network(\n",
       "   596         3          6.0      2.0      0.0              model,\n",
       "   597         3          7.0      2.3      0.0              train,\n",
       "   598         3          6.0      2.0      0.0              device,\n",
       "   599         3          5.0      1.7      0.0              next_optimiser,\n",
       "   600         3          6.0      2.0      0.0              criterion,\n",
       "   601         3          6.0      2.0      0.0              num_epochs=num_epochs,\n",
       "   602         3          5.0      1.7      0.0              early_stopping=early_stopping,\n",
       "   603         3          7.0      2.3      0.0              log_each=log_each,\n",
       "   604         3          7.0      2.3      0.0              dirichlet_regularization=dirichlet_regularization,\n",
       "   605         3          6.0      2.0      0.0              verbose=verbose,\n",
       "   606                                                   )\n",
       "   607                                           \n",
       "   608                                                   # load the best model after the final refinement\n",
       "   609         3        191.0     63.7      0.0          model.loss_color += list(np.repeat(colors[i], len(model.loss_history) - len(model.loss_color)))\n",
       "   610         3       1506.0    502.0      0.0          model.load_state_dict(model.best_model_state)\n",
       "   611                                           \n",
       "   612         3          7.0      2.3      0.0          if stop_at_kernel is not None and stop_at_kernel == i:\n",
       "   613                                                       break\n",
       "   614                                           \n",
       "   615         3          6.0      2.0      0.0          if show_logo:\n",
       "   616                                                       print(\"\\n##final motif signal (after final refinement)\")\n",
       "   617                                                       mb.pl.plot_activities(model, train)\n",
       "   618                                                       mb.pl.conv_mono(model)\n",
       "   619                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   620                                                       # mb.pl.plot_loss(model)\n",
       "   621                                           \n",
       "   622         3         71.0     23.7      0.0          print('best loss', model.best_loss)\n",
       "   623                                                   # if i == 1:\n",
       "   624                                                   #     assert False\n",
       "   625                                           \n",
       "   626                                               # r = [k_parms, w, n_feat, l_best]\n",
       "   627                                               # # print(r)\n",
       "   628                                               # res.append(r)\n",
       "   629                                           \n",
       "   630         1          2.0      2.0      0.0      return model, model.best_loss"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f mb.tl.train_iterative model, best_loss = mb.tl.train_iterative(train, device, num_epochs=500, show_logo=False, early_stopping=50, log_each=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67021cc8-1ebc-4891-a039-8e10ae1d9d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since natually it takes most of the time to train the model, I'll profile the forward method on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ed80104-27a7-4a00-84ab-40ce46779d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_rev = train.dataset.store_rev\n",
    "i, batch = enumerate(train).__next__()\n",
    "mononuc = batch[\"mononuc\"].to(device)\n",
    "b = batch[\"batch\"].to(device) if \"batch\" in batch else None\n",
    "rounds = batch[\"rounds\"].to(device) if \"rounds\" in batch else None\n",
    "countsum = batch[\"countsum\"].to(device) if \"countsum\" in batch else None\n",
    "residues = batch[\"residues\"].to(device) if \"residues\" in batch else None\n",
    "protein_id = batch[\"protein_id\"].to(device) if \"protein_id\" in batch else None\n",
    "inputs = {\"mono\": mononuc, \"batch\": b, \"countsum\": countsum}\n",
    "if store_rev:\n",
    "    mononuc_rev = batch[\"mononuc_rev\"].to(device)\n",
    "    inputs[\"mono_rev\"] = mononuc_rev\n",
    "if residues is not None:\n",
    "    inputs[\"residues\"] = residues\n",
    "if protein_id is not None:\n",
    "    inputs[\"protein_id\"] = protein_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24da93c7-d6c3-4be4-aa3a-9f1f3b30c660",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.001917 s\n",
       "File: /home/johanna/ICB/multibind/multibind/models/models.py\n",
       "Function: forward at line 81\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    81                                               def forward(self, mono, **kwargs):\n",
       "    82                                                   # mono_rev=None, di=None, di_rev=None, batch=None, countsum=None, residues=None, protein_id=None):\n",
       "    83         1          1.0      1.0      0.1          mono_rev = kwargs.get(\"mono_rev\", None)\n",
       "    84         1          1.0      1.0      0.1          di = kwargs.get(\"di\", None)\n",
       "    85         1          1.0      1.0      0.1          di_rev = kwargs.get(\"di_rev\", None)\n",
       "    86         1        120.0    120.0      6.3          mono = self.padding(mono)\n",
       "    87         1          1.0      1.0      0.1          if mono_rev is None:\n",
       "    88         1        110.0    110.0      5.7              mono_rev = mb.tl.mono2revmono(mono)\n",
       "    89                                                   else:\n",
       "    90                                                       mono_rev = self.padding(mono_rev)\n",
       "    91                                           \n",
       "    92                                                   # prepare the dinucleotide objects if we need them\n",
       "    93         1          1.0      1.0      0.1          if self.use_dinuc:\n",
       "    94                                                       if di is None:\n",
       "    95                                                           di = mb.tl.mono2dinuc(mono)\n",
       "    96                                                       if di_rev is None:\n",
       "    97                                                           di_rev = mb.tl.mono2dinuc(mono_rev)\n",
       "    98                                                       di = torch.unsqueeze(di, 1)\n",
       "    99                                                       di_rev = torch.unsqueeze(di_rev, 1)\n",
       "   100                                                       kwargs[\"di\"] = di\n",
       "   101                                                       kwargs[\"di_rev\"] = di_rev\n",
       "   102                                           \n",
       "   103                                                   # unsqueeze mono after preparing di and unsqueezing mono\n",
       "   104         1          7.0      7.0      0.4          mono_rev = torch.unsqueeze(mono_rev, 1)\n",
       "   105         1          2.0      2.0      0.1          mono = torch.unsqueeze(mono, 1)\n",
       "   106                                           \n",
       "   107                                                   # binding_per_mode: matrix of size [batchsize, number of binding modes]\n",
       "   108         1       1396.0   1396.0     72.8          binding_per_mode = self.binding_modes(mono=mono, mono_rev=mono_rev, **kwargs)\n",
       "   109         1        149.0    149.0      7.8          binding_scores = self.activities(binding_per_mode, **kwargs)\n",
       "   110                                           \n",
       "   111         1          1.0      1.0      0.1          if self.datatype == \"pbm\":\n",
       "   112                                                       return binding_scores\n",
       "   113         1          1.0      1.0      0.1          elif self.datatype == \"selex\":\n",
       "   114         1        126.0    126.0      6.6              return self.selex_module(binding_scores, **kwargs)\n",
       "   115                                                   else:\n",
       "   116                                                       return None  # this line should never be called"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f model.forward model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c111c-7e53-4488-a02b-a265e8729533",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "349cc672-2c4c-413f-b7ca-226e5708a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_path = os.path.join(bd.constants.ANNOTATIONS_DIRECTORY, 'pbm', 'affreg', 'PbmDataHom6_norm.mat')\n",
    "mat = scipy.io.loadmat(matlab_path)\n",
    "data = mat['PbmData'][0]\n",
    "seqs_dna =  data[0][5]\n",
    "seqs_dna = [s[0][0] for s in seqs_dna]\n",
    "# load the MSA sequences, one hot encoded\n",
    "df, signal = bd.datasets.PBM.pbm_homeo_affreg()\n",
    "# x, y = pickle.load(open('../../data/example_homeo_PbmData.pkl', 'rb'))\n",
    "# x, y = pickle.load(open('annotations/pbm/example_homeo_PbmData.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49dc5ecc-344d-41a2-8db9-0d1d6b171d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a small subsample\n",
    "# x = x[1:6]\n",
    "seqs_dna = seqs_dna[0:1000]\n",
    "signal = signal[0:1, 0:1000]\n",
    "\n",
    "# shift signal by adding a constant s.t. no negative values are included\n",
    "signal -= np.min(signal)\n",
    "\n",
    "# Set up the dataset\n",
    "df = pd.DataFrame(signal.T)\n",
    "df['seq'] = seqs_dna\n",
    "df.index = df['seq']\n",
    "del df['seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2df2296a-20ea-42ca-964c-8874bb77a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = mb.datasets.PBMDataset(df)\n",
    "train = tdata.DataLoader(dataset=dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feaba1b3-f5b1-4e76-af1f-38e94ff0175d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next w 15 <class 'int'>\n",
      "# proteins 1\n",
      "\n",
      "Kernel to optimize 0\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 1\n",
      "setting grad status of kernel at 1 to 0\n",
      "setting grad status of kernel at 2 to 0\n",
      "setting grad status of kernel at 3 to 0\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 175706.113281 , best epoch: 49 secs per epoch: 0.075 s\n",
      "Epoch: 101, Loss: 175599.359375 , best epoch: 99 secs per epoch: 0.075 s\n",
      "Epoch: 151, Loss: 175561.937500 , best epoch: 149 secs per epoch: 0.075 s\n",
      "Epoch: 201, Loss: 175543.156250 , best epoch: 199 secs per epoch: 0.075 s\n",
      "Epoch: 251, Loss: 175532.582031 , best epoch: 248 secs per epoch: 0.075 s\n",
      "Epoch: 301, Loss: 175526.265625 , best epoch: 295 secs per epoch: 0.075 s\n",
      "Epoch: 351, Loss: 175521.917969 , best epoch: 347 secs per epoch: 0.076 s\n",
      "Epoch: 401, Loss: 175517.859375 , best epoch: 386 secs per epoch: 0.076 s\n",
      "Epoch: 451, Loss: 175517.214844 , best epoch: 437 secs per epoch: 0.076 s\n",
      "total time: 37.510 s\n",
      "secs per epoch: 0.075 s\n",
      "\n",
      "Kernel to optimize 1\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 0\n",
      "setting grad status of kernel at 1 to 1\n",
      "setting grad status of kernel at 2 to 0\n",
      "setting grad status of kernel at 3 to 0\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 78328.748047 , best epoch: 49 secs per epoch: 0.091 s\n",
      "Epoch: 101, Loss: 78012.832031 , best epoch: 99 secs per epoch: 0.090 s\n",
      "Epoch: 151, Loss: 77922.695312 , best epoch: 149 secs per epoch: 0.090 s\n",
      "Epoch: 201, Loss: 77882.941406 , best epoch: 199 secs per epoch: 0.089 s\n",
      "Epoch: 251, Loss: 77860.757812 , best epoch: 248 secs per epoch: 0.089 s\n",
      "Epoch: 301, Loss: 77847.460938 , best epoch: 297 secs per epoch: 0.090 s\n",
      "Epoch: 351, Loss: 77839.037109 , best epoch: 349 secs per epoch: 0.090 s\n",
      "Epoch: 401, Loss: 77833.353516 , best epoch: 395 secs per epoch: 0.090 s\n",
      "Epoch: 451, Loss: 77829.177734 , best epoch: 447 secs per epoch: 0.090 s\n",
      "total time: 44.613 s\n",
      "secs per epoch: 0.089 s\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 77811.222656 , best epoch: 38 secs per epoch: 0.090 s\n",
      "Epoch: 101, Loss: 77810.373047 , best epoch: 84 secs per epoch: 0.089 s\n",
      "Epoch: 135, Loss: 77809.8555 , best epoch: 84 secs per epoch: 0.089 s\n",
      "early stop!\n",
      "total time: 11.931 s\n",
      "secs per epoch: 0.089 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 77810.865234 , best epoch: 12 secs per epoch: 0.090 s\n",
      "Epoch: 101, Loss: 77810.171875 , best epoch: 55 secs per epoch: 0.089 s\n",
      "Epoch: 106, Loss: 77810.3047 , best epoch: 55 secs per epoch: 0.089 s\n",
      "early stop!\n",
      "total time: 9.310 s\n",
      "secs per epoch: 0.089 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 77810.712891 , best epoch: 23 secs per epoch: 0.090 s\n",
      "Epoch: 101, Loss: 77809.429688 , best epoch: 87 secs per epoch: 0.089 s\n",
      "Epoch: 151, Loss: 77809.843750 , best epoch: 100 secs per epoch: 0.089 s\n",
      "Epoch: 151, Loss: 77809.8438 , best epoch: 100 secs per epoch: 0.089 s\n",
      "early stop!\n",
      "total time: 13.293 s\n",
      "secs per epoch: 0.089 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 77811.263672 , best epoch: 36 secs per epoch: 0.090 s\n",
      "Epoch: 87, Loss: 77810.3594 , best epoch: 36 secs per epoch: 0.090 s\n",
      "early stop!\n",
      "total time: 7.705 s\n",
      "secs per epoch: 0.090 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift          loss\n",
      "0            2             1      0  77809.429688\n",
      "1            2             2      0  77809.574219\n",
      "2            1             1      0  77809.619141\n",
      "3            1             2      0  77809.832031\n",
      "4            0             0      0  77825.404297\n",
      "action: (2, 1, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (again)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 77810.873047 , best epoch: 2 secs per epoch: 0.090 s\n",
      "Epoch: 53, Loss: 77810.8379 , best epoch: 2 secs per epoch: 0.090 s\n",
      "early stop!\n",
      "total time: 4.696 s\n",
      "secs per epoch: 0.090 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift          loss\n",
      "0            0             0      0  77809.429688\n",
      "1            1             1      0  77809.521484\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "sorted\n",
      "   expand.left  expand.right  shift          loss\n",
      "0            0             0      0  77809.429688\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "\n",
      "final refinement step (after shift)...\n",
      "\n",
      "unfreezing all layers for final refinement\n",
      "kernel grad (0) = 1 \n",
      "kernel grad (1) = 1 \n",
      "kernel grad (2) = 1 \n",
      "kernel grad (3) = 1 \n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 77810.187500 , best epoch: 17 secs per epoch: 0.090 s\n",
      "Epoch: 68, Loss: 77810.3594 , best epoch: 17 secs per epoch: 0.090 s\n",
      "early stop!\n",
      "total time: 6.017 s\n",
      "secs per epoch: 0.090 s\n",
      "best loss 77809.56640625\n",
      "\n",
      "Kernel to optimize 2\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 0\n",
      "setting grad status of kernel at 1 to 0\n",
      "setting grad status of kernel at 2 to 1\n",
      "setting grad status of kernel at 3 to 0\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 19627.455078 , best epoch: 49 secs per epoch: 0.095 s\n",
      "Epoch: 101, Loss: 19433.601562 , best epoch: 99 secs per epoch: 0.092 s\n",
      "Epoch: 151, Loss: 19376.639648 , best epoch: 149 secs per epoch: 0.091 s\n",
      "Epoch: 201, Loss: 19351.877930 , best epoch: 199 secs per epoch: 0.091 s\n",
      "Epoch: 251, Loss: 19337.731934 , best epoch: 249 secs per epoch: 0.090 s\n",
      "Epoch: 301, Loss: 19329.761719 , best epoch: 299 secs per epoch: 0.090 s\n",
      "Epoch: 351, Loss: 19324.344727 , best epoch: 349 secs per epoch: 0.090 s\n",
      "Epoch: 401, Loss: 19320.347656 , best epoch: 397 secs per epoch: 0.089 s\n",
      "Epoch: 451, Loss: 19317.702637 , best epoch: 448 secs per epoch: 0.089 s\n",
      "total time: 44.485 s\n",
      "secs per epoch: 0.089 s\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 19306.052246 , best epoch: 45 secs per epoch: 0.091 s\n",
      "Epoch: 101, Loss: 19305.939453 , best epoch: 62 secs per epoch: 0.090 s\n",
      "Epoch: 113, Loss: 19306.2573 , best epoch: 62 secs per epoch: 0.090 s\n",
      "early stop!\n",
      "total time: 10.035 s\n",
      "secs per epoch: 0.090 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 19306.239258 , best epoch: 49 secs per epoch: 0.091 s\n",
      "Epoch: 100, Loss: 19305.9951 , best epoch: 49 secs per epoch: 0.090 s\n",
      "early stop!\n",
      "total time: 8.912 s\n",
      "secs per epoch: 0.090 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 19306.004395 , best epoch: 35 secs per epoch: 0.094 s\n",
      "Epoch: 101, Loss: 19306.003418 , best epoch: 91 secs per epoch: 0.093 s\n",
      "Epoch: 151, Loss: 19306.130371 , best epoch: 129 secs per epoch: 0.092 s\n",
      "Epoch: 180, Loss: 19305.9580 , best epoch: 129 secs per epoch: 0.092 s\n",
      "early stop!\n",
      "total time: 16.429 s\n",
      "secs per epoch: 0.092 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 19306.009277 , best epoch: 44 secs per epoch: 0.093 s\n",
      "Epoch: 95, Loss: 19305.9756 , best epoch: 44 secs per epoch: 0.091 s\n",
      "early stop!\n",
      "total time: 8.568 s\n",
      "secs per epoch: 0.091 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift          loss\n",
      "0            2             1      0  19305.230957\n",
      "1            1             1      0  19305.424805\n",
      "2            2             2      0  19305.433105\n",
      "3            1             2      0  19305.562988\n",
      "4            0             0      0  19315.554199\n",
      "action: (2, 1, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (again)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 19306.112305 , best epoch: 23 secs per epoch: 0.091 s\n",
      "Epoch: 74, Loss: 19306.2036 , best epoch: 23 secs per epoch: 0.090 s\n",
      "early stop!\n",
      "total time: 6.604 s\n",
      "secs per epoch: 0.090 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift          loss\n",
      "0            0             0      0  19305.230957\n",
      "1            1             1      0  19305.511719\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "sorted\n",
      "   expand.left  expand.right  shift          loss\n",
      "0            0             0      0  19305.230957\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "\n",
      "final refinement step (after shift)...\n",
      "\n",
      "unfreezing all layers for final refinement\n",
      "kernel grad (0) = 1 \n",
      "kernel grad (1) = 1 \n",
      "kernel grad (2) = 1 \n",
      "kernel grad (3) = 1 \n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 19306.210938 , best epoch: 37 secs per epoch: 0.091 s\n",
      "Epoch: 88, Loss: 19305.9790 , best epoch: 37 secs per epoch: 0.090 s\n",
      "early stop!\n",
      "total time: 7.831 s\n",
      "secs per epoch: 0.090 s\n",
      "best loss 19305.28955078125\n",
      "\n",
      "Kernel to optimize 3\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 0\n",
      "setting grad status of kernel at 1 to 0\n",
      "setting grad status of kernel at 2 to 0\n",
      "setting grad status of kernel at 3 to 1\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 24.635372 , best epoch: 49 secs per epoch: 0.091 s\n",
      "Epoch: 101, Loss: 9.111907 , best epoch: 99 secs per epoch: 0.090 s\n",
      "Epoch: 151, Loss: 4.564687 , best epoch: 149 secs per epoch: 0.090 s\n",
      "Epoch: 201, Loss: 2.676018 , best epoch: 199 secs per epoch: 0.089 s\n",
      "Epoch: 251, Loss: 1.728382 , best epoch: 249 secs per epoch: 0.089 s\n",
      "Epoch: 301, Loss: 1.204736 , best epoch: 299 secs per epoch: 0.089 s\n",
      "Epoch: 351, Loss: 0.884836 , best epoch: 349 secs per epoch: 0.089 s\n",
      "Epoch: 401, Loss: 0.687826 , best epoch: 398 secs per epoch: 0.089 s\n",
      "Epoch: 451, Loss: 0.558140 , best epoch: 449 secs per epoch: 0.089 s\n",
      "total time: 44.476 s\n",
      "secs per epoch: 0.089 s\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.184005 , best epoch: 44 secs per epoch: 0.092 s\n",
      "Epoch: 101, Loss: 0.182773 , best epoch: 98 secs per epoch: 0.090 s\n",
      "Epoch: 151, Loss: 0.177686 , best epoch: 149 secs per epoch: 0.090 s\n",
      "Epoch: 201, Loss: 0.166536 , best epoch: 199 secs per epoch: 0.090 s\n",
      "Epoch: 251, Loss: 0.168201 , best epoch: 243 secs per epoch: 0.090 s\n",
      "Epoch: 301, Loss: 0.165232 , best epoch: 293 secs per epoch: 0.090 s\n",
      "Epoch: 351, Loss: 0.158103 , best epoch: 343 secs per epoch: 0.090 s\n",
      "Epoch: 401, Loss: 0.153008 , best epoch: 398 secs per epoch: 0.090 s\n",
      "Epoch: 451, Loss: 0.146514 , best epoch: 449 secs per epoch: 0.090 s\n",
      "total time: 44.755 s\n",
      "secs per epoch: 0.090 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.183964 , best epoch: 39 secs per epoch: 0.092 s\n",
      "Epoch: 101, Loss: 0.183753 , best epoch: 91 secs per epoch: 0.091 s\n",
      "Epoch: 151, Loss: 0.173569 , best epoch: 145 secs per epoch: 0.090 s\n",
      "Epoch: 201, Loss: 0.167701 , best epoch: 197 secs per epoch: 0.090 s\n",
      "Epoch: 251, Loss: 0.149633 , best epoch: 249 secs per epoch: 0.091 s\n",
      "Epoch: 301, Loss: 0.115750 , best epoch: 297 secs per epoch: 0.091 s\n",
      "Epoch: 351, Loss: 0.115564 , best epoch: 346 secs per epoch: 0.091 s\n",
      "Epoch: 401, Loss: 0.111053 , best epoch: 377 secs per epoch: 0.091 s\n",
      "Epoch: 451, Loss: 0.112408 , best epoch: 431 secs per epoch: 0.091 s\n",
      "total time: 45.479 s\n",
      "secs per epoch: 0.091 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.183692 , best epoch: 29 secs per epoch: 0.092 s\n",
      "Epoch: 101, Loss: 0.181914 , best epoch: 94 secs per epoch: 0.092 s\n",
      "Epoch: 151, Loss: 0.174493 , best epoch: 149 secs per epoch: 0.092 s\n",
      "Epoch: 201, Loss: 0.165547 , best epoch: 196 secs per epoch: 0.092 s\n",
      "Epoch: 251, Loss: 0.154061 , best epoch: 249 secs per epoch: 0.091 s\n",
      "Epoch: 301, Loss: 0.120234 , best epoch: 297 secs per epoch: 0.091 s\n",
      "Epoch: 351, Loss: 0.112756 , best epoch: 345 secs per epoch: 0.091 s\n",
      "Epoch: 401, Loss: 0.111910 , best epoch: 398 secs per epoch: 0.091 s\n",
      "Epoch: 451, Loss: 0.114190 , best epoch: 449 secs per epoch: 0.092 s\n",
      "total time: 45.819 s\n",
      "secs per epoch: 0.092 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.183667 , best epoch: 31 secs per epoch: 0.094 s\n",
      "Epoch: 101, Loss: 0.187983 , best epoch: 88 secs per epoch: 0.093 s\n",
      "Epoch: 151, Loss: 0.167119 , best epoch: 149 secs per epoch: 0.093 s\n",
      "Epoch: 201, Loss: 0.161584 , best epoch: 196 secs per epoch: 0.093 s\n",
      "Epoch: 251, Loss: 0.152430 , best epoch: 248 secs per epoch: 0.092 s\n",
      "Epoch: 301, Loss: 0.153801 , best epoch: 286 secs per epoch: 0.092 s\n",
      "Epoch: 351, Loss: 0.149932 , best epoch: 348 secs per epoch: 0.092 s\n",
      "Epoch: 401, Loss: 0.118711 , best epoch: 399 secs per epoch: 0.093 s\n",
      "Epoch: 451, Loss: 0.104259 , best epoch: 448 secs per epoch: 0.093 s\n",
      "total time: 46.280 s\n",
      "secs per epoch: 0.093 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            2             2      0  0.089777\n",
      "1            1             2      0  0.108524\n",
      "2            2             1      0  0.109129\n",
      "3            1             1      0  0.129172\n",
      "4            0             0      0  0.473727\n",
      "action: (2, 2, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (again)...\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            0             0      0  0.089777\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            0             0      0  0.089777\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "\n",
      "final refinement step (after shift)...\n",
      "\n",
      "unfreezing all layers for final refinement\n",
      "kernel grad (0) = 1 \n",
      "kernel grad (1) = 1 \n",
      "kernel grad (2) = 1 \n",
      "kernel grad (3) = 1 \n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.086619 , best epoch: 47 secs per epoch: 0.097 s\n",
      "Epoch: 101, Loss: 0.085002 , best epoch: 91 secs per epoch: 0.094 s\n",
      "Epoch: 151, Loss: 0.084027 , best epoch: 149 secs per epoch: 0.093 s\n",
      "Epoch: 201, Loss: 0.083602 , best epoch: 196 secs per epoch: 0.093 s\n",
      "Epoch: 251, Loss: 0.082636 , best epoch: 245 secs per epoch: 0.092 s\n",
      "Epoch: 296, Loss: 0.0827 , best epoch: 245 secs per epoch: 0.092 s\n",
      "early stop!\n",
      "total time: 27.260 s\n",
      "secs per epoch: 0.092 s\n",
      "best loss 0.08190999180078506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 492.297 s\n",
       "File: /home/johanna/ICB/multibind/multibind/tl/prediction.py\n",
       "Function: train_iterative at line 232\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   232                                           def train_iterative(\n",
       "   233                                               train,\n",
       "   234                                               device,\n",
       "   235                                               n_kernels=4,\n",
       "   236                                               w=15,\n",
       "   237                                               # min_w=10,\n",
       "   238                                               max_w=20,\n",
       "   239                                               num_epochs=100,\n",
       "   240                                               early_stopping=15,\n",
       "   241                                               log_each=10,\n",
       "   242                                               opt_kernel_shift=True,\n",
       "   243                                               opt_kernel_length=True,\n",
       "   244                                               expand_length_max=3,\n",
       "   245                                               expand_length_step=1,\n",
       "   246                                               show_logo=False,\n",
       "   247                                               optimiser=None,\n",
       "   248                                               criterion=None,\n",
       "   249                                               seed=None,\n",
       "   250                                               init_random=False,\n",
       "   251                                               lr=0.01,\n",
       "   252                                               joint_learning=False,\n",
       "   253                                               ignore_kernel=False,\n",
       "   254                                               weight_decay=0.001,\n",
       "   255                                               stop_at_kernel=None,\n",
       "   256                                               dirichlet_regularization=0,\n",
       "   257                                               verbose=2,\n",
       "   258                                               exp_max=40,\n",
       "   259                                               shift_max=3,\n",
       "   260                                               shift_step=2,\n",
       "   261                                               **kwargs,\n",
       "   262                                           ):\n",
       "   263                                               # color for visualization of history\n",
       "   264         1          3.0      3.0      0.0      colors = [\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#984ea3\", \"#ff7f00\", \"#ffff33\", \"#a65628\"]\n",
       "   265         1          2.0      2.0      0.0      if verbose != 0:\n",
       "   266         1        110.0    110.0      0.0          print(\"next w\", w, type(w))\n",
       "   267                                           \n",
       "   268         1          3.0      3.0      0.0      if isinstance(train.dataset, mb.datasets.SelexDataset):\n",
       "   269                                                   if criterion is None:\n",
       "   270                                                       criterion = mb.tl.PoissonLoss()\n",
       "   271                                           \n",
       "   272                                                   n_rounds = train.dataset.n_rounds\n",
       "   273                                                   n_batches = train.dataset.n_batches\n",
       "   274                                                   enr_series = train.dataset.enr_series\n",
       "   275                                                   if verbose != 0:\n",
       "   276                                                       print(\"# rounds\", n_rounds)\n",
       "   277                                                       print(\"# batches\", n_batches)\n",
       "   278                                                       print(\"# enr_series\", enr_series)\n",
       "   279                                           \n",
       "   280                                                   model = mb.models.Multibind(\n",
       "   281                                                       datatype=\"selex\",\n",
       "   282                                                       kernels=[0] + [w] * (n_kernels - 1),\n",
       "   283                                                       n_rounds=n_rounds,\n",
       "   284                                                       init_random=init_random,\n",
       "   285                                                       n_batches=n_batches,\n",
       "   286                                                       enr_series=enr_series,\n",
       "   287                                                       **kwargs,\n",
       "   288                                                   ).to(device)\n",
       "   289         1          2.0      2.0      0.0      elif isinstance(train.dataset, mb.datasets.PBMDataset):\n",
       "   290         1          2.0      2.0      0.0          if criterion is None:\n",
       "   291         1         58.0     58.0      0.0              criterion = mb.tl.MSELoss()\n",
       "   292                                           \n",
       "   293         1          2.0      2.0      0.0          n_proteins = train.dataset.n_proteins\n",
       "   294         1          2.0      2.0      0.0          if verbose != 0:\n",
       "   295         1         15.0     15.0      0.0              print(\"# proteins\", n_proteins)\n",
       "   296                                           \n",
       "   297         1          2.0      2.0      0.0          if joint_learning:\n",
       "   298                                                       model = mb.models.Multibind(\n",
       "   299                                                           datatype=\"pbm\",\n",
       "   300                                                           kernels=[0] + [w] * (n_kernels - 1),\n",
       "   301                                                           init_random=init_random,\n",
       "   302                                                           n_batches=n_proteins,\n",
       "   303                                                           **kwargs,\n",
       "   304                                                       ).to(device)\n",
       "   305                                                   else:\n",
       "   306         1        934.0    934.0      0.0              bm_generator = mb.models.BMCollection(n_proteins=n_proteins, n_kernels=n_kernels, init_random=init_random)\n",
       "   307         4        391.0     97.8      0.0              model = mb.models.Multibind(\n",
       "   308         1          2.0      2.0      0.0                  datatype=\"pbm\",\n",
       "   309         1          2.0      2.0      0.0                  init_random=init_random,\n",
       "   310         1          2.0      2.0      0.0                  n_proteins=n_proteins,\n",
       "   311         1          1.0      1.0      0.0                  bm_generator=bm_generator,\n",
       "   312         1          2.0      2.0      0.0                  n_kernels=n_kernels,\n",
       "   313         1          2.0      2.0      0.0                  **kwargs,\n",
       "   314         1        147.0    147.0      0.0              ).to(device)\n",
       "   315                                               elif isinstance(train.dataset, mb.datasets.GenomicsDataset):\n",
       "   316                                                   if criterion is None:\n",
       "   317                                                       criterion = mb.tl.MSELoss()\n",
       "   318                                           \n",
       "   319                                                   n_proteins = train.dataset.n_cells\n",
       "   320                                                   if verbose != 0:\n",
       "   321                                                       print(\"# cells\", n_proteins)\n",
       "   322                                           \n",
       "   323                                                   if joint_learning:\n",
       "   324                                                       model = mb.models.Multibind(\n",
       "   325                                                           datatype=\"pbm\",\n",
       "   326                                                           kernels=[0] + [w] * (n_kernels - 1),\n",
       "   327                                                           init_random=init_random,\n",
       "   328                                                           n_batches=n_proteins,\n",
       "   329                                                           **kwargs,\n",
       "   330                                                       ).to(device)\n",
       "   331                                                   else:\n",
       "   332                                                       bm_generator = mb.models.BMCollection(n_proteins=n_proteins, n_kernels=n_kernels, init_random=init_random)\n",
       "   333                                                       model = mb.models.Multibind(\n",
       "   334                                                           datatype=\"pbm\",\n",
       "   335                                                           init_random=init_random,\n",
       "   336                                                           n_proteins=n_proteins,\n",
       "   337                                                           bm_generator=bm_generator,\n",
       "   338                                                           n_kernels=n_kernels,\n",
       "   339                                                           **kwargs,\n",
       "   340                                                       ).to(device)\n",
       "   341                                               elif isinstance(train.dataset, mb.datasets.ResiduePBMDataset):\n",
       "   342                                                   model = mb.models.Multibind(\n",
       "   343                                                       datatype=\"pbm\",\n",
       "   344                                                       init_random=init_random,\n",
       "   345                                                       bm_generator=mb.models.BMPrediction(num_classes=1, input_size=21, hidden_size=2, num_layers=1,\n",
       "   346                                                                                           seq_length=train.dataset.get_max_residue_length()),\n",
       "   347                                                       **kwargs,\n",
       "   348                                                   ).to(device)\n",
       "   349                                               else:\n",
       "   350                                                   assert False  # not implemented yet\n",
       "   351                                           \n",
       "   352                                               # this sets up the seed at the first position\n",
       "   353         1          2.0      2.0      0.0      if seed is not None:\n",
       "   354                                                   # this sets up the seed at the first position\n",
       "   355                                                   for i, s, min_w, max_w in seed:\n",
       "   356                                                       if s is not None:\n",
       "   357                                                           print(i, s)\n",
       "   358                                                           model.set_seed(s, i, min=min_w, max=max_w)\n",
       "   359                                                   model = model.to(device)\n",
       "   360                                           \n",
       "   361                                               # step 1) freeze everything before the current binding mode\n",
       "   362         5         11.0      2.2      0.0      for i in range(0, n_kernels):\n",
       "   363         4          8.0      2.0      0.0          if verbose != 0:\n",
       "   364         4         43.0     10.8      0.0              print(\"\\nKernel to optimize %i\" % i)\n",
       "   365         4         33.0      8.2      0.0              print(\"\\nFreezing kernels\")\n",
       "   366        20         40.0      2.0      0.0          for ki in range(n_kernels):\n",
       "   367        16         30.0      1.9      0.0              if verbose != 0:\n",
       "   368        16        145.0      9.1      0.0                  print(\"setting grad status of kernel at %i to %i\" % (ki, ki == i))\n",
       "   369        16        556.0     34.8      0.0              model.update_grad(ki, ki == i)\n",
       "   370         4         33.0      8.2      0.0          print(\"\\n\")\n",
       "   371                                           \n",
       "   372         4          8.0      2.0      0.0          if show_logo:\n",
       "   373                                                       if verbose != 0:\n",
       "   374                                                           print(\"before kernel optimization.\")\n",
       "   375                                                       mb.pl.plot_activities(model, train)\n",
       "   376                                                       mb.pl.conv_mono(model)\n",
       "   377                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   378                                           \n",
       "   379         4          9.0      2.2      0.0          next_lr = lr if not isinstance(lr, list) else lr[i]\n",
       "   380         4          8.0      2.0      0.0          next_weight_decay = weight_decay if not isinstance(weight_decay, list) else weight_decay[i]\n",
       "   381                                           \n",
       "   382         4         18.0      4.5      0.0          next_optimiser = (\n",
       "   383         4        508.0    127.0      0.0              topti.Adam(model.parameters(), lr=next_lr, weight_decay=next_weight_decay)\n",
       "   384         4          8.0      2.0      0.0              if optimiser is None\n",
       "   385                                                       else optimiser(model.parameters(), lr=next_lr)\n",
       "   386                                                   )\n",
       "   387                                           \n",
       "   388                                                   # mask kernels to avoid using weights from further steps into early ones.\n",
       "   389         4          7.0      1.8      0.0          if ignore_kernel:\n",
       "   390                                                       model.set_ignore_kernel(np.array([0 for i in range(i + 1)] + [1 for i in range(i + 1, n_kernels)]))\n",
       "   391                                           \n",
       "   392         4          8.0      2.0      0.0          if verbose != 0:\n",
       "   393         4         78.0     19.5      0.0              print(\"kernels mask\", model.get_ignore_kernel())\n",
       "   394                                           \n",
       "   395                                                   # assert False\n",
       "   396         8  171085115.0 21385639.4     34.8          mb.tl.train_network(\n",
       "   397         4          8.0      2.0      0.0              model,\n",
       "   398         4          7.0      1.8      0.0              train,\n",
       "   399         4          7.0      1.8      0.0              device,\n",
       "   400         4          8.0      2.0      0.0              next_optimiser,\n",
       "   401         4          7.0      1.8      0.0              criterion,\n",
       "   402         4          8.0      2.0      0.0              num_epochs=num_epochs,\n",
       "   403         4          7.0      1.8      0.0              early_stopping=early_stopping,\n",
       "   404         4          6.0      1.5      0.0              log_each=log_each,\n",
       "   405         4          8.0      2.0      0.0              dirichlet_regularization=dirichlet_regularization,\n",
       "   406         4          7.0      1.8      0.0              exp_max=exp_max,\n",
       "   407         4          7.0      1.8      0.0              verbose=verbose,\n",
       "   408                                                   )\n",
       "   409                                                   # print('next color', colors[i])\n",
       "   410         4        712.0    178.0      0.0          model.loss_color += list(np.repeat(colors[i], len(model.loss_history) - len(model.loss_color)))\n",
       "   411                                                   # probably here load the state of the best epoch and save\n",
       "   412         4       1572.0    393.0      0.0          model.load_state_dict(model.best_model_state)\n",
       "   413         4         11.0      2.8      0.0          \"%i\" % w\n",
       "   414                                                   # store model parameters and fit for later visualization\n",
       "   415         4      31842.0   7960.5      0.0          model = copy.deepcopy(model)\n",
       "   416                                                   # optimizer for left / right flanks\n",
       "   417         4         12.0      3.0      0.0          best_loss = model.best_loss\n",
       "   418                                           \n",
       "   419         4          8.0      2.0      0.0          if show_logo:\n",
       "   420                                                       print(\"\\n##After kernel opt / before shift optim.\")\n",
       "   421                                                       mb.pl.plot_activities(model, train)\n",
       "   422                                                       mb.pl.conv_mono(model)\n",
       "   423                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   424                                                       mb.pl.plot_loss(model)\n",
       "   425                                           \n",
       "   426                                                   # print(model_by_k[k_parms].loss_color)\n",
       "   427                                                   #######\n",
       "   428                                                   # optimize the flanks through +1/-1 shifts\n",
       "   429                                                   #######\n",
       "   430         4          7.0      1.8      0.0          n_attempts = 0\n",
       "   431                                           \n",
       "   432         4          9.0      2.2      0.0          if (opt_kernel_shift or opt_kernel_length) and i != 0:\n",
       "   433                                           \n",
       "   434         3          7.0      2.3      0.0              opt_expand_left = range(1, expand_length_max, expand_length_step)\n",
       "   435         3          6.0      2.0      0.0              opt_expand_right = range(1, expand_length_max, expand_length_step)\n",
       "   436         3         10.0      3.3      0.0              opt_shift = [0] + list(range(-shift_max, shift_max + 1, shift_step))\n",
       "   437                                           \n",
       "   438        12         29.0      2.4      0.0              for opt_option_text, opt_option_next in zip(\n",
       "   439         3          6.0      2.0      0.0                  [\"FLANKS\", \"SHIFT\"], [[opt_expand_left, opt_expand_right, [0]], [[0], [0], opt_shift]]\n",
       "   440                                                       ):\n",
       "   441                                           \n",
       "   442                                                           # print(opt_option_text, opt_option_next)\n",
       "   443                                                           # assert False\n",
       "   444                                           \n",
       "   445         6         12.0      2.0      0.0                  next_loss = None\n",
       "   446        15         42.0      2.8      0.0                  while next_loss is None or next_loss < best_loss:\n",
       "   447         9         18.0      2.0      0.0                      n_attempts += 1\n",
       "   448                                           \n",
       "   449         9        222.0     24.7      0.0                      curr_w = model.get_kernel_width(i)\n",
       "   450         9         19.0      2.1      0.0                      if curr_w >= max_w:\n",
       "   451                                                                   print(\"stop. Reached maximum w...\")\n",
       "   452                                                                   break\n",
       "   453                                           \n",
       "   454         9         17.0      1.9      0.0                      if verbose != 0:\n",
       "   455        18        121.0      6.7      0.0                          print(\n",
       "   456         9         24.0      2.7      0.0                              \"\\noptimize_motif_shift (%s)...\" % (\"first\" if next_loss is None else \"again\"),\n",
       "   457         9         18.0      2.0      0.0                              end=\"\",\n",
       "   458                                                                   )\n",
       "   459         9         77.0      8.6      0.0                          print(\"\")\n",
       "   460         9      54411.0   6045.7      0.0                      model = copy.deepcopy(model)\n",
       "   461         9         25.0      2.8      0.0                      best_loss = model.best_loss\n",
       "   462         9         22.0      2.4      0.0                      next_color = colors[-(1 if n_attempts % 2 == 0 else -2)]\n",
       "   463                                           \n",
       "   464         9         16.0      1.8      0.0                      all_options = []\n",
       "   465                                           \n",
       "   466        18         80.0      4.4      0.0                      options = [\n",
       "   467                                                                   [expand_left, expand_right, shift]\n",
       "   468         9         18.0      2.0      0.0                          for expand_left in opt_option_next[0]\n",
       "   469                                                                   for expand_right in opt_option_next[1]\n",
       "   470                                                                   for shift in opt_option_next[2]\n",
       "   471                                                               ]\n",
       "   472                                           \n",
       "   473                                                               # print(options)\n",
       "   474                                           \n",
       "   475        48         98.0      2.0      0.0                      for expand_left, expand_right, shift in options:\n",
       "   476                                           \n",
       "   477        39         85.0      2.2      0.0                          if abs(expand_left) + abs(expand_right) + abs(shift) == 0:\n",
       "   478         3          6.0      2.0      0.0                              continue\n",
       "   479        36         70.0      1.9      0.0                          if abs(shift) > 0:  # skip shift for now.\n",
       "   480        12         24.0      2.0      0.0                              continue\n",
       "   481        24         48.0      2.0      0.0                          if curr_w + expand_left + expand_right > max_w:\n",
       "   482        10         19.0      1.9      0.0                              continue\n",
       "   483                                           \n",
       "   484                                                                   # print(expand_left, expand_right, shift)\n",
       "   485                                                                   # assert False\n",
       "   486                                           \n",
       "   487        14         27.0      1.9      0.0                          if verbose != 0:\n",
       "   488        28        157.0      5.6      0.0                              print(\n",
       "   489        28         59.0      2.1      0.0                                  \"next expand left: %i, next expand right: %i, shift: %i\"\n",
       "   490        14         27.0      1.9      0.0                                  % (expand_left, expand_right, shift)\n",
       "   491                                                                       )\n",
       "   492                                           \n",
       "   493        14     110585.0   7898.9      0.0                          model_shift = copy.deepcopy(model)\n",
       "   494        14        117.0      8.4      0.0                          model_shift.loss_history = []\n",
       "   495        14        253.0     18.1      0.0                          model_shift.loss_color = []\n",
       "   496                                           \n",
       "   497       252  279823374.0 1110410.2     56.8                          mb.tl.train_modified_kernel(\n",
       "   498        14         26.0      1.9      0.0                              model_shift,\n",
       "   499        14         28.0      2.0      0.0                              train,\n",
       "   500        14         27.0      1.9      0.0                              kernel_i=i,\n",
       "   501        14         28.0      2.0      0.0                              shift=shift,\n",
       "   502        14         26.0      1.9      0.0                              expand_left=expand_left,\n",
       "   503        14         27.0      1.9      0.0                              expand_right=expand_right,\n",
       "   504        14         28.0      2.0      0.0                              device=device,\n",
       "   505        14         28.0      2.0      0.0                              num_epochs=num_epochs,\n",
       "   506        14         26.0      1.9      0.0                              early_stopping=early_stopping,\n",
       "   507        14         27.0      1.9      0.0                              log_each=log_each,\n",
       "   508        14         24.0      1.7      0.0                              update_grad_i=i,\n",
       "   509        14         28.0      2.0      0.0                              lr=next_lr,\n",
       "   510        14         30.0      2.1      0.0                              weight_decay=next_weight_decay,\n",
       "   511        14         25.0      1.8      0.0                              optimiser=optimiser,\n",
       "   512        14         28.0      2.0      0.0                              criterion=criterion,\n",
       "   513        14         27.0      1.9      0.0                              dirichlet_regularization=dirichlet_regularization,\n",
       "   514        14         28.0      2.0      0.0                              exp_max=exp_max,\n",
       "   515        14         25.0      1.8      0.0                              verbose=verbose,\n",
       "   516        14         25.0      1.8      0.0                              **kwargs,\n",
       "   517                                                                   )\n",
       "   518        14       1800.0    128.6      0.0                          model_shift.loss_color += list(np.repeat(next_color, len(model_shift.loss_history)))\n",
       "   519                                                                   # print('history left', len(model_left.loss_history))\n",
       "   520        14         39.0      2.8      0.0                          all_options.append([expand_left, expand_right, shift, model_shift, model_shift.best_loss])\n",
       "   521                                                                   # print('\\n')\n",
       "   522                                           \n",
       "   523        14         29.0      2.1      0.0                          if verbose != 0:\n",
       "   524        14        141.0     10.1      0.0                              print(\"after opt.\")\n",
       "   525        14         31.0      2.2      0.0                              if show_logo:\n",
       "   526                                                                           mb.pl.conv_mono(model_shift)\n",
       "   527                                           \n",
       "   528                                                               # for shift, model_shift, loss in all_shifts:\n",
       "   529                                                               #     print('shift=%i' % shift, 'loss=%.4f' % loss)\n",
       "   530        18        410.0     22.8      0.0                      best = sorted(\n",
       "   531         9         22.0      2.4      0.0                          all_options + [[0, 0, 0, model, best_loss]],\n",
       "   532         9         18.0      2.0      0.0                          key=lambda x: x[-1],\n",
       "   533                                                               )\n",
       "   534         9         16.0      1.8      0.0                      if verbose != 0:\n",
       "   535         9         90.0     10.0      0.0                          print(\"sorted\")\n",
       "   536        18       4852.0    269.6      0.0                      best_df = pd.DataFrame(\n",
       "   537        18         52.0      2.9      0.0                          [\n",
       "   538                                                                       [expand_left, expand_right, shift, loss]\n",
       "   539         9         17.0      1.9      0.0                              for expand_left, expand_right, shift, model_shift, loss in best\n",
       "   540                                                                   ],\n",
       "   541         9         18.0      2.0      0.0                          columns=[\"expand.left\", \"expand.right\", \"shift\", \"loss\"],\n",
       "   542                                                               )\n",
       "   543         9         23.0      2.6      0.0                      if verbose != 0:\n",
       "   544         9      18682.0   2075.8      0.0                          print(best_df.sort_values(\"loss\"))\n",
       "   545                                                               # for shift, model_shift, loss in best:\n",
       "   546                                                               #     print('shift=%i' % shift, 'loss=%.4f' % loss)\n",
       "   547                                           \n",
       "   548                                                               # print('\\n history len')\n",
       "   549         9        217.0     24.1      0.0                      next_expand_left, next_expand_right, next_position, next_model, next_loss = best[0]\n",
       "   550         9         20.0      2.2      0.0                      if verbose != 0:\n",
       "   551         9        105.0     11.7      0.0                          print(\"action: %s\\n\" % str((next_expand_left, next_expand_right, next_position)))\n",
       "   552                                           \n",
       "   553         9         20.0      2.2      0.0                      if next_position != 0:\n",
       "   554                                                                   next_model.loss_history = model.loss_history + next_model.loss_history\n",
       "   555                                                                   next_model.loss_color = model.loss_color + next_model.loss_color\n",
       "   556                                           \n",
       "   557         9      44538.0   4948.7      0.0                      model = copy.deepcopy(next_model)\n",
       "   558                                           \n",
       "   559         4          8.0      2.0      0.0          if show_logo:\n",
       "   560                                                       if verbose != 0:\n",
       "   561                                                           print(\"after shift optimz model\")\n",
       "   562                                                       mb.pl.plot_activities(model, train)\n",
       "   563                                                       mb.pl.conv_mono(model)\n",
       "   564                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   565                                                       mb.pl.plot_loss(model)\n",
       "   566                                                       print(\"\")\n",
       "   567                                           \n",
       "   568                                                   # the first kernel does not require an additional fit.\n",
       "   569         4          8.0      2.0      0.0          if i == 0:\n",
       "   570         1          3.0      3.0      0.0              continue\n",
       "   571                                           \n",
       "   572         3          6.0      2.0      0.0          if verbose != 0:\n",
       "   573         3         33.0     11.0      0.0              print(\"\\n\\nfinal refinement step (after shift)...\")\n",
       "   574         3         25.0      8.3      0.0              print(\"\\nunfreezing all layers for final refinement\")\n",
       "   575                                           \n",
       "   576        15         33.0      2.2      0.0          for ki in range(n_kernels):\n",
       "   577        12         25.0      2.1      0.0              if verbose != 0:\n",
       "   578        12        115.0      9.6      0.0                  print(\"kernel grad (%i) = %i \\n\" % (ki, True), sep=\", \", end=\"\")\n",
       "   579        12        415.0     34.6      0.0              model.update_grad(ki, ki == i)\n",
       "   580         3          6.0      2.0      0.0          if verbose != 0:\n",
       "   581         3         27.0      9.0      0.0              print(\"\")\n",
       "   582                                           \n",
       "   583         3         26.0      8.7      0.0          next_optimiser = (\n",
       "   584         3        392.0    130.7      0.0              topti.Adam(model.parameters(), lr=next_lr, weight_decay=next_weight_decay)\n",
       "   585         3          6.0      2.0      0.0              if optimiser is None\n",
       "   586                                                       else optimiser(model.parameters(), lr=next_lr)\n",
       "   587                                                   )\n",
       "   588                                           \n",
       "   589                                                   # mask kernels to avoid using weights from further steps into early ones.\n",
       "   590         3          6.0      2.0      0.0          if ignore_kernel:\n",
       "   591                                                       model.set_ignore_kernel(np.array([0 for i in range(i + 1)] + [1 for i in range(i + 1, n_kernels)]))\n",
       "   592         3          6.0      2.0      0.0          if verbose != 0:\n",
       "   593         3         58.0     19.3      0.0              print(\"kernels mask\", model.get_ignore_kernel())\n",
       "   594                                                   # assert False\n",
       "   595         6   41109480.0 6851580.0      8.4          mb.tl.train_network(\n",
       "   596         3          6.0      2.0      0.0              model,\n",
       "   597         3          6.0      2.0      0.0              train,\n",
       "   598         3          5.0      1.7      0.0              device,\n",
       "   599         3          5.0      1.7      0.0              next_optimiser,\n",
       "   600         3          5.0      1.7      0.0              criterion,\n",
       "   601         3          6.0      2.0      0.0              num_epochs=num_epochs,\n",
       "   602         3          6.0      2.0      0.0              early_stopping=early_stopping,\n",
       "   603         3          5.0      1.7      0.0              log_each=log_each,\n",
       "   604         3          5.0      1.7      0.0              dirichlet_regularization=dirichlet_regularization,\n",
       "   605         3          5.0      1.7      0.0              verbose=verbose,\n",
       "   606                                                   )\n",
       "   607                                           \n",
       "   608                                                   # load the best model after the final refinement\n",
       "   609         3        459.0    153.0      0.0          model.loss_color += list(np.repeat(colors[i], len(model.loss_history) - len(model.loss_color)))\n",
       "   610         3       1123.0    374.3      0.0          model.load_state_dict(model.best_model_state)\n",
       "   611                                           \n",
       "   612         3          6.0      2.0      0.0          if stop_at_kernel is not None and stop_at_kernel == i:\n",
       "   613                                                       break\n",
       "   614                                           \n",
       "   615         3          6.0      2.0      0.0          if show_logo:\n",
       "   616                                                       print(\"\\n##final motif signal (after final refinement)\")\n",
       "   617                                                       mb.pl.plot_activities(model, train)\n",
       "   618                                                       mb.pl.conv_mono(model)\n",
       "   619                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   620                                                       # mb.pl.plot_loss(model)\n",
       "   621                                           \n",
       "   622         3         61.0     20.3      0.0          print('best loss', model.best_loss)\n",
       "   623                                                   # if i == 1:\n",
       "   624                                                   #     assert False\n",
       "   625                                           \n",
       "   626                                               # r = [k_parms, w, n_feat, l_best]\n",
       "   627                                               # # print(r)\n",
       "   628                                               # res.append(r)\n",
       "   629                                           \n",
       "   630         1          3.0      3.0      0.0      return model, model.best_loss"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f mb.tl.train_iterative model, best_loss = mb.tl.train_iterative(train, device, num_epochs=500, show_logo=False, early_stopping=50, log_each=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a06c7d1-8553-40a1-ab1f-6acbd2cbda96",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_rev = train.dataset.store_rev\n",
    "i, batch = enumerate(train).__next__()\n",
    "mononuc = batch[\"mononuc\"].to(device)\n",
    "b = batch[\"batch\"].to(device) if \"batch\" in batch else None\n",
    "rounds = batch[\"rounds\"].to(device) if \"rounds\" in batch else None\n",
    "countsum = batch[\"countsum\"].to(device) if \"countsum\" in batch else None\n",
    "residues = batch[\"residues\"].to(device) if \"residues\" in batch else None\n",
    "protein_id = batch[\"protein_id\"].to(device) if \"protein_id\" in batch else None\n",
    "inputs = {\"mono\": mononuc, \"batch\": b, \"countsum\": countsum}\n",
    "if store_rev:\n",
    "    mononuc_rev = batch[\"mononuc_rev\"].to(device)\n",
    "    inputs[\"mono_rev\"] = mononuc_rev\n",
    "if residues is not None:\n",
    "    inputs[\"residues\"] = residues\n",
    "if protein_id is not None:\n",
    "    inputs[\"protein_id\"] = protein_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a24bf99-ad7c-4554-91a7-af0240838808",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.024673 s\n",
       "File: /home/johanna/ICB/multibind/multibind/models/models.py\n",
       "Function: forward at line 81\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    81                                               def forward(self, mono, **kwargs):\n",
       "    82                                                   # mono_rev=None, di=None, di_rev=None, batch=None, countsum=None, residues=None, protein_id=None):\n",
       "    83         1          1.0      1.0      0.0          mono_rev = kwargs.get(\"mono_rev\", None)\n",
       "    84         1          1.0      1.0      0.0          di = kwargs.get(\"di\", None)\n",
       "    85         1          1.0      1.0      0.0          di_rev = kwargs.get(\"di_rev\", None)\n",
       "    86         1        157.0    157.0      0.6          mono = self.padding(mono)\n",
       "    87         1          1.0      1.0      0.0          if mono_rev is None:\n",
       "    88         1        185.0    185.0      0.7              mono_rev = mb.tl.mono2revmono(mono)\n",
       "    89                                                   else:\n",
       "    90                                                       mono_rev = self.padding(mono_rev)\n",
       "    91                                           \n",
       "    92                                                   # prepare the dinucleotide objects if we need them\n",
       "    93         1          2.0      2.0      0.0          if self.use_dinuc:\n",
       "    94                                                       if di is None:\n",
       "    95                                                           di = mb.tl.mono2dinuc(mono)\n",
       "    96                                                       if di_rev is None:\n",
       "    97                                                           di_rev = mb.tl.mono2dinuc(mono_rev)\n",
       "    98                                                       di = torch.unsqueeze(di, 1)\n",
       "    99                                                       di_rev = torch.unsqueeze(di_rev, 1)\n",
       "   100                                                       kwargs[\"di\"] = di\n",
       "   101                                                       kwargs[\"di_rev\"] = di_rev\n",
       "   102                                           \n",
       "   103                                                   # unsqueeze mono after preparing di and unsqueezing mono\n",
       "   104         1          7.0      7.0      0.0          mono_rev = torch.unsqueeze(mono_rev, 1)\n",
       "   105         1          2.0      2.0      0.0          mono = torch.unsqueeze(mono, 1)\n",
       "   106                                           \n",
       "   107                                                   # binding_per_mode: matrix of size [batchsize, number of binding modes]\n",
       "   108         1      24130.0  24130.0     97.8          binding_per_mode = self.binding_modes(mono=mono, mono_rev=mono_rev, **kwargs)\n",
       "   109         1        184.0    184.0      0.7          binding_scores = self.activities(binding_per_mode, **kwargs)\n",
       "   110                                           \n",
       "   111         1          2.0      2.0      0.0          if self.datatype == \"pbm\":\n",
       "   112         1          0.0      0.0      0.0              return binding_scores\n",
       "   113                                                   elif self.datatype == \"selex\":\n",
       "   114                                                       return self.selex_module(binding_scores, **kwargs)\n",
       "   115                                                   else:\n",
       "   116                                                       return None  # this line should never be called"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f model.forward model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df32da70-38eb-4e7c-bab6-0ddbcd247637",
   "metadata": {},
   "source": [
    "# Genomics Dataset (ATAC) - separated kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "767e6c4b-a066-4017-a006-de6043db6509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmppwpw7qtf\n",
      "genome hg38 False\n",
      "options\n",
      "/home/johanna/ICB/annotations/hg38/genome/hg38.fa\n",
      "True /home/johanna/ICB/annotations/hg38/genome/hg38.fa\n",
      "running bedtools...\n",
      "bedtools getfasta -fi /home/johanna/ICB/annotations/hg38/genome/hg38.fa -bed /tmp/tmppwpw7qtf -fo /tmp/tmpxcm0i89j\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10246, 1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = mb.bindome.datasets.scATAC.PBMCs_10x_v2(datadir='../../atac_poisson_study/data/')\n",
    "peak_ids = adata.var_names\n",
    "adata.var['summit'] = ((adata.var['end'] + adata.var['start']) / 2).astype(int)\n",
    "adata.var['summit.start'] = adata.var['summit'] - 100\n",
    "adata.var['summit.end'] = adata.var['summit'] + 100\n",
    "adata.var['k.summit'] = adata.var['chr'] + ':' + adata.var['summit.start'].astype(str) + '-' + adata.var['summit.end'].astype(str)\n",
    "n_seqs = 1000\n",
    "seqs = mb.bindome.tl.get_sequences_from_bed(adata.var[['chr', 'summit.start', 'summit.end']].head(n_seqs), genome='hg38', uppercase=True)\n",
    "keys = set([s[0] for s in seqs])\n",
    "adata = adata[:,adata.var['k.summit'].isin(keys)]\n",
    "# seqs = [[s[0], s[1].upper()] for s in seqs[0]]\n",
    "adata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a656b076-a6cd-4211-a045-8a92c056e5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1000, 21)\n"
     ]
    }
   ],
   "source": [
    "# remove Ns\n",
    "seqs = [[s[0], s[1].replace('N', '')] for s in seqs]\n",
    "counts = adata.X.T\n",
    "next_data = pd.DataFrame.sparse.from_spmatrix(counts)\n",
    "next_data = next_data[range(20)].copy()\n",
    "next_data['seq'] = [s[1] for s in seqs]\n",
    "var = []\n",
    "for ri, r in next_data.iterrows():\n",
    "    if ri % 10000 == 0:\n",
    "        print(ri, next_data.shape)\n",
    "    # print(ri, r.values[:-1], r.values[:-1].var())\n",
    "    var.append(r.values[:-1].var())\n",
    "    # break\n",
    "next_data['var'] = var\n",
    "top_var = next_data[['var']].sort_values('var', ascending=False).index[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1d43126-e9a1-4c21-b5bc-6123e9752eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_data = next_data.head(10000)\n",
    "next_data_sel = next_data.reindex(top_var).reset_index(drop=True)\n",
    "del next_data_sel['var']\n",
    "next_data_sel.index = next_data_sel['seq']\n",
    "del next_data_sel['seq']\n",
    "df = next_data_sel\n",
    "dataset = mb.datasets.GenomicsDataset(df)\n",
    "train = tdata.DataLoader(dataset=dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e4f9405-86ef-4e2e-af7e-7f0601883ed3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next w 15 <class 'int'>\n",
      "# cells 20\n",
      "\n",
      "Kernel to optimize 0\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 1\n",
      "setting grad status of kernel at 1 to 0\n",
      "setting grad status of kernel at 2 to 0\n",
      "setting grad status of kernel at 3 to 0\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 1587260.705696 , best epoch: 46 secs per epoch: 2.104 s\n",
      "Epoch: 101, Loss: 1587257.107595 , best epoch: 88 secs per epoch: 2.114 s\n",
      "Epoch: 139, Loss: 1587258.2184 , best epoch: 88 secs per epoch: 2.107 s\n",
      "early stop!\n",
      "total time: 290.729 s\n",
      "secs per epoch: 2.107 s\n",
      "\n",
      "Kernel to optimize 1\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 0\n",
      "setting grad status of kernel at 1 to 1\n",
      "setting grad status of kernel at 2 to 0\n",
      "setting grad status of kernel at 3 to 0\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 705404.458861 , best epoch: 49 secs per epoch: 2.608 s\n",
      "Epoch: 101, Loss: 705370.973101 , best epoch: 90 secs per epoch: 2.583 s\n",
      "Epoch: 151, Loss: 705370.037975 , best epoch: 127 secs per epoch: 2.573 s\n",
      "Epoch: 178, Loss: 705370.8101 , best epoch: 127 secs per epoch: 2.571 s\n",
      "early stop!\n",
      "total time: 455.008 s\n",
      "secs per epoch: 2.571 s\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 705371.295095 , best epoch: 25 secs per epoch: 2.691 s\n",
      "Epoch: 76, Loss: 705370.7176 , best epoch: 25 secs per epoch: 2.673 s\n",
      "early stop!\n",
      "total time: 200.451 s\n",
      "secs per epoch: 2.673 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 705367.240506 , best epoch: 4 secs per epoch: 2.715 s\n",
      "Epoch: 55, Loss: 705370.1345 , best epoch: 4 secs per epoch: 2.709 s\n",
      "early stop!\n",
      "total time: 146.304 s\n",
      "secs per epoch: 2.709 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 705370.719146 , best epoch: 25 secs per epoch: 2.740 s\n",
      "Epoch: 76, Loss: 705370.1369 , best epoch: 25 secs per epoch: 2.721 s\n",
      "early stop!\n",
      "total time: 204.076 s\n",
      "secs per epoch: 2.721 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 705373.040348 , best epoch: 33 secs per epoch: 2.747 s\n",
      "Epoch: 101, Loss: 705370.713608 , best epoch: 52 secs per epoch: 2.724 s\n",
      "Epoch: 103, Loss: 705371.8710 , best epoch: 52 secs per epoch: 2.723 s\n",
      "early stop!\n",
      "total time: 277.770 s\n",
      "secs per epoch: 2.723 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift           loss\n",
      "0            2             1      0  705365.496835\n",
      "1            2             2      0  705366.071994\n",
      "2            1             1      0  705367.227848\n",
      "3            1             2      0  705367.230222\n",
      "4            0             0      0  705368.446203\n",
      "action: (2, 1, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (again)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 705368.973101 , best epoch: 29 secs per epoch: 2.828 s\n",
      "Epoch: 80, Loss: 705371.8750 , best epoch: 29 secs per epoch: 2.792 s\n",
      "early stop!\n",
      "total time: 220.544 s\n",
      "secs per epoch: 2.792 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift           loss\n",
      "0            0             0      0  705365.496835\n",
      "1            1             1      0  705366.069620\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "sorted\n",
      "   expand.left  expand.right  shift           loss\n",
      "0            0             0      0  705365.496835\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "\n",
      "final refinement step (after shift)...\n",
      "\n",
      "unfreezing all layers for final refinement\n",
      "kernel grad (0) = 1 \n",
      "kernel grad (1) = 1 \n",
      "kernel grad (2) = 1 \n",
      "kernel grad (3) = 1 \n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 705371.298259 , best epoch: 37 secs per epoch: 2.773 s\n",
      "Epoch: 88, Loss: 705371.8797 , best epoch: 37 secs per epoch: 2.752 s\n",
      "early stop!\n",
      "total time: 239.439 s\n",
      "secs per epoch: 2.752 s\n",
      "best loss 705364.9185126582\n",
      "\n",
      "Kernel to optimize 2\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 0\n",
      "setting grad status of kernel at 1 to 0\n",
      "setting grad status of kernel at 2 to 1\n",
      "setting grad status of kernel at 3 to 0\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 176306.464399 , best epoch: 49 secs per epoch: 2.727 s\n",
      "Epoch: 101, Loss: 176287.899328 , best epoch: 90 secs per epoch: 2.694 s\n",
      "Epoch: 151, Loss: 176285.545293 , best epoch: 149 secs per epoch: 2.688 s\n",
      "Epoch: 201, Loss: 176286.425040 , best epoch: 159 secs per epoch: 2.683 s\n",
      "Epoch: 210, Loss: 176284.3873 , best epoch: 159 secs per epoch: 2.682 s\n",
      "early stop!\n",
      "total time: 560.524 s\n",
      "secs per epoch: 2.682 s\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 176284.677809 , best epoch: 5 secs per epoch: 2.731 s\n",
      "Epoch: 56, Loss: 176286.1256 , best epoch: 5 secs per epoch: 2.726 s\n",
      "early stop!\n",
      "total time: 149.944 s\n",
      "secs per epoch: 2.726 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 176284.383703 , best epoch: 34 secs per epoch: 2.774 s\n",
      "Epoch: 101, Loss: 176286.124209 , best epoch: 83 secs per epoch: 2.803 s\n",
      "Epoch: 134, Loss: 176286.4128 , best epoch: 83 secs per epoch: 2.827 s\n",
      "early stop!\n",
      "total time: 375.963 s\n",
      "secs per epoch: 2.827 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 176286.413172 , best epoch: 5 secs per epoch: 2.864 s\n",
      "Epoch: 101, Loss: 176283.803006 , best epoch: 55 secs per epoch: 2.841 s\n",
      "Epoch: 106, Loss: 176285.5455 , best epoch: 55 secs per epoch: 2.834 s\n",
      "early stop!\n",
      "total time: 297.565 s\n",
      "secs per epoch: 2.834 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 176285.539953 , best epoch: 17 secs per epoch: 2.798 s\n",
      "Epoch: 101, Loss: 176285.834850 , best epoch: 84 secs per epoch: 2.767 s\n",
      "Epoch: 135, Loss: 176285.8321 , best epoch: 84 secs per epoch: 2.759 s\n",
      "early stop!\n",
      "total time: 369.669 s\n",
      "secs per epoch: 2.759 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift           loss\n",
      "0            1             2      0  176281.488924\n",
      "1            2             1      0  176282.359177\n",
      "2            0             0      0  176283.133307\n",
      "3            2             2      0  176283.224288\n",
      "4            1             1      0  176283.803204\n",
      "action: (1, 2, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (again)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 176286.119066 , best epoch: 23 secs per epoch: 2.834 s\n",
      "Epoch: 101, Loss: 176285.537579 , best epoch: 98 secs per epoch: 2.804 s\n",
      "Epoch: 149, Loss: 176285.5392 , best epoch: 98 secs per epoch: 2.794 s\n",
      "early stop!\n",
      "total time: 413.525 s\n",
      "secs per epoch: 2.794 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift           loss\n",
      "0            0             0      0  176281.488924\n",
      "1            1             1      0  176282.061511\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "sorted\n",
      "   expand.left  expand.right  shift           loss\n",
      "0            0             0      0  176281.488924\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "\n",
      "final refinement step (after shift)...\n",
      "\n",
      "unfreezing all layers for final refinement\n",
      "kernel grad (0) = 1 \n",
      "kernel grad (1) = 1 \n",
      "kernel grad (2) = 1 \n",
      "kernel grad (3) = 1 \n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 176285.251384 , best epoch: 10 secs per epoch: 2.765 s\n",
      "Epoch: 61, Loss: 176285.8341 , best epoch: 10 secs per epoch: 2.756 s\n",
      "early stop!\n",
      "total time: 165.360 s\n",
      "secs per epoch: 2.756 s\n",
      "best loss 176283.51186708861\n",
      "\n",
      "Kernel to optimize 3\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 0\n",
      "setting grad status of kernel at 1 to 0\n",
      "setting grad status of kernel at 2 to 0\n",
      "setting grad status of kernel at 3 to 1\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 2.255819 , best epoch: 49 secs per epoch: 2.742 s\n",
      "Epoch: 101, Loss: 0.429827 , best epoch: 99 secs per epoch: 2.691 s\n",
      "Epoch: 151, Loss: 0.267866 , best epoch: 149 secs per epoch: 2.674 s\n",
      "Epoch: 201, Loss: 0.250105 , best epoch: 199 secs per epoch: 2.668 s\n",
      "Epoch: 251, Loss: 0.246678 , best epoch: 238 secs per epoch: 2.667 s\n",
      "Epoch: 301, Loss: 0.244549 , best epoch: 269 secs per epoch: 2.672 s\n",
      "Epoch: 351, Loss: 0.244175 , best epoch: 302 secs per epoch: 2.673 s\n",
      "Epoch: 353, Loss: 0.2439 , best epoch: 302 secs per epoch: 2.673 s\n",
      "early stop!\n",
      "total time: 940.937 s\n",
      "secs per epoch: 2.673 s\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.244234 , best epoch: 5 secs per epoch: 2.777 s\n",
      "Epoch: 56, Loss: 0.2433 , best epoch: 5 secs per epoch: 2.773 s\n",
      "early stop!\n",
      "total time: 152.521 s\n",
      "secs per epoch: 2.773 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.243723 , best epoch: 14 secs per epoch: 2.823 s\n",
      "Epoch: 65, Loss: 0.2456 , best epoch: 14 secs per epoch: 2.802 s\n",
      "early stop!\n",
      "total time: 179.356 s\n",
      "secs per epoch: 2.802 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.243675 , best epoch: 40 secs per epoch: 2.791 s\n",
      "Epoch: 91, Loss: 0.2466 , best epoch: 40 secs per epoch: 2.769 s\n",
      "early stop!\n",
      "total time: 249.165 s\n",
      "secs per epoch: 2.769 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.249092 , best epoch: 33 secs per epoch: 2.830 s\n",
      "Epoch: 101, Loss: 0.243485 , best epoch: 82 secs per epoch: 2.800 s\n",
      "Epoch: 133, Loss: 0.2470 , best epoch: 82 secs per epoch: 2.793 s\n",
      "early stop!\n",
      "total time: 368.740 s\n",
      "secs per epoch: 2.793 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            2             2      0  0.241596\n",
      "1            1             2      0  0.241821\n",
      "2            2             1      0  0.242203\n",
      "3            1             1      0  0.242393\n",
      "4            0             0      0  0.243680\n",
      "action: (2, 2, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (again)...\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            0             0      0  0.241596\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            0             0      0  0.241596\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "\n",
      "final refinement step (after shift)...\n",
      "\n",
      "unfreezing all layers for final refinement\n",
      "kernel grad (0) = 1 \n",
      "kernel grad (1) = 1 \n",
      "kernel grad (2) = 1 \n",
      "kernel grad (3) = 1 \n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.242548 , best epoch: 16 secs per epoch: 2.825 s\n",
      "Epoch: 67, Loss: 0.2477 , best epoch: 16 secs per epoch: 2.812 s\n",
      "early stop!\n",
      "total time: 185.606 s\n",
      "secs per epoch: 2.812 s\n",
      "best loss 0.24193944616974156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 6444.22 s\n",
       "File: /home/johanna/ICB/multibind/multibind/tl/prediction.py\n",
       "Function: train_iterative at line 232\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   232                                           def train_iterative(\n",
       "   233                                               train,\n",
       "   234                                               device,\n",
       "   235                                               n_kernels=4,\n",
       "   236                                               w=15,\n",
       "   237                                               # min_w=10,\n",
       "   238                                               max_w=20,\n",
       "   239                                               num_epochs=100,\n",
       "   240                                               early_stopping=15,\n",
       "   241                                               log_each=10,\n",
       "   242                                               opt_kernel_shift=True,\n",
       "   243                                               opt_kernel_length=True,\n",
       "   244                                               expand_length_max=3,\n",
       "   245                                               expand_length_step=1,\n",
       "   246                                               show_logo=False,\n",
       "   247                                               optimiser=None,\n",
       "   248                                               criterion=None,\n",
       "   249                                               seed=None,\n",
       "   250                                               init_random=False,\n",
       "   251                                               lr=0.01,\n",
       "   252                                               joint_learning=False,\n",
       "   253                                               ignore_kernel=False,\n",
       "   254                                               weight_decay=0.001,\n",
       "   255                                               stop_at_kernel=None,\n",
       "   256                                               dirichlet_regularization=0,\n",
       "   257                                               verbose=2,\n",
       "   258                                               exp_max=40,\n",
       "   259                                               shift_max=3,\n",
       "   260                                               shift_step=2,\n",
       "   261                                               **kwargs,\n",
       "   262                                           ):\n",
       "   263                                               # color for visualization of history\n",
       "   264         1          3.0      3.0      0.0      colors = [\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#984ea3\", \"#ff7f00\", \"#ffff33\", \"#a65628\"]\n",
       "   265         1          3.0      3.0      0.0      if verbose != 0:\n",
       "   266         1         61.0     61.0      0.0          print(\"next w\", w, type(w))\n",
       "   267                                           \n",
       "   268         1          4.0      4.0      0.0      if isinstance(train.dataset, mb.datasets.SelexDataset):\n",
       "   269                                                   if criterion is None:\n",
       "   270                                                       criterion = mb.tl.PoissonLoss()\n",
       "   271                                           \n",
       "   272                                                   n_rounds = train.dataset.n_rounds\n",
       "   273                                                   n_batches = train.dataset.n_batches\n",
       "   274                                                   enr_series = train.dataset.enr_series\n",
       "   275                                                   if verbose != 0:\n",
       "   276                                                       print(\"# rounds\", n_rounds)\n",
       "   277                                                       print(\"# batches\", n_batches)\n",
       "   278                                                       print(\"# enr_series\", enr_series)\n",
       "   279                                           \n",
       "   280                                                   model = mb.models.Multibind(\n",
       "   281                                                       datatype=\"selex\",\n",
       "   282                                                       kernels=[0] + [w] * (n_kernels - 1),\n",
       "   283                                                       n_rounds=n_rounds,\n",
       "   284                                                       init_random=init_random,\n",
       "   285                                                       n_batches=n_batches,\n",
       "   286                                                       enr_series=enr_series,\n",
       "   287                                                       **kwargs,\n",
       "   288                                                   ).to(device)\n",
       "   289         1          2.0      2.0      0.0      elif isinstance(train.dataset, mb.datasets.PBMDataset):\n",
       "   290                                                   if criterion is None:\n",
       "   291                                                       criterion = mb.tl.MSELoss()\n",
       "   292                                           \n",
       "   293                                                   n_proteins = train.dataset.n_proteins\n",
       "   294                                                   if verbose != 0:\n",
       "   295                                                       print(\"# proteins\", n_proteins)\n",
       "   296                                           \n",
       "   297                                                   if joint_learning:\n",
       "   298                                                       model = mb.models.Multibind(\n",
       "   299                                                           datatype=\"pbm\",\n",
       "   300                                                           kernels=[0] + [w] * (n_kernels - 1),\n",
       "   301                                                           init_random=init_random,\n",
       "   302                                                           n_batches=n_proteins,\n",
       "   303                                                           **kwargs,\n",
       "   304                                                       ).to(device)\n",
       "   305                                                   else:\n",
       "   306                                                       bm_generator = mb.models.BMCollection(n_proteins=n_proteins, n_kernels=n_kernels, init_random=init_random)\n",
       "   307                                                       model = mb.models.Multibind(\n",
       "   308                                                           datatype=\"pbm\",\n",
       "   309                                                           init_random=init_random,\n",
       "   310                                                           n_proteins=n_proteins,\n",
       "   311                                                           bm_generator=bm_generator,\n",
       "   312                                                           n_kernels=n_kernels,\n",
       "   313                                                           **kwargs,\n",
       "   314                                                       ).to(device)\n",
       "   315         1          2.0      2.0      0.0      elif isinstance(train.dataset, mb.datasets.GenomicsDataset):\n",
       "   316         1          2.0      2.0      0.0          if criterion is None:\n",
       "   317         1         56.0     56.0      0.0              criterion = mb.tl.MSELoss()\n",
       "   318                                           \n",
       "   319         1          2.0      2.0      0.0          n_proteins = train.dataset.n_cells\n",
       "   320         1          2.0      2.0      0.0          if verbose != 0:\n",
       "   321         1         15.0     15.0      0.0              print(\"# cells\", n_proteins)\n",
       "   322                                           \n",
       "   323         1          2.0      2.0      0.0          if joint_learning:\n",
       "   324                                                       model = mb.models.Multibind(\n",
       "   325                                                           datatype=\"pbm\",\n",
       "   326                                                           kernels=[0] + [w] * (n_kernels - 1),\n",
       "   327                                                           init_random=init_random,\n",
       "   328                                                           n_batches=n_proteins,\n",
       "   329                                                           **kwargs,\n",
       "   330                                                       ).to(device)\n",
       "   331                                                   else:\n",
       "   332         1       9079.0   9079.0      0.0              bm_generator = mb.models.BMCollection(n_proteins=n_proteins, n_kernels=n_kernels, init_random=init_random)\n",
       "   333         4        383.0     95.8      0.0              model = mb.models.Multibind(\n",
       "   334         1          2.0      2.0      0.0                  datatype=\"pbm\",\n",
       "   335         1          1.0      1.0      0.0                  init_random=init_random,\n",
       "   336         1          2.0      2.0      0.0                  n_proteins=n_proteins,\n",
       "   337         1          1.0      1.0      0.0                  bm_generator=bm_generator,\n",
       "   338         1          2.0      2.0      0.0                  n_kernels=n_kernels,\n",
       "   339         1          2.0      2.0      0.0                  **kwargs,\n",
       "   340         1        880.0    880.0      0.0              ).to(device)\n",
       "   341                                               elif isinstance(train.dataset, mb.datasets.ResiduePBMDataset):\n",
       "   342                                                   model = mb.models.Multibind(\n",
       "   343                                                       datatype=\"pbm\",\n",
       "   344                                                       init_random=init_random,\n",
       "   345                                                       bm_generator=mb.models.BMPrediction(num_classes=1, input_size=21, hidden_size=2, num_layers=1,\n",
       "   346                                                                                           seq_length=train.dataset.get_max_residue_length()),\n",
       "   347                                                       **kwargs,\n",
       "   348                                                   ).to(device)\n",
       "   349                                               else:\n",
       "   350                                                   assert False  # not implemented yet\n",
       "   351                                           \n",
       "   352                                               # this sets up the seed at the first position\n",
       "   353         1          2.0      2.0      0.0      if seed is not None:\n",
       "   354                                                   # this sets up the seed at the first position\n",
       "   355                                                   for i, s, min_w, max_w in seed:\n",
       "   356                                                       if s is not None:\n",
       "   357                                                           print(i, s)\n",
       "   358                                                           model.set_seed(s, i, min=min_w, max=max_w)\n",
       "   359                                                   model = model.to(device)\n",
       "   360                                           \n",
       "   361                                               # step 1) freeze everything before the current binding mode\n",
       "   362         5         12.0      2.4      0.0      for i in range(0, n_kernels):\n",
       "   363         4          8.0      2.0      0.0          if verbose != 0:\n",
       "   364         4         44.0     11.0      0.0              print(\"\\nKernel to optimize %i\" % i)\n",
       "   365         4         32.0      8.0      0.0              print(\"\\nFreezing kernels\")\n",
       "   366        20         40.0      2.0      0.0          for ki in range(n_kernels):\n",
       "   367        16         31.0      1.9      0.0              if verbose != 0:\n",
       "   368        16        148.0      9.2      0.0                  print(\"setting grad status of kernel at %i to %i\" % (ki, ki == i))\n",
       "   369        16       3563.0    222.7      0.0              model.update_grad(ki, ki == i)\n",
       "   370         4         35.0      8.8      0.0          print(\"\\n\")\n",
       "   371                                           \n",
       "   372         4          8.0      2.0      0.0          if show_logo:\n",
       "   373                                                       if verbose != 0:\n",
       "   374                                                           print(\"before kernel optimization.\")\n",
       "   375                                                       mb.pl.plot_activities(model, train)\n",
       "   376                                                       mb.pl.conv_mono(model)\n",
       "   377                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   378                                           \n",
       "   379         4          9.0      2.2      0.0          next_lr = lr if not isinstance(lr, list) else lr[i]\n",
       "   380         4          8.0      2.0      0.0          next_weight_decay = weight_decay if not isinstance(weight_decay, list) else weight_decay[i]\n",
       "   381                                           \n",
       "   382         4         37.0      9.2      0.0          next_optimiser = (\n",
       "   383         4       2596.0    649.0      0.0              topti.Adam(model.parameters(), lr=next_lr, weight_decay=next_weight_decay)\n",
       "   384         4         16.0      4.0      0.0              if optimiser is None\n",
       "   385                                                       else optimiser(model.parameters(), lr=next_lr)\n",
       "   386                                                   )\n",
       "   387                                           \n",
       "   388                                                   # mask kernels to avoid using weights from further steps into early ones.\n",
       "   389         4          9.0      2.2      0.0          if ignore_kernel:\n",
       "   390                                                       model.set_ignore_kernel(np.array([0 for i in range(i + 1)] + [1 for i in range(i + 1, n_kernels)]))\n",
       "   391                                           \n",
       "   392         4          7.0      1.8      0.0          if verbose != 0:\n",
       "   393         4         79.0     19.8      0.0              print(\"kernels mask\", model.get_ignore_kernel())\n",
       "   394                                           \n",
       "   395                                                   # assert False\n",
       "   396         8 2247200133.0 280900016.6     34.9          mb.tl.train_network(\n",
       "   397         4          8.0      2.0      0.0              model,\n",
       "   398         4          8.0      2.0      0.0              train,\n",
       "   399         4          8.0      2.0      0.0              device,\n",
       "   400         4          8.0      2.0      0.0              next_optimiser,\n",
       "   401         4          7.0      1.8      0.0              criterion,\n",
       "   402         4          8.0      2.0      0.0              num_epochs=num_epochs,\n",
       "   403         4          7.0      1.8      0.0              early_stopping=early_stopping,\n",
       "   404         4          6.0      1.5      0.0              log_each=log_each,\n",
       "   405         4          8.0      2.0      0.0              dirichlet_regularization=dirichlet_regularization,\n",
       "   406         4          7.0      1.8      0.0              exp_max=exp_max,\n",
       "   407         4          8.0      2.0      0.0              verbose=verbose,\n",
       "   408                                                   )\n",
       "   409                                                   # print('next color', colors[i])\n",
       "   410         4        954.0    238.5      0.0          model.loss_color += list(np.repeat(colors[i], len(model.loss_history) - len(model.loss_color)))\n",
       "   411                                                   # probably here load the state of the best epoch and save\n",
       "   412         4      19612.0   4903.0      0.0          model.load_state_dict(model.best_model_state)\n",
       "   413         4         12.0      3.0      0.0          \"%i\" % w\n",
       "   414                                                   # store model parameters and fit for later visualization\n",
       "   415         4      99452.0  24863.0      0.0          model = copy.deepcopy(model)\n",
       "   416                                                   # optimizer for left / right flanks\n",
       "   417         4         12.0      3.0      0.0          best_loss = model.best_loss\n",
       "   418                                           \n",
       "   419         4          8.0      2.0      0.0          if show_logo:\n",
       "   420                                                       print(\"\\n##After kernel opt / before shift optim.\")\n",
       "   421                                                       mb.pl.plot_activities(model, train)\n",
       "   422                                                       mb.pl.conv_mono(model)\n",
       "   423                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   424                                                       mb.pl.plot_loss(model)\n",
       "   425                                           \n",
       "   426                                                   # print(model_by_k[k_parms].loss_color)\n",
       "   427                                                   #######\n",
       "   428                                                   # optimize the flanks through +1/-1 shifts\n",
       "   429                                                   #######\n",
       "   430         4          7.0      1.8      0.0          n_attempts = 0\n",
       "   431                                           \n",
       "   432         4          8.0      2.0      0.0          if (opt_kernel_shift or opt_kernel_length) and i != 0:\n",
       "   433                                           \n",
       "   434         3          8.0      2.7      0.0              opt_expand_left = range(1, expand_length_max, expand_length_step)\n",
       "   435         3          6.0      2.0      0.0              opt_expand_right = range(1, expand_length_max, expand_length_step)\n",
       "   436         3          9.0      3.0      0.0              opt_shift = [0] + list(range(-shift_max, shift_max + 1, shift_step))\n",
       "   437                                           \n",
       "   438        12         31.0      2.6      0.0              for opt_option_text, opt_option_next in zip(\n",
       "   439         3          7.0      2.3      0.0                  [\"FLANKS\", \"SHIFT\"], [[opt_expand_left, opt_expand_right, [0]], [[0], [0], opt_shift]]\n",
       "   440                                                       ):\n",
       "   441                                           \n",
       "   442                                                           # print(opt_option_text, opt_option_next)\n",
       "   443                                                           # assert False\n",
       "   444                                           \n",
       "   445         6         12.0      2.0      0.0                  next_loss = None\n",
       "   446        15         44.0      2.9      0.0                  while next_loss is None or next_loss < best_loss:\n",
       "   447         9         18.0      2.0      0.0                      n_attempts += 1\n",
       "   448                                           \n",
       "   449         9        242.0     26.9      0.0                      curr_w = model.get_kernel_width(i)\n",
       "   450         9         20.0      2.2      0.0                      if curr_w >= max_w:\n",
       "   451                                                                   print(\"stop. Reached maximum w...\")\n",
       "   452                                                                   break\n",
       "   453                                           \n",
       "   454         9         18.0      2.0      0.0                      if verbose != 0:\n",
       "   455        18        140.0      7.8      0.0                          print(\n",
       "   456         9         23.0      2.6      0.0                              \"\\noptimize_motif_shift (%s)...\" % (\"first\" if next_loss is None else \"again\"),\n",
       "   457         9         14.0      1.6      0.0                              end=\"\",\n",
       "   458                                                                   )\n",
       "   459         9         75.0      8.3      0.0                          print(\"\")\n",
       "   460         9     218361.0  24262.3      0.0                      model = copy.deepcopy(model)\n",
       "   461         9         25.0      2.8      0.0                      best_loss = model.best_loss\n",
       "   462         9         21.0      2.3      0.0                      next_color = colors[-(1 if n_attempts % 2 == 0 else -2)]\n",
       "   463                                           \n",
       "   464         9         18.0      2.0      0.0                      all_options = []\n",
       "   465                                           \n",
       "   466        18         81.0      4.5      0.0                      options = [\n",
       "   467                                                                   [expand_left, expand_right, shift]\n",
       "   468         9         19.0      2.1      0.0                          for expand_left in opt_option_next[0]\n",
       "   469                                                                   for expand_right in opt_option_next[1]\n",
       "   470                                                                   for shift in opt_option_next[2]\n",
       "   471                                                               ]\n",
       "   472                                           \n",
       "   473                                                               # print(options)\n",
       "   474                                           \n",
       "   475        48         97.0      2.0      0.0                      for expand_left, expand_right, shift in options:\n",
       "   476                                           \n",
       "   477        39         95.0      2.4      0.0                          if abs(expand_left) + abs(expand_right) + abs(shift) == 0:\n",
       "   478         3          6.0      2.0      0.0                              continue\n",
       "   479        36         74.0      2.1      0.0                          if abs(shift) > 0:  # skip shift for now.\n",
       "   480        12         23.0      1.9      0.0                              continue\n",
       "   481        24         47.0      2.0      0.0                          if curr_w + expand_left + expand_right > max_w:\n",
       "   482        10         19.0      1.9      0.0                              continue\n",
       "   483                                           \n",
       "   484                                                                   # print(expand_left, expand_right, shift)\n",
       "   485                                                                   # assert False\n",
       "   486                                           \n",
       "   487        14         26.0      1.9      0.0                          if verbose != 0:\n",
       "   488        28        169.0      6.0      0.0                              print(\n",
       "   489        28         60.0      2.1      0.0                                  \"next expand left: %i, next expand right: %i, shift: %i\"\n",
       "   490        14         27.0      1.9      0.0                                  % (expand_left, expand_right, shift)\n",
       "   491                                                                       )\n",
       "   492                                           \n",
       "   493        14     355728.0  25409.1      0.0                          model_shift = copy.deepcopy(model)\n",
       "   494        14        131.0      9.4      0.0                          model_shift.loss_history = []\n",
       "   495        14        169.0     12.1      0.0                          model_shift.loss_color = []\n",
       "   496                                           \n",
       "   497       252 3605632852.0 14308066.9     56.0                          mb.tl.train_modified_kernel(\n",
       "   498        14         28.0      2.0      0.0                              model_shift,\n",
       "   499        14         26.0      1.9      0.0                              train,\n",
       "   500        14         28.0      2.0      0.0                              kernel_i=i,\n",
       "   501        14         28.0      2.0      0.0                              shift=shift,\n",
       "   502        14         28.0      2.0      0.0                              expand_left=expand_left,\n",
       "   503        14         28.0      2.0      0.0                              expand_right=expand_right,\n",
       "   504        14         28.0      2.0      0.0                              device=device,\n",
       "   505        14         31.0      2.2      0.0                              num_epochs=num_epochs,\n",
       "   506        14         27.0      1.9      0.0                              early_stopping=early_stopping,\n",
       "   507        14         28.0      2.0      0.0                              log_each=log_each,\n",
       "   508        14         27.0      1.9      0.0                              update_grad_i=i,\n",
       "   509        14         27.0      1.9      0.0                              lr=next_lr,\n",
       "   510        14         30.0      2.1      0.0                              weight_decay=next_weight_decay,\n",
       "   511        14         26.0      1.9      0.0                              optimiser=optimiser,\n",
       "   512        14         28.0      2.0      0.0                              criterion=criterion,\n",
       "   513        14         28.0      2.0      0.0                              dirichlet_regularization=dirichlet_regularization,\n",
       "   514        14         28.0      2.0      0.0                              exp_max=exp_max,\n",
       "   515        14         28.0      2.0      0.0                              verbose=verbose,\n",
       "   516        14         26.0      1.9      0.0                              **kwargs,\n",
       "   517                                                                   )\n",
       "   518        14       2822.0    201.6      0.0                          model_shift.loss_color += list(np.repeat(next_color, len(model_shift.loss_history)))\n",
       "   519                                                                   # print('history left', len(model_left.loss_history))\n",
       "   520        14         43.0      3.1      0.0                          all_options.append([expand_left, expand_right, shift, model_shift, model_shift.best_loss])\n",
       "   521                                                                   # print('\\n')\n",
       "   522                                           \n",
       "   523        14         28.0      2.0      0.0                          if verbose != 0:\n",
       "   524        14        148.0     10.6      0.0                              print(\"after opt.\")\n",
       "   525        14         33.0      2.4      0.0                              if show_logo:\n",
       "   526                                                                           mb.pl.conv_mono(model_shift)\n",
       "   527                                           \n",
       "   528                                                               # for shift, model_shift, loss in all_shifts:\n",
       "   529                                                               #     print('shift=%i' % shift, 'loss=%.4f' % loss)\n",
       "   530        18       1637.0     90.9      0.0                      best = sorted(\n",
       "   531         9         22.0      2.4      0.0                          all_options + [[0, 0, 0, model, best_loss]],\n",
       "   532         9         19.0      2.1      0.0                          key=lambda x: x[-1],\n",
       "   533                                                               )\n",
       "   534         9         19.0      2.1      0.0                      if verbose != 0:\n",
       "   535         9         99.0     11.0      0.0                          print(\"sorted\")\n",
       "   536        18       5054.0    280.8      0.0                      best_df = pd.DataFrame(\n",
       "   537        18         53.0      2.9      0.0                          [\n",
       "   538                                                                       [expand_left, expand_right, shift, loss]\n",
       "   539         9         19.0      2.1      0.0                              for expand_left, expand_right, shift, model_shift, loss in best\n",
       "   540                                                                   ],\n",
       "   541         9         18.0      2.0      0.0                          columns=[\"expand.left\", \"expand.right\", \"shift\", \"loss\"],\n",
       "   542                                                               )\n",
       "   543         9         22.0      2.4      0.0                      if verbose != 0:\n",
       "   544         9      19012.0   2112.4      0.0                          print(best_df.sort_values(\"loss\"))\n",
       "   545                                                               # for shift, model_shift, loss in best:\n",
       "   546                                                               #     print('shift=%i' % shift, 'loss=%.4f' % loss)\n",
       "   547                                           \n",
       "   548                                                               # print('\\n history len')\n",
       "   549         9        894.0     99.3      0.0                      next_expand_left, next_expand_right, next_position, next_model, next_loss = best[0]\n",
       "   550         9         20.0      2.2      0.0                      if verbose != 0:\n",
       "   551         9        106.0     11.8      0.0                          print(\"action: %s\\n\" % str((next_expand_left, next_expand_right, next_position)))\n",
       "   552                                           \n",
       "   553         9         19.0      2.1      0.0                      if next_position != 0:\n",
       "   554                                                                   next_model.loss_history = model.loss_history + next_model.loss_history\n",
       "   555                                                                   next_model.loss_color = model.loss_color + next_model.loss_color\n",
       "   556                                           \n",
       "   557         9     213410.0  23712.2      0.0                      model = copy.deepcopy(next_model)\n",
       "   558                                           \n",
       "   559         4          8.0      2.0      0.0          if show_logo:\n",
       "   560                                                       if verbose != 0:\n",
       "   561                                                           print(\"after shift optimz model\")\n",
       "   562                                                       mb.pl.plot_activities(model, train)\n",
       "   563                                                       mb.pl.conv_mono(model)\n",
       "   564                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   565                                                       mb.pl.plot_loss(model)\n",
       "   566                                                       print(\"\")\n",
       "   567                                           \n",
       "   568                                                   # the first kernel does not require an additional fit.\n",
       "   569         4          8.0      2.0      0.0          if i == 0:\n",
       "   570         1          3.0      3.0      0.0              continue\n",
       "   571                                           \n",
       "   572         3          6.0      2.0      0.0          if verbose != 0:\n",
       "   573         3         38.0     12.7      0.0              print(\"\\n\\nfinal refinement step (after shift)...\")\n",
       "   574         3         25.0      8.3      0.0              print(\"\\nunfreezing all layers for final refinement\")\n",
       "   575                                           \n",
       "   576        15         33.0      2.2      0.0          for ki in range(n_kernels):\n",
       "   577        12         23.0      1.9      0.0              if verbose != 0:\n",
       "   578        12        117.0      9.8      0.0                  print(\"kernel grad (%i) = %i \\n\" % (ki, True), sep=\", \", end=\"\")\n",
       "   579        12       2591.0    215.9      0.0              model.update_grad(ki, ki == i)\n",
       "   580         3          6.0      2.0      0.0          if verbose != 0:\n",
       "   581         3         26.0      8.7      0.0              print(\"\")\n",
       "   582                                           \n",
       "   583         3        169.0     56.3      0.0          next_optimiser = (\n",
       "   584         3       2070.0    690.0      0.0              topti.Adam(model.parameters(), lr=next_lr, weight_decay=next_weight_decay)\n",
       "   585         3          6.0      2.0      0.0              if optimiser is None\n",
       "   586                                                       else optimiser(model.parameters(), lr=next_lr)\n",
       "   587                                                   )\n",
       "   588                                           \n",
       "   589                                                   # mask kernels to avoid using weights from further steps into early ones.\n",
       "   590         3          6.0      2.0      0.0          if ignore_kernel:\n",
       "   591                                                       model.set_ignore_kernel(np.array([0 for i in range(i + 1)] + [1 for i in range(i + 1, n_kernels)]))\n",
       "   592         3          6.0      2.0      0.0          if verbose != 0:\n",
       "   593         3         63.0     21.0      0.0              print(\"kernels mask\", model.get_ignore_kernel())\n",
       "   594                                                   # assert False\n",
       "   595         6  590405563.0 98400927.2      9.2          mb.tl.train_network(\n",
       "   596         3          6.0      2.0      0.0              model,\n",
       "   597         3          6.0      2.0      0.0              train,\n",
       "   598         3          6.0      2.0      0.0              device,\n",
       "   599         3          7.0      2.3      0.0              next_optimiser,\n",
       "   600         3          6.0      2.0      0.0              criterion,\n",
       "   601         3          6.0      2.0      0.0              num_epochs=num_epochs,\n",
       "   602         3          7.0      2.3      0.0              early_stopping=early_stopping,\n",
       "   603         3          6.0      2.0      0.0              log_each=log_each,\n",
       "   604         3          6.0      2.0      0.0              dirichlet_regularization=dirichlet_regularization,\n",
       "   605         3          4.0      1.3      0.0              verbose=verbose,\n",
       "   606                                                   )\n",
       "   607                                           \n",
       "   608                                                   # load the best model after the final refinement\n",
       "   609         3        561.0    187.0      0.0          model.loss_color += list(np.repeat(colors[i], len(model.loss_history) - len(model.loss_color)))\n",
       "   610         3      14624.0   4874.7      0.0          model.load_state_dict(model.best_model_state)\n",
       "   611                                           \n",
       "   612         3          6.0      2.0      0.0          if stop_at_kernel is not None and stop_at_kernel == i:\n",
       "   613                                                       break\n",
       "   614                                           \n",
       "   615         3          5.0      1.7      0.0          if show_logo:\n",
       "   616                                                       print(\"\\n##final motif signal (after final refinement)\")\n",
       "   617                                                       mb.pl.plot_activities(model, train)\n",
       "   618                                                       mb.pl.conv_mono(model)\n",
       "   619                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   620                                                       # mb.pl.plot_loss(model)\n",
       "   621                                           \n",
       "   622         3         63.0     21.0      0.0          print('best loss', model.best_loss)\n",
       "   623                                                   # if i == 1:\n",
       "   624                                                   #     assert False\n",
       "   625                                           \n",
       "   626                                               # r = [k_parms, w, n_feat, l_best]\n",
       "   627                                               # # print(r)\n",
       "   628                                               # res.append(r)\n",
       "   629                                           \n",
       "   630         1          2.0      2.0      0.0      return model, model.best_loss"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f mb.tl.train_iterative model, best_loss = mb.tl.train_iterative(train, device, num_epochs=500, show_logo=False, early_stopping=50, log_each=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ac5b37c-09a4-4be3-9824-29486534406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_rev = train.dataset.store_rev\n",
    "i, batch = enumerate(train).__next__()\n",
    "mononuc = batch[\"mononuc\"].to(device)\n",
    "b = batch[\"batch\"].to(device) if \"batch\" in batch else None\n",
    "rounds = batch[\"rounds\"].to(device) if \"rounds\" in batch else None\n",
    "countsum = batch[\"countsum\"].to(device) if \"countsum\" in batch else None\n",
    "residues = batch[\"residues\"].to(device) if \"residues\" in batch else None\n",
    "protein_id = batch[\"protein_id\"].to(device) if \"protein_id\" in batch else None\n",
    "inputs = {\"mono\": mononuc, \"batch\": b, \"countsum\": countsum}\n",
    "if store_rev:\n",
    "    mononuc_rev = batch[\"mononuc_rev\"].to(device)\n",
    "    inputs[\"mono_rev\"] = mononuc_rev\n",
    "if residues is not None:\n",
    "    inputs[\"residues\"] = residues\n",
    "if protein_id is not None:\n",
    "    inputs[\"protein_id\"] = protein_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "697dbb57-81cf-4ced-8d8f-9baf9b73aa41",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.030239 s\n",
       "File: /home/johanna/ICB/multibind/multibind/models/models.py\n",
       "Function: forward at line 81\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    81                                               def forward(self, mono, **kwargs):\n",
       "    82                                                   # mono_rev=None, di=None, di_rev=None, batch=None, countsum=None, residues=None, protein_id=None):\n",
       "    83         1          1.0      1.0      0.0          mono_rev = kwargs.get(\"mono_rev\", None)\n",
       "    84         1          0.0      0.0      0.0          di = kwargs.get(\"di\", None)\n",
       "    85         1          0.0      0.0      0.0          di_rev = kwargs.get(\"di_rev\", None)\n",
       "    86         1        177.0    177.0      0.6          mono = self.padding(mono)\n",
       "    87         1          1.0      1.0      0.0          if mono_rev is None:\n",
       "    88         1        195.0    195.0      0.6              mono_rev = mb.tl.mono2revmono(mono)\n",
       "    89                                                   else:\n",
       "    90                                                       mono_rev = self.padding(mono_rev)\n",
       "    91                                           \n",
       "    92                                                   # prepare the dinucleotide objects if we need them\n",
       "    93         1          1.0      1.0      0.0          if self.use_dinuc:\n",
       "    94                                                       if di is None:\n",
       "    95                                                           di = mb.tl.mono2dinuc(mono)\n",
       "    96                                                       if di_rev is None:\n",
       "    97                                                           di_rev = mb.tl.mono2dinuc(mono_rev)\n",
       "    98                                                       di = torch.unsqueeze(di, 1)\n",
       "    99                                                       di_rev = torch.unsqueeze(di_rev, 1)\n",
       "   100                                                       kwargs[\"di\"] = di\n",
       "   101                                                       kwargs[\"di_rev\"] = di_rev\n",
       "   102                                           \n",
       "   103                                                   # unsqueeze mono after preparing di and unsqueezing mono\n",
       "   104         1          7.0      7.0      0.0          mono_rev = torch.unsqueeze(mono_rev, 1)\n",
       "   105         1          2.0      2.0      0.0          mono = torch.unsqueeze(mono, 1)\n",
       "   106                                           \n",
       "   107                                                   # binding_per_mode: matrix of size [batchsize, number of binding modes]\n",
       "   108         1      28711.0  28711.0     94.9          binding_per_mode = self.binding_modes(mono=mono, mono_rev=mono_rev, **kwargs)\n",
       "   109         1       1141.0   1141.0      3.8          binding_scores = self.activities(binding_per_mode, **kwargs)\n",
       "   110                                           \n",
       "   111         1          2.0      2.0      0.0          if self.datatype == \"pbm\":\n",
       "   112         1          1.0      1.0      0.0              return binding_scores\n",
       "   113                                                   elif self.datatype == \"selex\":\n",
       "   114                                                       return self.selex_module(binding_scores, **kwargs)\n",
       "   115                                                   else:\n",
       "   116                                                       return None  # this line should never be called"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f model.forward model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67bfcbc-67f9-41d4-9947-7dbf12e229b7",
   "metadata": {},
   "source": [
    "# Genomics Dataset (ATAC) - joint learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b12846-092a-471c-869a-a20f3e75fec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp3t23lcnb\n",
      "genome hg38 False\n",
      "options\n",
      "/home/johanna/ICB/annotations/hg38/genome/hg38.fa\n",
      "True /home/johanna/ICB/annotations/hg38/genome/hg38.fa\n",
      "running bedtools...\n",
      "bedtools getfasta -fi /home/johanna/ICB/annotations/hg38/genome/hg38.fa -bed /tmp/tmp3t23lcnb -fo /tmp/tmp7im5j941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10246, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = mb.bindome.datasets.scATAC.PBMCs_10x_v2(datadir='../../atac_poisson_study/data/')\n",
    "peak_ids = adata.var_names\n",
    "adata.var['summit'] = ((adata.var['end'] + adata.var['start']) / 2).astype(int)\n",
    "adata.var['summit.start'] = adata.var['summit'] - 100\n",
    "adata.var['summit.end'] = adata.var['summit'] + 100\n",
    "adata.var['k.summit'] = adata.var['chr'] + ':' + adata.var['summit.start'].astype(str) + '-' + adata.var['summit.end'].astype(str)\n",
    "n_seqs = 1000\n",
    "seqs = mb.bindome.tl.get_sequences_from_bed(adata.var[['chr', 'summit.start', 'summit.end']].head(n_seqs), genome='hg38', uppercase=True)\n",
    "keys = set([s[0] for s in seqs])\n",
    "adata = adata[:,adata.var['k.summit'].isin(keys)]\n",
    "# seqs = [[s[0], s[1].upper()] for s in seqs[0]]\n",
    "adata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d375b515-4dfb-4933-86e6-66308898f961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1000, 21)\n"
     ]
    }
   ],
   "source": [
    "# remove Ns\n",
    "seqs = [[s[0], s[1].replace('N', '')] for s in seqs]\n",
    "counts = adata.X.T\n",
    "next_data = pd.DataFrame.sparse.from_spmatrix(counts)\n",
    "next_data = next_data[range(20)].copy()\n",
    "next_data['seq'] = [s[1] for s in seqs]\n",
    "var = []\n",
    "for ri, r in next_data.iterrows():\n",
    "    if ri % 10000 == 0:\n",
    "        print(ri, next_data.shape)\n",
    "    # print(ri, r.values[:-1], r.values[:-1].var())\n",
    "    var.append(r.values[:-1].var())\n",
    "    # break\n",
    "next_data['var'] = var\n",
    "top_var = next_data[['var']].sort_values('var', ascending=False).index[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86797120-7ce4-4573-be63-511abbda3458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_data = next_data.head(10000)\n",
    "next_data_sel = next_data.reindex(top_var).reset_index(drop=True)\n",
    "del next_data_sel['var']\n",
    "next_data_sel.index = next_data_sel['seq']\n",
    "del next_data_sel['seq']\n",
    "df = next_data_sel\n",
    "dataset = mb.datasets.GenomicsDataset(df)\n",
    "train = tdata.DataLoader(dataset=dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6ed0bde-5aa6-4cf5-ae74-04a080f189f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next w 15 <class 'int'>\n",
      "# cells 20\n",
      "\n",
      "Kernel to optimize 0\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 1\n",
      "setting grad status of kernel at 1 to 0\n",
      "setting grad status of kernel at 2 to 0\n",
      "setting grad status of kernel at 3 to 0\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 1648303.976266 , best epoch: 47 secs per epoch: 4.097 s\n",
      "Epoch: 101, Loss: 1648304.770570 , best epoch: 95 secs per epoch: 3.895 s\n",
      "Epoch: 151, Loss: 1648305.180380 , best epoch: 138 secs per epoch: 3.866 s\n",
      "Epoch: 189, Loss: 1648304.0016 , best epoch: 138 secs per epoch: 3.867 s\n",
      "early stop!\n",
      "total time: 727.041 s\n",
      "secs per epoch: 3.867 s\n",
      "\n",
      "Kernel to optimize 1\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 0\n",
      "setting grad status of kernel at 1 to 1\n",
      "setting grad status of kernel at 2 to 0\n",
      "setting grad status of kernel at 3 to 0\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 732531.079905 , best epoch: 49 secs per epoch: 4.810 s\n",
      "Epoch: 101, Loss: 732505.983386 , best epoch: 93 secs per epoch: 4.689 s\n",
      "Epoch: 151, Loss: 732498.491297 , best epoch: 130 secs per epoch: 4.728 s\n",
      "Epoch: 181, Loss: 732501.8236 , best epoch: 130 secs per epoch: 4.739 s\n",
      "early stop!\n",
      "total time: 852.990 s\n",
      "secs per epoch: 4.739 s\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 732500.958070 , best epoch: 38 secs per epoch: 3.391 s\n",
      "Epoch: 89, Loss: 732499.7816 , best epoch: 38 secs per epoch: 3.381 s\n",
      "early stop!\n",
      "total time: 297.488 s\n",
      "secs per epoch: 3.381 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 732502.143196 , best epoch: 11 secs per epoch: 3.654 s\n",
      "Epoch: 62, Loss: 732499.1922 , best epoch: 11 secs per epoch: 3.631 s\n",
      "early stop!\n",
      "total time: 221.461 s\n",
      "secs per epoch: 3.631 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 732498.013449 , best epoch: 22 secs per epoch: 3.567 s\n",
      "Epoch: 101, Loss: 732501.557753 , best epoch: 99 secs per epoch: 3.408 s\n",
      "Epoch: 150, Loss: 732502.1384 , best epoch: 99 secs per epoch: 3.452 s\n",
      "early stop!\n",
      "total time: 514.332 s\n",
      "secs per epoch: 3.452 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 732499.780854 , best epoch: 37 secs per epoch: 3.524 s\n",
      "Epoch: 101, Loss: 732500.965981 , best epoch: 92 secs per epoch: 3.461 s\n",
      "Epoch: 151, Loss: 732502.738924 , best epoch: 129 secs per epoch: 3.471 s\n",
      "Epoch: 180, Loss: 732502.1464 , best epoch: 129 secs per epoch: 3.468 s\n",
      "early stop!\n",
      "total time: 620.809 s\n",
      "secs per epoch: 3.468 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift           loss\n",
      "0            1             1      0  732494.458861\n",
      "1            2             2      0  732495.632120\n",
      "2            2             1      0  732496.231804\n",
      "3            1             2      0  732496.822785\n",
      "4            0             0      0  732498.015032\n",
      "action: (1, 1, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (again)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 732502.143987 , best epoch: 7 secs per epoch: 3.628 s\n",
      "Epoch: 58, Loss: 732503.3299 , best epoch: 7 secs per epoch: 3.640 s\n",
      "early stop!\n",
      "total time: 207.463 s\n",
      "secs per epoch: 3.640 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 732502.731804 , best epoch: 44 secs per epoch: 3.792 s\n",
      "Epoch: 95, Loss: 732499.7809 , best epoch: 44 secs per epoch: 3.720 s\n",
      "early stop!\n",
      "total time: 349.699 s\n",
      "secs per epoch: 3.720 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 732501.558544 , best epoch: 4 secs per epoch: 3.680 s\n",
      "Epoch: 55, Loss: 732503.9193 , best epoch: 4 secs per epoch: 3.667 s\n",
      "early stop!\n",
      "total time: 198.030 s\n",
      "secs per epoch: 3.667 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift           loss\n",
      "0            0             0      0  732494.458861\n",
      "1            2             1      0  732495.636076\n",
      "2            1             2      0  732496.238924\n",
      "3            1             1      0  732496.822785\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "sorted\n",
      "   expand.left  expand.right  shift           loss\n",
      "0            0             0      0  732494.458861\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "\n",
      "final refinement step (after shift)...\n",
      "\n",
      "unfreezing all layers for final refinement\n",
      "kernel grad (0) = 1 \n",
      "kernel grad (1) = 1 \n",
      "kernel grad (2) = 1 \n",
      "kernel grad (3) = 1 \n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 732502.143196 , best epoch: 4 secs per epoch: 3.630 s\n",
      "Epoch: 55, Loss: 732501.5451 , best epoch: 4 secs per epoch: 3.610 s\n",
      "early stop!\n",
      "total time: 194.963 s\n",
      "secs per epoch: 3.610 s\n",
      "best loss 732498.0031645569\n",
      "\n",
      "Kernel to optimize 2\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 0\n",
      "setting grad status of kernel at 1 to 0\n",
      "setting grad status of kernel at 2 to 1\n",
      "setting grad status of kernel at 3 to 0\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 183087.613133 , best epoch: 48 secs per epoch: 3.562 s\n",
      "Epoch: 101, Loss: 183067.864320 , best epoch: 95 secs per epoch: 3.557 s\n",
      "Epoch: 151, Loss: 183064.607991 , best epoch: 138 secs per epoch: 3.615 s\n",
      "Epoch: 189, Loss: 183067.8815 , best epoch: 138 secs per epoch: 3.618 s\n",
      "early stop!\n",
      "total time: 680.175 s\n",
      "secs per epoch: 3.618 s\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 183066.371835 , best epoch: 26 secs per epoch: 2.307 s\n",
      "Epoch: 101, Loss: 183066.668710 , best epoch: 85 secs per epoch: 2.325 s\n",
      "Epoch: 136, Loss: 183065.7795 , best epoch: 85 secs per epoch: 2.323 s\n",
      "early stop!\n",
      "total time: 313.666 s\n",
      "secs per epoch: 2.323 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 183064.894581 , best epoch: 25 secs per epoch: 2.463 s\n",
      "Epoch: 76, Loss: 183066.6659 , best epoch: 25 secs per epoch: 2.447 s\n",
      "early stop!\n",
      "total time: 183.531 s\n",
      "secs per epoch: 2.447 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 183067.552017 , best epoch: 21 secs per epoch: 2.406 s\n",
      "Epoch: 72, Loss: 183066.0763 , best epoch: 21 secs per epoch: 2.387 s\n",
      "early stop!\n",
      "total time: 169.485 s\n",
      "secs per epoch: 2.387 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 183066.076741 , best epoch: 27 secs per epoch: 2.463 s\n",
      "Epoch: 78, Loss: 183065.7795 , best epoch: 27 secs per epoch: 2.462 s\n",
      "early stop!\n",
      "total time: 189.559 s\n",
      "secs per epoch: 2.462 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift           loss\n",
      "0            2             1      0  183063.713805\n",
      "1            1             1      0  183064.304984\n",
      "2            0             0      0  183064.518394\n",
      "3            1             2      0  183064.600672\n",
      "4            2             2      0  183064.891416\n",
      "action: (2, 1, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (again)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 183066.964992 , best epoch: 29 secs per epoch: 2.573 s\n",
      "Epoch: 101, Loss: 183065.483188 , best epoch: 69 secs per epoch: 2.566 s\n",
      "Epoch: 120, Loss: 183066.0807 , best epoch: 69 secs per epoch: 2.564 s\n",
      "early stop!\n",
      "total time: 305.152 s\n",
      "secs per epoch: 2.564 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift           loss\n",
      "0            0             0      0  183063.713805\n",
      "1            1             1      0  183064.010285\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "sorted\n",
      "   expand.left  expand.right  shift           loss\n",
      "0            0             0      0  183063.713805\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "\n",
      "final refinement step (after shift)...\n",
      "\n",
      "unfreezing all layers for final refinement\n",
      "kernel grad (0) = 1 \n",
      "kernel grad (1) = 1 \n",
      "kernel grad (2) = 1 \n",
      "kernel grad (3) = 1 \n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 183066.665941 , best epoch: 31 secs per epoch: 2.354 s\n",
      "Epoch: 82, Loss: 183066.6673 , best epoch: 31 secs per epoch: 2.369 s\n",
      "early stop!\n",
      "total time: 191.902 s\n",
      "secs per epoch: 2.369 s\n",
      "best loss 183064.01226265822\n",
      "\n",
      "Kernel to optimize 3\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 0\n",
      "setting grad status of kernel at 1 to 0\n",
      "setting grad status of kernel at 2 to 0\n",
      "setting grad status of kernel at 3 to 1\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 2.250547 , best epoch: 49 secs per epoch: 2.608 s\n",
      "Epoch: 101, Loss: 0.422822 , best epoch: 99 secs per epoch: 2.514 s\n",
      "Epoch: 151, Loss: 0.261688 , best epoch: 149 secs per epoch: 2.511 s\n",
      "Epoch: 201, Loss: 0.242080 , best epoch: 193 secs per epoch: 2.499 s\n",
      "Epoch: 251, Loss: 0.242017 , best epoch: 249 secs per epoch: 2.486 s\n",
      "Epoch: 301, Loss: 0.226132 , best epoch: 277 secs per epoch: 2.488 s\n",
      "Epoch: 351, Loss: 0.224113 , best epoch: 342 secs per epoch: 2.481 s\n",
      "Epoch: 401, Loss: 0.222888 , best epoch: 396 secs per epoch: 2.483 s\n",
      "Epoch: 451, Loss: 0.211584 , best epoch: 448 secs per epoch: 2.477 s\n",
      "total time: 1232.682 s\n",
      "secs per epoch: 2.470 s\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.204487 , best epoch: 49 secs per epoch: 0.994 s\n",
      "Epoch: 101, Loss: 0.204373 , best epoch: 90 secs per epoch: 0.996 s\n",
      "Epoch: 151, Loss: 0.207265 , best epoch: 134 secs per epoch: 0.993 s\n",
      "Epoch: 201, Loss: 0.207491 , best epoch: 177 secs per epoch: 0.987 s\n",
      "Epoch: 228, Loss: 0.2095 , best epoch: 177 secs per epoch: 0.985 s\n",
      "early stop!\n",
      "total time: 223.651 s\n",
      "secs per epoch: 0.985 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.204427 , best epoch: 46 secs per epoch: 1.111 s\n",
      "Epoch: 101, Loss: 0.203084 , best epoch: 88 secs per epoch: 1.106 s\n",
      "Epoch: 151, Loss: 0.207671 , best epoch: 117 secs per epoch: 1.106 s\n",
      "Epoch: 168, Loss: 0.2062 , best epoch: 117 secs per epoch: 1.105 s\n",
      "early stop!\n",
      "total time: 184.577 s\n",
      "secs per epoch: 1.105 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.204163 , best epoch: 42 secs per epoch: 1.130 s\n",
      "Epoch: 101, Loss: 0.200778 , best epoch: 97 secs per epoch: 1.120 s\n",
      "Epoch: 151, Loss: 0.197604 , best epoch: 118 secs per epoch: 1.112 s\n",
      "Epoch: 169, Loss: 0.1987 , best epoch: 118 secs per epoch: 1.110 s\n",
      "early stop!\n",
      "total time: 186.445 s\n",
      "secs per epoch: 1.110 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.203361 , best epoch: 36 secs per epoch: 1.138 s\n",
      "Epoch: 101, Loss: 0.199687 , best epoch: 96 secs per epoch: 1.144 s\n",
      "Epoch: 147, Loss: 0.2003 , best epoch: 96 secs per epoch: 1.142 s\n",
      "early stop!\n",
      "total time: 166.685 s\n",
      "secs per epoch: 1.142 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            2             1      0  0.195773\n",
      "1            2             2      0  0.198128\n",
      "2            1             2      0  0.202078\n",
      "3            1             1      0  0.203184\n",
      "4            0             0      0  0.208375\n",
      "action: (2, 1, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (again)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.198770 , best epoch: 4 secs per epoch: 1.258 s\n",
      "Epoch: 55, Loss: 0.2168 , best epoch: 4 secs per epoch: 1.257 s\n",
      "early stop!\n",
      "total time: 67.896 s\n",
      "secs per epoch: 1.257 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            1             1      0  0.195276\n",
      "1            0             0      0  0.195773\n",
      "action: (1, 1, 0)\n",
      "\n",
      "stop. Reached maximum w...\n",
      "stop. Reached maximum w...\n",
      "\n",
      "\n",
      "final refinement step (after shift)...\n",
      "\n",
      "unfreezing all layers for final refinement\n",
      "kernel grad (0) = 1 \n",
      "kernel grad (1) = 1 \n",
      "kernel grad (2) = 1 \n",
      "kernel grad (3) = 1 \n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.200852 , best epoch: 11 secs per epoch: 1.275 s\n",
      "Epoch: 62, Loss: 0.1971 , best epoch: 11 secs per epoch: 1.274 s\n",
      "early stop!\n",
      "total time: 77.697 s\n",
      "secs per epoch: 1.274 s\n",
      "best loss 0.1955678237106981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 8357.71 s\n",
       "File: /home/johanna/ICB/multibind/multibind/tl/prediction.py\n",
       "Function: train_iterative at line 232\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   232                                           def train_iterative(\n",
       "   233                                               train,\n",
       "   234                                               device,\n",
       "   235                                               n_kernels=4,\n",
       "   236                                               w=15,\n",
       "   237                                               # min_w=10,\n",
       "   238                                               max_w=20,\n",
       "   239                                               num_epochs=100,\n",
       "   240                                               early_stopping=15,\n",
       "   241                                               log_each=10,\n",
       "   242                                               opt_kernel_shift=True,\n",
       "   243                                               opt_kernel_length=True,\n",
       "   244                                               expand_length_max=3,\n",
       "   245                                               expand_length_step=1,\n",
       "   246                                               show_logo=False,\n",
       "   247                                               optimiser=None,\n",
       "   248                                               criterion=None,\n",
       "   249                                               seed=None,\n",
       "   250                                               init_random=False,\n",
       "   251                                               lr=0.01,\n",
       "   252                                               joint_learning=False,\n",
       "   253                                               ignore_kernel=False,\n",
       "   254                                               weight_decay=0.001,\n",
       "   255                                               stop_at_kernel=None,\n",
       "   256                                               dirichlet_regularization=0,\n",
       "   257                                               verbose=2,\n",
       "   258                                               exp_max=40,\n",
       "   259                                               shift_max=3,\n",
       "   260                                               shift_step=2,\n",
       "   261                                               **kwargs,\n",
       "   262                                           ):\n",
       "   263                                               # color for visualization of history\n",
       "   264         1          3.0      3.0      0.0      colors = [\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#984ea3\", \"#ff7f00\", \"#ffff33\", \"#a65628\"]\n",
       "   265         1          2.0      2.0      0.0      if verbose != 0:\n",
       "   266         1        121.0    121.0      0.0          print(\"next w\", w, type(w))\n",
       "   267                                           \n",
       "   268         1          5.0      5.0      0.0      if isinstance(train.dataset, mb.datasets.SelexDataset):\n",
       "   269                                                   if criterion is None:\n",
       "   270                                                       criterion = mb.tl.PoissonLoss()\n",
       "   271                                           \n",
       "   272                                                   n_rounds = train.dataset.n_rounds\n",
       "   273                                                   n_batches = train.dataset.n_batches\n",
       "   274                                                   enr_series = train.dataset.enr_series\n",
       "   275                                                   if verbose != 0:\n",
       "   276                                                       print(\"# rounds\", n_rounds)\n",
       "   277                                                       print(\"# batches\", n_batches)\n",
       "   278                                                       print(\"# enr_series\", enr_series)\n",
       "   279                                           \n",
       "   280                                                   model = mb.models.Multibind(\n",
       "   281                                                       datatype=\"selex\",\n",
       "   282                                                       kernels=[0] + [w] * (n_kernels - 1),\n",
       "   283                                                       n_rounds=n_rounds,\n",
       "   284                                                       init_random=init_random,\n",
       "   285                                                       n_batches=n_batches,\n",
       "   286                                                       enr_series=enr_series,\n",
       "   287                                                       **kwargs,\n",
       "   288                                                   ).to(device)\n",
       "   289         1          2.0      2.0      0.0      elif isinstance(train.dataset, mb.datasets.PBMDataset):\n",
       "   290                                                   if criterion is None:\n",
       "   291                                                       criterion = mb.tl.MSELoss()\n",
       "   292                                           \n",
       "   293                                                   n_proteins = train.dataset.n_proteins\n",
       "   294                                                   if verbose != 0:\n",
       "   295                                                       print(\"# proteins\", n_proteins)\n",
       "   296                                           \n",
       "   297                                                   if joint_learning:\n",
       "   298                                                       model = mb.models.Multibind(\n",
       "   299                                                           datatype=\"pbm\",\n",
       "   300                                                           kernels=[0] + [w] * (n_kernels - 1),\n",
       "   301                                                           init_random=init_random,\n",
       "   302                                                           n_batches=n_proteins,\n",
       "   303                                                           **kwargs,\n",
       "   304                                                       ).to(device)\n",
       "   305                                                   else:\n",
       "   306                                                       bm_generator = mb.models.BMCollection(n_proteins=n_proteins, n_kernels=n_kernels, init_random=init_random)\n",
       "   307                                                       model = mb.models.Multibind(\n",
       "   308                                                           datatype=\"pbm\",\n",
       "   309                                                           init_random=init_random,\n",
       "   310                                                           n_proteins=n_proteins,\n",
       "   311                                                           bm_generator=bm_generator,\n",
       "   312                                                           n_kernels=n_kernels,\n",
       "   313                                                           **kwargs,\n",
       "   314                                                       ).to(device)\n",
       "   315         1          2.0      2.0      0.0      elif isinstance(train.dataset, mb.datasets.GenomicsDataset):\n",
       "   316         1          2.0      2.0      0.0          if criterion is None:\n",
       "   317         1         59.0     59.0      0.0              criterion = mb.tl.MSELoss()\n",
       "   318                                           \n",
       "   319         1          3.0      3.0      0.0          n_proteins = train.dataset.n_cells\n",
       "   320         1          2.0      2.0      0.0          if verbose != 0:\n",
       "   321         1         16.0     16.0      0.0              print(\"# cells\", n_proteins)\n",
       "   322                                           \n",
       "   323         1          2.0      2.0      0.0          if joint_learning:\n",
       "   324         4      19765.0   4941.2      0.0              model = mb.models.Multibind(\n",
       "   325         1          2.0      2.0      0.0                  datatype=\"pbm\",\n",
       "   326         1          3.0      3.0      0.0                  kernels=[0] + [w] * (n_kernels - 1),\n",
       "   327         1          2.0      2.0      0.0                  init_random=init_random,\n",
       "   328         1          2.0      2.0      0.0                  n_batches=n_proteins,\n",
       "   329         1          2.0      2.0      0.0                  **kwargs,\n",
       "   330         1        805.0    805.0      0.0              ).to(device)\n",
       "   331                                                   else:\n",
       "   332                                                       bm_generator = mb.models.BMCollection(n_proteins=n_proteins, n_kernels=n_kernels, init_random=init_random)\n",
       "   333                                                       model = mb.models.Multibind(\n",
       "   334                                                           datatype=\"pbm\",\n",
       "   335                                                           init_random=init_random,\n",
       "   336                                                           n_proteins=n_proteins,\n",
       "   337                                                           bm_generator=bm_generator,\n",
       "   338                                                           n_kernels=n_kernels,\n",
       "   339                                                           **kwargs,\n",
       "   340                                                       ).to(device)\n",
       "   341                                               elif isinstance(train.dataset, mb.datasets.ResiduePBMDataset):\n",
       "   342                                                   model = mb.models.Multibind(\n",
       "   343                                                       datatype=\"pbm\",\n",
       "   344                                                       init_random=init_random,\n",
       "   345                                                       bm_generator=mb.models.BMPrediction(num_classes=1, input_size=21, hidden_size=2, num_layers=1,\n",
       "   346                                                                                           seq_length=train.dataset.get_max_residue_length()),\n",
       "   347                                                       **kwargs,\n",
       "   348                                                   ).to(device)\n",
       "   349                                               else:\n",
       "   350                                                   assert False  # not implemented yet\n",
       "   351                                           \n",
       "   352                                               # this sets up the seed at the first position\n",
       "   353         1          4.0      4.0      0.0      if seed is not None:\n",
       "   354                                                   # this sets up the seed at the first position\n",
       "   355                                                   for i, s, min_w, max_w in seed:\n",
       "   356                                                       if s is not None:\n",
       "   357                                                           print(i, s)\n",
       "   358                                                           model.set_seed(s, i, min=min_w, max=max_w)\n",
       "   359                                                   model = model.to(device)\n",
       "   360                                           \n",
       "   361                                               # step 1) freeze everything before the current binding mode\n",
       "   362         5         14.0      2.8      0.0      for i in range(0, n_kernels):\n",
       "   363         4         11.0      2.8      0.0          if verbose != 0:\n",
       "   364         4         63.0     15.8      0.0              print(\"\\nKernel to optimize %i\" % i)\n",
       "   365         4         46.0     11.5      0.0              print(\"\\nFreezing kernels\")\n",
       "   366        20         54.0      2.7      0.0          for ki in range(n_kernels):\n",
       "   367        16         39.0      2.4      0.0              if verbose != 0:\n",
       "   368        16        199.0     12.4      0.0                  print(\"setting grad status of kernel at %i to %i\" % (ki, ki == i))\n",
       "   369        16        870.0     54.4      0.0              model.update_grad(ki, ki == i)\n",
       "   370         4         45.0     11.2      0.0          print(\"\\n\")\n",
       "   371                                           \n",
       "   372         4         10.0      2.5      0.0          if show_logo:\n",
       "   373                                                       if verbose != 0:\n",
       "   374                                                           print(\"before kernel optimization.\")\n",
       "   375                                                       mb.pl.plot_activities(model, train)\n",
       "   376                                                       mb.pl.conv_mono(model)\n",
       "   377                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   378                                           \n",
       "   379         4         11.0      2.8      0.0          next_lr = lr if not isinstance(lr, list) else lr[i]\n",
       "   380         4         11.0      2.8      0.0          next_weight_decay = weight_decay if not isinstance(weight_decay, list) else weight_decay[i]\n",
       "   381                                           \n",
       "   382         4         24.0      6.0      0.0          next_optimiser = (\n",
       "   383         4        758.0    189.5      0.0              topti.Adam(model.parameters(), lr=next_lr, weight_decay=next_weight_decay)\n",
       "   384         4         11.0      2.8      0.0              if optimiser is None\n",
       "   385                                                       else optimiser(model.parameters(), lr=next_lr)\n",
       "   386                                                   )\n",
       "   387                                           \n",
       "   388                                                   # mask kernels to avoid using weights from further steps into early ones.\n",
       "   389         4         11.0      2.8      0.0          if ignore_kernel:\n",
       "   390                                                       model.set_ignore_kernel(np.array([0 for i in range(i + 1)] + [1 for i in range(i + 1, n_kernels)]))\n",
       "   391                                           \n",
       "   392         4         11.0      2.8      0.0          if verbose != 0:\n",
       "   393         4        110.0     27.5      0.0              print(\"kernels mask\", model.get_ignore_kernel())\n",
       "   394                                           \n",
       "   395                                                   # assert False\n",
       "   396         8 3492888871.0 436611108.9     41.8          mb.tl.train_network(\n",
       "   397         4         10.0      2.5      0.0              model,\n",
       "   398         4          8.0      2.0      0.0              train,\n",
       "   399         4         10.0      2.5      0.0              device,\n",
       "   400         4         11.0      2.8      0.0              next_optimiser,\n",
       "   401         4          9.0      2.2      0.0              criterion,\n",
       "   402         4         10.0      2.5      0.0              num_epochs=num_epochs,\n",
       "   403         4          8.0      2.0      0.0              early_stopping=early_stopping,\n",
       "   404         4         10.0      2.5      0.0              log_each=log_each,\n",
       "   405         4          9.0      2.2      0.0              dirichlet_regularization=dirichlet_regularization,\n",
       "   406         4         10.0      2.5      0.0              exp_max=exp_max,\n",
       "   407         4          9.0      2.2      0.0              verbose=verbose,\n",
       "   408                                                   )\n",
       "   409                                                   # print('next color', colors[i])\n",
       "   410         4        628.0    157.0      0.0          model.loss_color += list(np.repeat(colors[i], len(model.loss_history) - len(model.loss_color)))\n",
       "   411                                                   # probably here load the state of the best epoch and save\n",
       "   412         4       1795.0    448.8      0.0          model.load_state_dict(model.best_model_state)\n",
       "   413         4         12.0      3.0      0.0          \"%i\" % w\n",
       "   414                                                   # store model parameters and fit for later visualization\n",
       "   415         4      29195.0   7298.8      0.0          model = copy.deepcopy(model)\n",
       "   416                                                   # optimizer for left / right flanks\n",
       "   417         4         11.0      2.8      0.0          best_loss = model.best_loss\n",
       "   418                                           \n",
       "   419         4         10.0      2.5      0.0          if show_logo:\n",
       "   420                                                       print(\"\\n##After kernel opt / before shift optim.\")\n",
       "   421                                                       mb.pl.plot_activities(model, train)\n",
       "   422                                                       mb.pl.conv_mono(model)\n",
       "   423                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   424                                                       mb.pl.plot_loss(model)\n",
       "   425                                           \n",
       "   426                                                   # print(model_by_k[k_parms].loss_color)\n",
       "   427                                                   #######\n",
       "   428                                                   # optimize the flanks through +1/-1 shifts\n",
       "   429                                                   #######\n",
       "   430         4          8.0      2.0      0.0          n_attempts = 0\n",
       "   431                                           \n",
       "   432         4          9.0      2.2      0.0          if (opt_kernel_shift or opt_kernel_length) and i != 0:\n",
       "   433                                           \n",
       "   434         3         10.0      3.3      0.0              opt_expand_left = range(1, expand_length_max, expand_length_step)\n",
       "   435         3         15.0      5.0      0.0              opt_expand_right = range(1, expand_length_max, expand_length_step)\n",
       "   436         3         11.0      3.7      0.0              opt_shift = [0] + list(range(-shift_max, shift_max + 1, shift_step))\n",
       "   437                                           \n",
       "   438        12         33.0      2.8      0.0              for opt_option_text, opt_option_next in zip(\n",
       "   439         3          8.0      2.7      0.0                  [\"FLANKS\", \"SHIFT\"], [[opt_expand_left, opt_expand_right, [0]], [[0], [0], opt_shift]]\n",
       "   440                                                       ):\n",
       "   441                                           \n",
       "   442                                                           # print(opt_option_text, opt_option_next)\n",
       "   443                                                           # assert False\n",
       "   444                                           \n",
       "   445         6         13.0      2.2      0.0                  next_loss = None\n",
       "   446        14         41.0      2.9      0.0                  while next_loss is None or next_loss < best_loss:\n",
       "   447        10         22.0      2.2      0.0                      n_attempts += 1\n",
       "   448                                           \n",
       "   449        10        231.0     23.1      0.0                      curr_w = model.get_kernel_width(i)\n",
       "   450        10         24.0      2.4      0.0                      if curr_w >= max_w:\n",
       "   451         2         19.0      9.5      0.0                          print(\"stop. Reached maximum w...\")\n",
       "   452         2          4.0      2.0      0.0                          break\n",
       "   453                                           \n",
       "   454         8         18.0      2.2      0.0                      if verbose != 0:\n",
       "   455        16        137.0      8.6      0.0                          print(\n",
       "   456         8         24.0      3.0      0.0                              \"\\noptimize_motif_shift (%s)...\" % (\"first\" if next_loss is None else \"again\"),\n",
       "   457         8         19.0      2.4      0.0                              end=\"\",\n",
       "   458                                                                   )\n",
       "   459         8         87.0     10.9      0.0                          print(\"\")\n",
       "   460         8      48285.0   6035.6      0.0                      model = copy.deepcopy(model)\n",
       "   461         8         26.0      3.2      0.0                      best_loss = model.best_loss\n",
       "   462         8         22.0      2.8      0.0                      next_color = colors[-(1 if n_attempts % 2 == 0 else -2)]\n",
       "   463                                           \n",
       "   464         8         16.0      2.0      0.0                      all_options = []\n",
       "   465                                           \n",
       "   466        16         78.0      4.9      0.0                      options = [\n",
       "   467                                                                   [expand_left, expand_right, shift]\n",
       "   468         8         18.0      2.2      0.0                          for expand_left in opt_option_next[0]\n",
       "   469                                                                   for expand_right in opt_option_next[1]\n",
       "   470                                                                   for shift in opt_option_next[2]\n",
       "   471                                                               ]\n",
       "   472                                           \n",
       "   473                                                               # print(options)\n",
       "   474                                           \n",
       "   475        42        104.0      2.5      0.0                      for expand_left, expand_right, shift in options:\n",
       "   476                                           \n",
       "   477        34         93.0      2.7      0.0                          if abs(expand_left) + abs(expand_right) + abs(shift) == 0:\n",
       "   478         2          4.0      2.0      0.0                              continue\n",
       "   479        32         70.0      2.2      0.0                          if abs(shift) > 0:  # skip shift for now.\n",
       "   480         8         16.0      2.0      0.0                              continue\n",
       "   481        24         56.0      2.3      0.0                          if curr_w + expand_left + expand_right > max_w:\n",
       "   482         7         15.0      2.1      0.0                              continue\n",
       "   483                                           \n",
       "   484                                                                   # print(expand_left, expand_right, shift)\n",
       "   485                                                                   # assert False\n",
       "   486                                           \n",
       "   487        17         36.0      2.1      0.0                          if verbose != 0:\n",
       "   488        34        235.0      6.9      0.0                              print(\n",
       "   489        34         88.0      2.6      0.0                                  \"next expand left: %i, next expand right: %i, shift: %i\"\n",
       "   490        17         39.0      2.3      0.0                                  % (expand_left, expand_right, shift)\n",
       "   491                                                                       )\n",
       "   492                                           \n",
       "   493        17     119148.0   7008.7      0.0                          model_shift = copy.deepcopy(model)\n",
       "   494        17        163.0      9.6      0.0                          model_shift.loss_history = []\n",
       "   495        17        213.0     12.5      0.0                          model_shift.loss_color = []\n",
       "   496                                           \n",
       "   497       306 4399943083.0 14378899.0     52.6                          mb.tl.train_modified_kernel(\n",
       "   498        17         40.0      2.4      0.0                              model_shift,\n",
       "   499        17         40.0      2.4      0.0                              train,\n",
       "   500        17         40.0      2.4      0.0                              kernel_i=i,\n",
       "   501        17         38.0      2.2      0.0                              shift=shift,\n",
       "   502        17         36.0      2.1      0.0                              expand_left=expand_left,\n",
       "   503        17         39.0      2.3      0.0                              expand_right=expand_right,\n",
       "   504        17         37.0      2.2      0.0                              device=device,\n",
       "   505        17         38.0      2.2      0.0                              num_epochs=num_epochs,\n",
       "   506        17         36.0      2.1      0.0                              early_stopping=early_stopping,\n",
       "   507        17         38.0      2.2      0.0                              log_each=log_each,\n",
       "   508        17         38.0      2.2      0.0                              update_grad_i=i,\n",
       "   509        17         37.0      2.2      0.0                              lr=next_lr,\n",
       "   510        17         37.0      2.2      0.0                              weight_decay=next_weight_decay,\n",
       "   511        17         36.0      2.1      0.0                              optimiser=optimiser,\n",
       "   512        17         38.0      2.2      0.0                              criterion=criterion,\n",
       "   513        17         36.0      2.1      0.0                              dirichlet_regularization=dirichlet_regularization,\n",
       "   514        17         38.0      2.2      0.0                              exp_max=exp_max,\n",
       "   515        17         38.0      2.2      0.0                              verbose=verbose,\n",
       "   516        17         34.0      2.0      0.0                              **kwargs,\n",
       "   517                                                                   )\n",
       "   518        17       2330.0    137.1      0.0                          model_shift.loss_color += list(np.repeat(next_color, len(model_shift.loss_history)))\n",
       "   519                                                                   # print('history left', len(model_left.loss_history))\n",
       "   520        17         55.0      3.2      0.0                          all_options.append([expand_left, expand_right, shift, model_shift, model_shift.best_loss])\n",
       "   521                                                                   # print('\\n')\n",
       "   522                                           \n",
       "   523        17         40.0      2.4      0.0                          if verbose != 0:\n",
       "   524        17        217.0     12.8      0.0                              print(\"after opt.\")\n",
       "   525        17         47.0      2.8      0.0                              if show_logo:\n",
       "   526                                                                           mb.pl.conv_mono(model_shift)\n",
       "   527                                           \n",
       "   528                                                               # for shift, model_shift, loss in all_shifts:\n",
       "   529                                                               #     print('shift=%i' % shift, 'loss=%.4f' % loss)\n",
       "   530        16        624.0     39.0      0.0                      best = sorted(\n",
       "   531         8         23.0      2.9      0.0                          all_options + [[0, 0, 0, model, best_loss]],\n",
       "   532         8         19.0      2.4      0.0                          key=lambda x: x[-1],\n",
       "   533                                                               )\n",
       "   534         8         17.0      2.1      0.0                      if verbose != 0:\n",
       "   535         8         91.0     11.4      0.0                          print(\"sorted\")\n",
       "   536        16       5300.0    331.2      0.0                      best_df = pd.DataFrame(\n",
       "   537        16         58.0      3.6      0.0                          [\n",
       "   538                                                                       [expand_left, expand_right, shift, loss]\n",
       "   539         8         18.0      2.2      0.0                              for expand_left, expand_right, shift, model_shift, loss in best\n",
       "   540                                                                   ],\n",
       "   541         8         17.0      2.1      0.0                          columns=[\"expand.left\", \"expand.right\", \"shift\", \"loss\"],\n",
       "   542                                                               )\n",
       "   543         8         22.0      2.8      0.0                      if verbose != 0:\n",
       "   544         8      35338.0   4417.2      0.0                          print(best_df.sort_values(\"loss\"))\n",
       "   545                                                               # for shift, model_shift, loss in best:\n",
       "   546                                                               #     print('shift=%i' % shift, 'loss=%.4f' % loss)\n",
       "   547                                           \n",
       "   548                                                               # print('\\n history len')\n",
       "   549         8        279.0     34.9      0.0                      next_expand_left, next_expand_right, next_position, next_model, next_loss = best[0]\n",
       "   550         8         21.0      2.6      0.0                      if verbose != 0:\n",
       "   551         8        113.0     14.1      0.0                          print(\"action: %s\\n\" % str((next_expand_left, next_expand_right, next_position)))\n",
       "   552                                           \n",
       "   553         8         19.0      2.4      0.0                      if next_position != 0:\n",
       "   554                                                                   next_model.loss_history = model.loss_history + next_model.loss_history\n",
       "   555                                                                   next_model.loss_color = model.loss_color + next_model.loss_color\n",
       "   556                                           \n",
       "   557         8      39114.0   4889.2      0.0                      model = copy.deepcopy(next_model)\n",
       "   558                                           \n",
       "   559         4          8.0      2.0      0.0          if show_logo:\n",
       "   560                                                       if verbose != 0:\n",
       "   561                                                           print(\"after shift optimz model\")\n",
       "   562                                                       mb.pl.plot_activities(model, train)\n",
       "   563                                                       mb.pl.conv_mono(model)\n",
       "   564                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   565                                                       mb.pl.plot_loss(model)\n",
       "   566                                                       print(\"\")\n",
       "   567                                           \n",
       "   568                                                   # the first kernel does not require an additional fit.\n",
       "   569         4          8.0      2.0      0.0          if i == 0:\n",
       "   570         1          3.0      3.0      0.0              continue\n",
       "   571                                           \n",
       "   572         3          6.0      2.0      0.0          if verbose != 0:\n",
       "   573         3         34.0     11.3      0.0              print(\"\\n\\nfinal refinement step (after shift)...\")\n",
       "   574         3         26.0      8.7      0.0              print(\"\\nunfreezing all layers for final refinement\")\n",
       "   575                                           \n",
       "   576        15         32.0      2.1      0.0          for ki in range(n_kernels):\n",
       "   577        12         22.0      1.8      0.0              if verbose != 0:\n",
       "   578        12        115.0      9.6      0.0                  print(\"kernel grad (%i) = %i \\n\" % (ki, True), sep=\", \", end=\"\")\n",
       "   579        12        503.0     41.9      0.0              model.update_grad(ki, ki == i)\n",
       "   580         3          6.0      2.0      0.0          if verbose != 0:\n",
       "   581         3         27.0      9.0      0.0              print(\"\")\n",
       "   582                                           \n",
       "   583         3         34.0     11.3      0.0          next_optimiser = (\n",
       "   584         3        429.0    143.0      0.0              topti.Adam(model.parameters(), lr=next_lr, weight_decay=next_weight_decay)\n",
       "   585         3          6.0      2.0      0.0              if optimiser is None\n",
       "   586                                                       else optimiser(model.parameters(), lr=next_lr)\n",
       "   587                                                   )\n",
       "   588                                           \n",
       "   589                                                   # mask kernels to avoid using weights from further steps into early ones.\n",
       "   590         3          5.0      1.7      0.0          if ignore_kernel:\n",
       "   591                                                       model.set_ignore_kernel(np.array([0 for i in range(i + 1)] + [1 for i in range(i + 1, n_kernels)]))\n",
       "   592         3          6.0      2.0      0.0          if verbose != 0:\n",
       "   593         3         61.0     20.3      0.0              print(\"kernels mask\", model.get_ignore_kernel())\n",
       "   594                                                   # assert False\n",
       "   595         6  464563252.0 77427208.7      5.6          mb.tl.train_network(\n",
       "   596         3          6.0      2.0      0.0              model,\n",
       "   597         3          6.0      2.0      0.0              train,\n",
       "   598         3          5.0      1.7      0.0              device,\n",
       "   599         3          6.0      2.0      0.0              next_optimiser,\n",
       "   600         3          5.0      1.7      0.0              criterion,\n",
       "   601         3          5.0      1.7      0.0              num_epochs=num_epochs,\n",
       "   602         3          5.0      1.7      0.0              early_stopping=early_stopping,\n",
       "   603         3          4.0      1.3      0.0              log_each=log_each,\n",
       "   604         3          5.0      1.7      0.0              dirichlet_regularization=dirichlet_regularization,\n",
       "   605         3          6.0      2.0      0.0              verbose=verbose,\n",
       "   606                                                   )\n",
       "   607                                           \n",
       "   608                                                   # load the best model after the final refinement\n",
       "   609         3        542.0    180.7      0.0          model.loss_color += list(np.repeat(colors[i], len(model.loss_history) - len(model.loss_color)))\n",
       "   610         3       1369.0    456.3      0.0          model.load_state_dict(model.best_model_state)\n",
       "   611                                           \n",
       "   612         3          6.0      2.0      0.0          if stop_at_kernel is not None and stop_at_kernel == i:\n",
       "   613                                                       break\n",
       "   614                                           \n",
       "   615         3          6.0      2.0      0.0          if show_logo:\n",
       "   616                                                       print(\"\\n##final motif signal (after final refinement)\")\n",
       "   617                                                       mb.pl.plot_activities(model, train)\n",
       "   618                                                       mb.pl.conv_mono(model)\n",
       "   619                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   620                                                       # mb.pl.plot_loss(model)\n",
       "   621                                           \n",
       "   622         3         68.0     22.7      0.0          print('best loss', model.best_loss)\n",
       "   623                                                   # if i == 1:\n",
       "   624                                                   #     assert False\n",
       "   625                                           \n",
       "   626                                               # r = [k_parms, w, n_feat, l_best]\n",
       "   627                                               # # print(r)\n",
       "   628                                               # res.append(r)\n",
       "   629                                           \n",
       "   630         1          2.0      2.0      0.0      return model, model.best_loss"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f mb.tl.train_iterative model, best_loss = mb.tl.train_iterative(train, device, joint_learning=True, num_epochs=500, show_logo=False, early_stopping=50, log_each=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e191384-8a2f-4747-81af-ade9de098000",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_rev = train.dataset.store_rev\n",
    "i, batch = enumerate(train).__next__()\n",
    "mononuc = batch[\"mononuc\"].to(device)\n",
    "b = batch[\"batch\"].to(device) if \"batch\" in batch else None\n",
    "rounds = batch[\"rounds\"].to(device) if \"rounds\" in batch else None\n",
    "countsum = batch[\"countsum\"].to(device) if \"countsum\" in batch else None\n",
    "residues = batch[\"residues\"].to(device) if \"residues\" in batch else None\n",
    "protein_id = batch[\"protein_id\"].to(device) if \"protein_id\" in batch else None\n",
    "inputs = {\"mono\": mononuc, \"batch\": b, \"countsum\": countsum}\n",
    "if store_rev:\n",
    "    mononuc_rev = batch[\"mononuc_rev\"].to(device)\n",
    "    inputs[\"mono_rev\"] = mononuc_rev\n",
    "if residues is not None:\n",
    "    inputs[\"residues\"] = residues\n",
    "if protein_id is not None:\n",
    "    inputs[\"protein_id\"] = protein_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53305703-d706-4856-8d35-ec7da1ddea52",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.012556 s\n",
       "File: /home/johanna/ICB/multibind/multibind/models/models.py\n",
       "Function: forward at line 81\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    81                                               def forward(self, mono, **kwargs):\n",
       "    82                                                   # mono_rev=None, di=None, di_rev=None, batch=None, countsum=None, residues=None, protein_id=None):\n",
       "    83         1          1.0      1.0      0.0          mono_rev = kwargs.get(\"mono_rev\", None)\n",
       "    84         1          1.0      1.0      0.0          di = kwargs.get(\"di\", None)\n",
       "    85         1          1.0      1.0      0.0          di_rev = kwargs.get(\"di_rev\", None)\n",
       "    86         1        265.0    265.0      2.1          mono = self.padding(mono)\n",
       "    87         1          1.0      1.0      0.0          if mono_rev is None:\n",
       "    88         1        395.0    395.0      3.1              mono_rev = mb.tl.mono2revmono(mono)\n",
       "    89                                                   else:\n",
       "    90                                                       mono_rev = self.padding(mono_rev)\n",
       "    91                                           \n",
       "    92                                                   # prepare the dinucleotide objects if we need them\n",
       "    93         1          1.0      1.0      0.0          if self.use_dinuc:\n",
       "    94                                                       if di is None:\n",
       "    95                                                           di = mb.tl.mono2dinuc(mono)\n",
       "    96                                                       if di_rev is None:\n",
       "    97                                                           di_rev = mb.tl.mono2dinuc(mono_rev)\n",
       "    98                                                       di = torch.unsqueeze(di, 1)\n",
       "    99                                                       di_rev = torch.unsqueeze(di_rev, 1)\n",
       "   100                                                       kwargs[\"di\"] = di\n",
       "   101                                                       kwargs[\"di_rev\"] = di_rev\n",
       "   102                                           \n",
       "   103                                                   # unsqueeze mono after preparing di and unsqueezing mono\n",
       "   104         1          7.0      7.0      0.1          mono_rev = torch.unsqueeze(mono_rev, 1)\n",
       "   105         1          2.0      2.0      0.0          mono = torch.unsqueeze(mono, 1)\n",
       "   106                                           \n",
       "   107                                                   # binding_per_mode: matrix of size [batchsize, number of binding modes]\n",
       "   108         1      10642.0  10642.0     84.8          binding_per_mode = self.binding_modes(mono=mono, mono_rev=mono_rev, **kwargs)\n",
       "   109         1       1238.0   1238.0      9.9          binding_scores = self.activities(binding_per_mode, **kwargs)\n",
       "   110                                           \n",
       "   111         1          2.0      2.0      0.0          if self.datatype == \"pbm\":\n",
       "   112         1          0.0      0.0      0.0              return binding_scores\n",
       "   113                                                   elif self.datatype == \"selex\":\n",
       "   114                                                       return self.selex_module(binding_scores, **kwargs)\n",
       "   115                                                   else:\n",
       "   116                                                       return None  # this line should never be called"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f model.forward model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074d490-1631-4960-be5e-1a07d842eaa7",
   "metadata": {},
   "source": [
    "# PBM after improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0775813-e10a-4c93-a62c-fa192ff6f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_path = os.path.join(bd.constants.ANNOTATIONS_DIRECTORY, 'pbm', 'affreg', 'PbmDataHom6_norm.mat')\n",
    "mat = scipy.io.loadmat(matlab_path)\n",
    "data = mat['PbmData'][0]\n",
    "seqs_dna =  data[0][5]\n",
    "seqs_dna = [s[0][0] for s in seqs_dna]\n",
    "# load the MSA sequences, one hot encoded\n",
    "df, signal = bd.datasets.PBM.pbm_homeo_affreg()\n",
    "# x, y = pickle.load(open('../../data/example_homeo_PbmData.pkl', 'rb'))\n",
    "# x, y = pickle.load(open('annotations/pbm/example_homeo_PbmData.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acd2ecac-c16d-45a2-9797-cbe15ebf2352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a small subsample\n",
    "# x = x[1:6]\n",
    "seqs_dna = seqs_dna[0:1000]\n",
    "signal = signal[0:1, 0:1000]\n",
    "\n",
    "# shift signal by adding a constant s.t. no negative values are included\n",
    "signal -= np.min(signal)\n",
    "\n",
    "# Set up the dataset\n",
    "df = pd.DataFrame(signal.T)\n",
    "df['seq'] = seqs_dna\n",
    "df.index = df['seq']\n",
    "del df['seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e384d91-5efc-4cd5-8d23-017ab1a1c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = mb.datasets.PBMDataset(df)\n",
    "train = tdata.DataLoader(dataset=dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fd0f506-615c-451e-a3dd-befd38258a98",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next w 15 <class 'int'>\n",
      "# proteins 1\n",
      "\n",
      "Kernel to optimize 0\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 1\n",
      "setting grad status of kernel at 1 to 0\n",
      "setting grad status of kernel at 2 to 0\n",
      "setting grad status of kernel at 3 to 0\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 196404.035156 , best epoch: 49 secs per epoch: 0.079 s\n",
      "Epoch: 101, Loss: 196289.984375 , best epoch: 99 secs per epoch: 0.077 s\n",
      "Epoch: 151, Loss: 196249.402344 , best epoch: 149 secs per epoch: 0.077 s\n",
      "Epoch: 201, Loss: 196230.250000 , best epoch: 199 secs per epoch: 0.077 s\n",
      "Epoch: 251, Loss: 196219.617188 , best epoch: 245 secs per epoch: 0.078 s\n",
      "Epoch: 301, Loss: 196212.281250 , best epoch: 299 secs per epoch: 0.078 s\n",
      "Epoch: 351, Loss: 196208.464844 , best epoch: 343 secs per epoch: 0.078 s\n",
      "Epoch: 401, Loss: 196205.296875 , best epoch: 396 secs per epoch: 0.078 s\n",
      "Epoch: 451, Loss: 196202.699219 , best epoch: 428 secs per epoch: 0.078 s\n",
      "total time: 38.861 s\n",
      "secs per epoch: 0.078 s\n",
      "\n",
      "Kernel to optimize 1\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 0\n",
      "setting grad status of kernel at 1 to 1\n",
      "setting grad status of kernel at 2 to 0\n",
      "setting grad status of kernel at 3 to 0\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 87571.564453 , best epoch: 49 secs per epoch: 0.092 s\n",
      "Epoch: 101, Loss: 87219.275391 , best epoch: 99 secs per epoch: 0.090 s\n",
      "Epoch: 151, Loss: 87118.826172 , best epoch: 149 secs per epoch: 0.092 s\n",
      "Epoch: 201, Loss: 87073.876953 , best epoch: 199 secs per epoch: 0.091 s\n",
      "Epoch: 251, Loss: 87049.035156 , best epoch: 249 secs per epoch: 0.092 s\n",
      "Epoch: 301, Loss: 87033.898438 , best epoch: 299 secs per epoch: 0.092 s\n",
      "Epoch: 351, Loss: 87025.234375 , best epoch: 347 secs per epoch: 0.093 s\n",
      "Epoch: 401, Loss: 87017.984375 , best epoch: 398 secs per epoch: 0.093 s\n",
      "Epoch: 451, Loss: 87013.398438 , best epoch: 447 secs per epoch: 0.093 s\n",
      "total time: 46.761 s\n",
      "secs per epoch: 0.094 s\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 86992.830078 , best epoch: 29 secs per epoch: 0.069 s\n",
      "Epoch: 101, Loss: 86993.443359 , best epoch: 72 secs per epoch: 0.069 s\n",
      "Epoch: 123, Loss: 86992.6367 , best epoch: 72 secs per epoch: 0.068 s\n",
      "early stop!\n",
      "total time: 8.342 s\n",
      "secs per epoch: 0.068 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 86992.476562 , best epoch: 37 secs per epoch: 0.068 s\n",
      "Epoch: 101, Loss: 86991.759766 , best epoch: 74 secs per epoch: 0.067 s\n",
      "Epoch: 125, Loss: 86993.0820 , best epoch: 74 secs per epoch: 0.067 s\n",
      "early stop!\n",
      "total time: 8.345 s\n",
      "secs per epoch: 0.067 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 86993.150391 , best epoch: 40 secs per epoch: 0.070 s\n",
      "Epoch: 101, Loss: 86992.531250 , best epoch: 79 secs per epoch: 0.069 s\n",
      "Epoch: 130, Loss: 86992.0000 , best epoch: 79 secs per epoch: 0.070 s\n",
      "early stop!\n",
      "total time: 8.994 s\n",
      "secs per epoch: 0.070 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 86992.912109 , best epoch: 16 secs per epoch: 0.066 s\n",
      "Epoch: 67, Loss: 86993.4590 , best epoch: 16 secs per epoch: 0.066 s\n",
      "early stop!\n",
      "total time: 4.346 s\n",
      "secs per epoch: 0.066 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift          loss\n",
      "0            2             1      0  86991.515625\n",
      "1            1             2      0  86991.722656\n",
      "2            1             1      0  86991.851562\n",
      "3            2             2      0  86992.248047\n",
      "4            0             0      0  87009.064453\n",
      "action: (2, 1, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (again)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 86991.976562 , best epoch: 45 secs per epoch: 0.066 s\n",
      "Epoch: 101, Loss: 86993.169922 , best epoch: 63 secs per epoch: 0.065 s\n",
      "Epoch: 114, Loss: 86992.4336 , best epoch: 63 secs per epoch: 0.064 s\n",
      "early stop!\n",
      "total time: 7.281 s\n",
      "secs per epoch: 0.064 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift          loss\n",
      "0            1             1      0  86991.439453\n",
      "1            0             0      0  86991.515625\n",
      "action: (1, 1, 0)\n",
      "\n",
      "stop. Reached maximum w...\n",
      "stop. Reached maximum w...\n",
      "\n",
      "\n",
      "final refinement step (after shift)...\n",
      "\n",
      "unfreezing all layers for final refinement\n",
      "kernel grad (0) = 1 \n",
      "kernel grad (1) = 1 \n",
      "kernel grad (2) = 1 \n",
      "kernel grad (3) = 1 \n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 86992.716797 , best epoch: 11 secs per epoch: 0.066 s\n",
      "Epoch: 62, Loss: 86992.8164 , best epoch: 11 secs per epoch: 0.066 s\n",
      "early stop!\n",
      "total time: 4.006 s\n",
      "secs per epoch: 0.066 s\n",
      "best loss 86991.423828125\n",
      "\n",
      "Kernel to optimize 2\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 0\n",
      "setting grad status of kernel at 1 to 0\n",
      "setting grad status of kernel at 2 to 1\n",
      "setting grad status of kernel at 3 to 0\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 21952.528809 , best epoch: 49 secs per epoch: 0.073 s\n",
      "Epoch: 101, Loss: 21735.856934 , best epoch: 99 secs per epoch: 0.073 s\n",
      "Epoch: 151, Loss: 21671.957031 , best epoch: 149 secs per epoch: 0.073 s\n",
      "Epoch: 201, Loss: 21643.814941 , best epoch: 199 secs per epoch: 0.073 s\n",
      "Epoch: 251, Loss: 21628.478516 , best epoch: 249 secs per epoch: 0.073 s\n",
      "Epoch: 301, Loss: 21619.559570 , best epoch: 299 secs per epoch: 0.073 s\n",
      "Epoch: 351, Loss: 21613.759277 , best epoch: 349 secs per epoch: 0.073 s\n",
      "Epoch: 401, Loss: 21608.768555 , best epoch: 399 secs per epoch: 0.073 s\n",
      "Epoch: 451, Loss: 21606.137695 , best epoch: 446 secs per epoch: 0.073 s\n",
      "total time: 36.639 s\n",
      "secs per epoch: 0.073 s\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 21593.438477 , best epoch: 38 secs per epoch: 0.051 s\n",
      "Epoch: 101, Loss: 21593.114746 , best epoch: 57 secs per epoch: 0.049 s\n",
      "Epoch: 108, Loss: 21593.1582 , best epoch: 57 secs per epoch: 0.049 s\n",
      "early stop!\n",
      "total time: 5.251 s\n",
      "secs per epoch: 0.049 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 21593.311035 , best epoch: 40 secs per epoch: 0.047 s\n",
      "Epoch: 91, Loss: 21592.8960 , best epoch: 40 secs per epoch: 0.047 s\n",
      "early stop!\n",
      "total time: 4.207 s\n",
      "secs per epoch: 0.047 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 21593.164062 , best epoch: 48 secs per epoch: 0.048 s\n",
      "Epoch: 99, Loss: 21592.6304 , best epoch: 48 secs per epoch: 0.048 s\n",
      "early stop!\n",
      "total time: 4.672 s\n",
      "secs per epoch: 0.048 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 21592.970703 , best epoch: 37 secs per epoch: 0.048 s\n",
      "Epoch: 101, Loss: 21592.881348 , best epoch: 81 secs per epoch: 0.047 s\n",
      "Epoch: 151, Loss: 21592.839844 , best epoch: 130 secs per epoch: 0.047 s\n",
      "Epoch: 201, Loss: 21593.096680 , best epoch: 159 secs per epoch: 0.047 s\n",
      "Epoch: 210, Loss: 21593.2026 , best epoch: 159 secs per epoch: 0.047 s\n",
      "early stop!\n",
      "total time: 9.742 s\n",
      "secs per epoch: 0.047 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift          loss\n",
      "0            2             1      0  21592.446289\n",
      "1            2             2      0  21592.504395\n",
      "2            1             2      0  21592.622070\n",
      "3            1             1      0  21592.635254\n",
      "4            0             0      0  21603.655762\n",
      "action: (2, 1, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (again)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 21593.031250 , best epoch: 6 secs per epoch: 0.048 s\n",
      "Epoch: 57, Loss: 21592.9434 , best epoch: 6 secs per epoch: 0.048 s\n",
      "early stop!\n",
      "total time: 2.675 s\n",
      "secs per epoch: 0.048 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift          loss\n",
      "0            0             0      0  21592.446289\n",
      "1            1             1      0  21592.615234\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "sorted\n",
      "   expand.left  expand.right  shift          loss\n",
      "0            0             0      0  21592.446289\n",
      "action: (0, 0, 0)\n",
      "\n",
      "\n",
      "\n",
      "final refinement step (after shift)...\n",
      "\n",
      "unfreezing all layers for final refinement\n",
      "kernel grad (0) = 1 \n",
      "kernel grad (1) = 1 \n",
      "kernel grad (2) = 1 \n",
      "kernel grad (3) = 1 \n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 21592.939453 , best epoch: 26 secs per epoch: 0.047 s\n",
      "Epoch: 101, Loss: 21593.081055 , best epoch: 74 secs per epoch: 0.046 s\n",
      "Epoch: 125, Loss: 21593.2812 , best epoch: 74 secs per epoch: 0.046 s\n",
      "early stop!\n",
      "total time: 5.693 s\n",
      "secs per epoch: 0.046 s\n",
      "best loss 21592.419921875\n",
      "\n",
      "Kernel to optimize 3\n",
      "\n",
      "Freezing kernels\n",
      "setting grad status of kernel at 0 to 0\n",
      "setting grad status of kernel at 1 to 0\n",
      "setting grad status of kernel at 2 to 0\n",
      "setting grad status of kernel at 3 to 1\n",
      "\n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 27.926521 , best epoch: 49 secs per epoch: 0.056 s\n",
      "Epoch: 101, Loss: 10.384115 , best epoch: 99 secs per epoch: 0.055 s\n",
      "Epoch: 151, Loss: 5.231502 , best epoch: 149 secs per epoch: 0.055 s\n",
      "Epoch: 201, Loss: 3.069730 , best epoch: 199 secs per epoch: 0.055 s\n",
      "Epoch: 251, Loss: 1.984557 , best epoch: 249 secs per epoch: 0.054 s\n",
      "Epoch: 301, Loss: 1.377258 , best epoch: 299 secs per epoch: 0.054 s\n",
      "Epoch: 351, Loss: 1.009451 , best epoch: 349 secs per epoch: 0.054 s\n",
      "Epoch: 401, Loss: 0.778703 , best epoch: 399 secs per epoch: 0.053 s\n",
      "Epoch: 451, Loss: 0.622499 , best epoch: 449 secs per epoch: 0.053 s\n",
      "total time: 26.266 s\n",
      "secs per epoch: 0.053 s\n",
      "\n",
      "optimize_motif_shift (first)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.185642 , best epoch: 44 secs per epoch: 0.022 s\n",
      "Epoch: 101, Loss: 0.184191 , best epoch: 99 secs per epoch: 0.022 s\n",
      "Epoch: 151, Loss: 0.185158 , best epoch: 149 secs per epoch: 0.022 s\n",
      "Epoch: 201, Loss: 0.166259 , best epoch: 193 secs per epoch: 0.022 s\n",
      "Epoch: 251, Loss: 0.162537 , best epoch: 241 secs per epoch: 0.022 s\n",
      "Epoch: 301, Loss: 0.154061 , best epoch: 295 secs per epoch: 0.022 s\n",
      "Epoch: 351, Loss: 0.148108 , best epoch: 341 secs per epoch: 0.022 s\n",
      "Epoch: 401, Loss: 0.145966 , best epoch: 381 secs per epoch: 0.022 s\n",
      "Epoch: 451, Loss: 0.147101 , best epoch: 445 secs per epoch: 0.022 s\n",
      "total time: 10.917 s\n",
      "secs per epoch: 0.022 s\n",
      "after opt.\n",
      "next expand left: 1, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.185103 , best epoch: 49 secs per epoch: 0.023 s\n",
      "Epoch: 101, Loss: 0.184289 , best epoch: 84 secs per epoch: 0.023 s\n",
      "Epoch: 151, Loss: 0.183764 , best epoch: 147 secs per epoch: 0.023 s\n",
      "Epoch: 201, Loss: 0.167103 , best epoch: 191 secs per epoch: 0.022 s\n",
      "Epoch: 251, Loss: 0.152835 , best epoch: 247 secs per epoch: 0.023 s\n",
      "Epoch: 301, Loss: 0.100462 , best epoch: 299 secs per epoch: 0.023 s\n",
      "Epoch: 351, Loss: 0.091624 , best epoch: 347 secs per epoch: 0.023 s\n",
      "Epoch: 401, Loss: 0.087427 , best epoch: 399 secs per epoch: 0.023 s\n",
      "Epoch: 451, Loss: 0.086733 , best epoch: 446 secs per epoch: 0.023 s\n",
      "total time: 11.243 s\n",
      "secs per epoch: 0.023 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.189435 , best epoch: 43 secs per epoch: 0.023 s\n",
      "Epoch: 101, Loss: 0.184831 , best epoch: 92 secs per epoch: 0.023 s\n",
      "Epoch: 151, Loss: 0.175979 , best epoch: 149 secs per epoch: 0.023 s\n",
      "Epoch: 201, Loss: 0.166306 , best epoch: 198 secs per epoch: 0.023 s\n",
      "Epoch: 251, Loss: 0.149260 , best epoch: 249 secs per epoch: 0.023 s\n",
      "Epoch: 301, Loss: 0.098743 , best epoch: 296 secs per epoch: 0.023 s\n",
      "Epoch: 351, Loss: 0.090073 , best epoch: 348 secs per epoch: 0.023 s\n",
      "Epoch: 401, Loss: 0.086846 , best epoch: 395 secs per epoch: 0.023 s\n",
      "Epoch: 451, Loss: 0.085998 , best epoch: 449 secs per epoch: 0.023 s\n",
      "total time: 11.334 s\n",
      "secs per epoch: 0.023 s\n",
      "after opt.\n",
      "next expand left: 2, next expand right: 2, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.185866 , best epoch: 42 secs per epoch: 0.024 s\n",
      "Epoch: 101, Loss: 0.185135 , best epoch: 92 secs per epoch: 0.023 s\n",
      "Epoch: 151, Loss: 0.177816 , best epoch: 147 secs per epoch: 0.023 s\n",
      "Epoch: 201, Loss: 0.160822 , best epoch: 196 secs per epoch: 0.023 s\n",
      "Epoch: 251, Loss: 0.156927 , best epoch: 244 secs per epoch: 0.023 s\n",
      "Epoch: 301, Loss: 0.145257 , best epoch: 298 secs per epoch: 0.023 s\n",
      "Epoch: 351, Loss: 0.127197 , best epoch: 349 secs per epoch: 0.023 s\n",
      "Epoch: 401, Loss: 0.115412 , best epoch: 399 secs per epoch: 0.023 s\n",
      "Epoch: 451, Loss: 0.098382 , best epoch: 449 secs per epoch: 0.023 s\n",
      "total time: 11.495 s\n",
      "secs per epoch: 0.023 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            2             1      0  0.084993\n",
      "1            1             2      0  0.085523\n",
      "2            2             2      0  0.091909\n",
      "3            1             1      0  0.123335\n",
      "4            0             0      0  0.521671\n",
      "action: (2, 1, 0)\n",
      "\n",
      "\n",
      "optimize_motif_shift (again)...\n",
      "next expand left: 1, next expand right: 1, shift: 0\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.081819 , best epoch: 49 secs per epoch: 0.024 s\n",
      "Epoch: 101, Loss: 0.081489 , best epoch: 94 secs per epoch: 0.023 s\n",
      "Epoch: 151, Loss: 0.080274 , best epoch: 140 secs per epoch: 0.023 s\n",
      "Epoch: 191, Loss: 0.0803 , best epoch: 140 secs per epoch: 0.023 s\n",
      "early stop!\n",
      "total time: 4.403 s\n",
      "secs per epoch: 0.023 s\n",
      "after opt.\n",
      "sorted\n",
      "   expand.left  expand.right  shift      loss\n",
      "0            1             1      0  0.079556\n",
      "1            0             0      0  0.084993\n",
      "action: (1, 1, 0)\n",
      "\n",
      "stop. Reached maximum w...\n",
      "stop. Reached maximum w...\n",
      "\n",
      "\n",
      "final refinement step (after shift)...\n",
      "\n",
      "unfreezing all layers for final refinement\n",
      "kernel grad (0) = 1 \n",
      "kernel grad (1) = 1 \n",
      "kernel grad (2) = 1 \n",
      "kernel grad (3) = 1 \n",
      "\n",
      "kernels mask None\n",
      "optimizing using <class 'torch.optim.adam.Adam'> and <class 'multibind.tl.loss.MSELoss'> n_epochs 500 early_stopping 50\n",
      "lr= 0.01, weight_decay= 0.001, dir weight= 0\n",
      "Epoch: 51, Loss: 0.080722 , best epoch: 19 secs per epoch: 0.023 s\n",
      "Epoch: 70, Loss: 0.0804 , best epoch: 19 secs per epoch: 0.024 s\n",
      "early stop!\n",
      "total time: 1.623 s\n",
      "secs per epoch: 0.024 s\n",
      "best loss 0.07979601621627808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 273.429 s\n",
       "File: /home/johanna/ICB/multibind/multibind/tl/prediction.py\n",
       "Function: train_iterative at line 232\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   232                                           def train_iterative(\n",
       "   233                                               train,\n",
       "   234                                               device,\n",
       "   235                                               n_kernels=4,\n",
       "   236                                               w=15,\n",
       "   237                                               # min_w=10,\n",
       "   238                                               max_w=20,\n",
       "   239                                               num_epochs=100,\n",
       "   240                                               early_stopping=15,\n",
       "   241                                               log_each=10,\n",
       "   242                                               opt_kernel_shift=True,\n",
       "   243                                               opt_kernel_length=True,\n",
       "   244                                               expand_length_max=3,\n",
       "   245                                               expand_length_step=1,\n",
       "   246                                               show_logo=False,\n",
       "   247                                               optimiser=None,\n",
       "   248                                               criterion=None,\n",
       "   249                                               seed=None,\n",
       "   250                                               init_random=False,\n",
       "   251                                               lr=0.01,\n",
       "   252                                               joint_learning=False,\n",
       "   253                                               ignore_kernel=False,\n",
       "   254                                               weight_decay=0.001,\n",
       "   255                                               stop_at_kernel=None,\n",
       "   256                                               dirichlet_regularization=0,\n",
       "   257                                               verbose=2,\n",
       "   258                                               exp_max=40,\n",
       "   259                                               shift_max=3,\n",
       "   260                                               shift_step=2,\n",
       "   261                                               **kwargs,\n",
       "   262                                           ):\n",
       "   263                                               # color for visualization of history\n",
       "   264         1          3.0      3.0      0.0      colors = [\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#984ea3\", \"#ff7f00\", \"#ffff33\", \"#a65628\"]\n",
       "   265         1          2.0      2.0      0.0      if verbose != 0:\n",
       "   266         1        102.0    102.0      0.0          print(\"next w\", w, type(w))\n",
       "   267                                           \n",
       "   268         1          3.0      3.0      0.0      if isinstance(train.dataset, mb.datasets.SelexDataset):\n",
       "   269                                                   if criterion is None:\n",
       "   270                                                       criterion = mb.tl.PoissonLoss()\n",
       "   271                                           \n",
       "   272                                                   n_rounds = train.dataset.n_rounds\n",
       "   273                                                   n_batches = train.dataset.n_batches\n",
       "   274                                                   enr_series = train.dataset.enr_series\n",
       "   275                                                   if verbose != 0:\n",
       "   276                                                       print(\"# rounds\", n_rounds)\n",
       "   277                                                       print(\"# batches\", n_batches)\n",
       "   278                                                       print(\"# enr_series\", enr_series)\n",
       "   279                                           \n",
       "   280                                                   model = mb.models.Multibind(\n",
       "   281                                                       datatype=\"selex\",\n",
       "   282                                                       kernels=[0] + [w] * (n_kernels - 1),\n",
       "   283                                                       n_rounds=n_rounds,\n",
       "   284                                                       init_random=init_random,\n",
       "   285                                                       n_batches=n_batches,\n",
       "   286                                                       enr_series=enr_series,\n",
       "   287                                                       **kwargs,\n",
       "   288                                                   ).to(device)\n",
       "   289         1          2.0      2.0      0.0      elif isinstance(train.dataset, mb.datasets.PBMDataset) or isinstance(train.dataset, mb.datasets.GenomicsDataset):\n",
       "   290         1          2.0      2.0      0.0          if criterion is None:\n",
       "   291         1         57.0     57.0      0.0              criterion = mb.tl.MSELoss()\n",
       "   292         1          2.0      2.0      0.0          if isinstance(train.dataset, mb.datasets.PBMDataset):\n",
       "   293         1          2.0      2.0      0.0              n_proteins = train.dataset.n_proteins\n",
       "   294                                                   else:\n",
       "   295                                                       n_proteins = train.dataset.n_cells\n",
       "   296         1          2.0      2.0      0.0          if verbose != 0:\n",
       "   297         1         16.0     16.0      0.0              print(\"# proteins\", n_proteins)\n",
       "   298                                           \n",
       "   299         1          3.0      3.0      0.0          if joint_learning or n_proteins == 1:\n",
       "   300         4       1789.0    447.2      0.0              model = mb.models.Multibind(\n",
       "   301         1          2.0      2.0      0.0                  datatype=\"pbm\",\n",
       "   302         1          3.0      3.0      0.0                  kernels=[0] + [w] * (n_kernels - 1),\n",
       "   303         1          2.0      2.0      0.0                  init_random=init_random,\n",
       "   304         1          2.0      2.0      0.0                  n_batches=n_proteins,\n",
       "   305         1          2.0      2.0      0.0                  **kwargs,\n",
       "   306         1        173.0    173.0      0.0              ).to(device)\n",
       "   307                                                   else:\n",
       "   308                                                       bm_generator = mb.models.BMCollection(n_proteins=n_proteins, n_kernels=n_kernels, init_random=init_random)\n",
       "   309                                                       model = mb.models.Multibind(\n",
       "   310                                                           datatype=\"pbm\",\n",
       "   311                                                           init_random=init_random,\n",
       "   312                                                           n_proteins=n_proteins,\n",
       "   313                                                           bm_generator=bm_generator,\n",
       "   314                                                           n_kernels=n_kernels,\n",
       "   315                                                           **kwargs,\n",
       "   316                                                       ).to(device)\n",
       "   317                                               elif isinstance(train.dataset, mb.datasets.ResiduePBMDataset):\n",
       "   318                                                   model = mb.models.Multibind(\n",
       "   319                                                       datatype=\"pbm\",\n",
       "   320                                                       init_random=init_random,\n",
       "   321                                                       bm_generator=mb.models.BMPrediction(num_classes=1, input_size=21, hidden_size=2, num_layers=1,\n",
       "   322                                                                                           seq_length=train.dataset.get_max_residue_length()),\n",
       "   323                                                       **kwargs,\n",
       "   324                                                   ).to(device)\n",
       "   325                                               else:\n",
       "   326                                                   assert False  # not implemented yet\n",
       "   327                                           \n",
       "   328                                               # this sets up the seed at the first position\n",
       "   329         1          2.0      2.0      0.0      if seed is not None:\n",
       "   330                                                   # this sets up the seed at the first position\n",
       "   331                                                   for i, s, min_w, max_w in seed:\n",
       "   332                                                       if s is not None:\n",
       "   333                                                           print(i, s)\n",
       "   334                                                           model.set_seed(s, i, min=min_w, max=max_w)\n",
       "   335                                                   model = model.to(device)\n",
       "   336                                           \n",
       "   337                                               # step 1) freeze everything before the current binding mode\n",
       "   338         5         14.0      2.8      0.0      for i in range(0, n_kernels):\n",
       "   339         4          9.0      2.2      0.0          if verbose != 0:\n",
       "   340         4         50.0     12.5      0.0              print(\"\\nKernel to optimize %i\" % i)\n",
       "   341         4         40.0     10.0      0.0              print(\"\\nFreezing kernels\")\n",
       "   342        20         51.0      2.5      0.0          for ki in range(n_kernels):\n",
       "   343        16         37.0      2.3      0.0              if verbose != 0:\n",
       "   344        16        175.0     10.9      0.0                  print(\"setting grad status of kernel at %i to %i\" % (ki, ki == i))\n",
       "   345        16        760.0     47.5      0.0              model.update_grad(ki, ki == i)\n",
       "   346         4         41.0     10.2      0.0          print(\"\\n\")\n",
       "   347                                           \n",
       "   348         4          9.0      2.2      0.0          if show_logo:\n",
       "   349                                                       if verbose != 0:\n",
       "   350                                                           print(\"before kernel optimization.\")\n",
       "   351                                                       mb.pl.plot_activities(model, train)\n",
       "   352                                                       mb.pl.conv_mono(model)\n",
       "   353                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   354                                           \n",
       "   355         4         10.0      2.5      0.0          next_lr = lr if not isinstance(lr, list) else lr[i]\n",
       "   356         4         10.0      2.5      0.0          next_weight_decay = weight_decay if not isinstance(weight_decay, list) else weight_decay[i]\n",
       "   357                                           \n",
       "   358         4         23.0      5.8      0.0          next_optimiser = (\n",
       "   359         4        636.0    159.0      0.0              topti.Adam(model.parameters(), lr=next_lr, weight_decay=next_weight_decay)\n",
       "   360         4          8.0      2.0      0.0              if optimiser is None\n",
       "   361                                                       else optimiser(model.parameters(), lr=next_lr)\n",
       "   362                                                   )\n",
       "   363                                           \n",
       "   364                                                   # mask kernels to avoid using weights from further steps into early ones.\n",
       "   365         4         10.0      2.5      0.0          if ignore_kernel:\n",
       "   366                                                       model.set_ignore_kernel(np.array([0 for i in range(i + 1)] + [1 for i in range(i + 1, n_kernels)]))\n",
       "   367                                           \n",
       "   368         4         10.0      2.5      0.0          if verbose != 0:\n",
       "   369         4         93.0     23.2      0.0              print(\"kernels mask\", model.get_ignore_kernel())\n",
       "   370                                           \n",
       "   371                                                   # assert False\n",
       "   372         8  148527893.0 18565986.6     54.3          mb.tl.train_network(\n",
       "   373         4         10.0      2.5      0.0              model,\n",
       "   374         4          9.0      2.2      0.0              train,\n",
       "   375         4          9.0      2.2      0.0              device,\n",
       "   376         4          8.0      2.0      0.0              next_optimiser,\n",
       "   377         4          8.0      2.0      0.0              criterion,\n",
       "   378         4          8.0      2.0      0.0              num_epochs=num_epochs,\n",
       "   379         4         10.0      2.5      0.0              early_stopping=early_stopping,\n",
       "   380         4          9.0      2.2      0.0              log_each=log_each,\n",
       "   381         4          9.0      2.2      0.0              dirichlet_regularization=dirichlet_regularization,\n",
       "   382         4         10.0      2.5      0.0              exp_max=exp_max,\n",
       "   383         4         10.0      2.5      0.0              verbose=verbose,\n",
       "   384                                                   )\n",
       "   385                                                   # print('next color', colors[i])\n",
       "   386         4        825.0    206.2      0.0          model.loss_color += list(np.repeat(colors[i], len(model.loss_history) - len(model.loss_color)))\n",
       "   387                                                   # probably here load the state of the best epoch and save\n",
       "   388         4       1881.0    470.2      0.0          model.load_state_dict(model.best_model_state)\n",
       "   389         4         14.0      3.5      0.0          \"%i\" % w\n",
       "   390                                                   # store model parameters and fit for later visualization\n",
       "   391         4      39977.0   9994.2      0.0          model = copy.deepcopy(model)\n",
       "   392                                                   # optimizer for left / right flanks\n",
       "   393         4         13.0      3.2      0.0          best_loss = model.best_loss\n",
       "   394                                           \n",
       "   395         4          9.0      2.2      0.0          if show_logo:\n",
       "   396                                                       print(\"\\n##After kernel opt / before shift optim.\")\n",
       "   397                                                       mb.pl.plot_activities(model, train)\n",
       "   398                                                       mb.pl.conv_mono(model)\n",
       "   399                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   400                                                       mb.pl.plot_loss(model)\n",
       "   401                                           \n",
       "   402                                                   # print(model_by_k[k_parms].loss_color)\n",
       "   403                                                   #######\n",
       "   404                                                   # optimize the flanks through +1/-1 shifts\n",
       "   405                                                   #######\n",
       "   406         4          9.0      2.2      0.0          n_attempts = 0\n",
       "   407                                           \n",
       "   408         4          9.0      2.2      0.0          if (opt_kernel_shift or opt_kernel_length) and i != 0:\n",
       "   409                                           \n",
       "   410         3          9.0      3.0      0.0              opt_expand_left = range(1, expand_length_max, expand_length_step)\n",
       "   411         3          8.0      2.7      0.0              opt_expand_right = range(1, expand_length_max, expand_length_step)\n",
       "   412         3         12.0      4.0      0.0              opt_shift = [0] + list(range(-shift_max, shift_max + 1, shift_step))\n",
       "   413                                           \n",
       "   414        12         36.0      3.0      0.0              for opt_option_text, opt_option_next in zip(\n",
       "   415         3          8.0      2.7      0.0                  [\"FLANKS\", \"SHIFT\"], [[opt_expand_left, opt_expand_right, [0]], [[0], [0], opt_shift]]\n",
       "   416                                                       ):\n",
       "   417                                           \n",
       "   418                                                           # print(opt_option_text, opt_option_next)\n",
       "   419                                                           # assert False\n",
       "   420                                           \n",
       "   421         6         14.0      2.3      0.0                  next_loss = None\n",
       "   422        13         39.0      3.0      0.0                  while next_loss is None or next_loss < best_loss:\n",
       "   423        11         26.0      2.4      0.0                      n_attempts += 1\n",
       "   424                                           \n",
       "   425        11        261.0     23.7      0.0                      curr_w = model.get_kernel_width(i)\n",
       "   426        11         25.0      2.3      0.0                      if curr_w >= max_w:\n",
       "   427         4         56.0     14.0      0.0                          print(\"stop. Reached maximum w...\")\n",
       "   428         4         13.0      3.2      0.0                          break\n",
       "   429                                           \n",
       "   430         7         17.0      2.4      0.0                      if verbose != 0:\n",
       "   431        14        119.0      8.5      0.0                          print(\n",
       "   432         7         22.0      3.1      0.0                              \"\\noptimize_motif_shift (%s)...\" % (\"first\" if next_loss is None else \"again\"),\n",
       "   433         7         16.0      2.3      0.0                              end=\"\",\n",
       "   434                                                                   )\n",
       "   435         7         75.0     10.7      0.0                          print(\"\")\n",
       "   436         7      53123.0   7589.0      0.0                      model = copy.deepcopy(model)\n",
       "   437         7         21.0      3.0      0.0                      best_loss = model.best_loss\n",
       "   438         7         20.0      2.9      0.0                      next_color = colors[-(1 if n_attempts % 2 == 0 else -2)]\n",
       "   439                                           \n",
       "   440         7         17.0      2.4      0.0                      all_options = []\n",
       "   441                                           \n",
       "   442        14         72.0      5.1      0.0                      options = [\n",
       "   443                                                                   [expand_left, expand_right, shift]\n",
       "   444         7         17.0      2.4      0.0                          for expand_left in opt_option_next[0]\n",
       "   445                                                                   for expand_right in opt_option_next[1]\n",
       "   446                                                                   for shift in opt_option_next[2]\n",
       "   447                                                               ]\n",
       "   448                                           \n",
       "   449                                                               # print(options)\n",
       "   450                                           \n",
       "   451        36        103.0      2.9      0.0                      for expand_left, expand_right, shift in options:\n",
       "   452                                           \n",
       "   453        29         91.0      3.1      0.0                          if abs(expand_left) + abs(expand_right) + abs(shift) == 0:\n",
       "   454         1          3.0      3.0      0.0                              continue\n",
       "   455        28         69.0      2.5      0.0                          if abs(shift) > 0:  # skip shift for now.\n",
       "   456         4         10.0      2.5      0.0                              continue\n",
       "   457        24         55.0      2.3      0.0                          if curr_w + expand_left + expand_right > max_w:\n",
       "   458         9         22.0      2.4      0.0                              continue\n",
       "   459                                           \n",
       "   460                                                                   # print(expand_left, expand_right, shift)\n",
       "   461                                                                   # assert False\n",
       "   462                                           \n",
       "   463        15         36.0      2.4      0.0                          if verbose != 0:\n",
       "   464        30        221.0      7.4      0.0                              print(\n",
       "   465        30         86.0      2.9      0.0                                  \"next expand left: %i, next expand right: %i, shift: %i\"\n",
       "   466        15         37.0      2.5      0.0                                  % (expand_left, expand_right, shift)\n",
       "   467                                                                       )\n",
       "   468                                           \n",
       "   469        15     143883.0   9592.2      0.1                          model_shift = copy.deepcopy(model)\n",
       "   470        15        163.0     10.9      0.0                          model_shift.loss_history = []\n",
       "   471        15        262.0     17.5      0.0                          model_shift.loss_color = []\n",
       "   472                                           \n",
       "   473       270  113259172.0 419478.4     41.4                          mb.tl.train_modified_kernel(\n",
       "   474        15         35.0      2.3      0.0                              model_shift,\n",
       "   475        15         33.0      2.2      0.0                              train,\n",
       "   476        15         32.0      2.1      0.0                              kernel_i=i,\n",
       "   477        15         34.0      2.3      0.0                              shift=shift,\n",
       "   478        15         33.0      2.2      0.0                              expand_left=expand_left,\n",
       "   479        15         33.0      2.2      0.0                              expand_right=expand_right,\n",
       "   480        15         34.0      2.3      0.0                              device=device,\n",
       "   481        15         38.0      2.5      0.0                              num_epochs=num_epochs,\n",
       "   482        15         32.0      2.1      0.0                              early_stopping=early_stopping,\n",
       "   483        15         31.0      2.1      0.0                              log_each=log_each,\n",
       "   484        15         32.0      2.1      0.0                              update_grad_i=i,\n",
       "   485        15         36.0      2.4      0.0                              lr=next_lr,\n",
       "   486        15         37.0      2.5      0.0                              weight_decay=next_weight_decay,\n",
       "   487        15         32.0      2.1      0.0                              optimiser=optimiser,\n",
       "   488        15         35.0      2.3      0.0                              criterion=criterion,\n",
       "   489        15         32.0      2.1      0.0                              dirichlet_regularization=dirichlet_regularization,\n",
       "   490        15         33.0      2.2      0.0                              exp_max=exp_max,\n",
       "   491        15         35.0      2.3      0.0                              verbose=verbose,\n",
       "   492        15         35.0      2.3      0.0                              **kwargs,\n",
       "   493                                                                   )\n",
       "   494        15       2667.0    177.8      0.0                          model_shift.loss_color += list(np.repeat(next_color, len(model_shift.loss_history)))\n",
       "   495                                                                   # print('history left', len(model_left.loss_history))\n",
       "   496        15         50.0      3.3      0.0                          all_options.append([expand_left, expand_right, shift, model_shift, model_shift.best_loss])\n",
       "   497                                                                   # print('\\n')\n",
       "   498                                           \n",
       "   499        15         39.0      2.6      0.0                          if verbose != 0:\n",
       "   500        15        188.0     12.5      0.0                              print(\"after opt.\")\n",
       "   501        15         42.0      2.8      0.0                              if show_logo:\n",
       "   502                                                                           mb.pl.conv_mono(model_shift)\n",
       "   503                                           \n",
       "   504                                                               # for shift, model_shift, loss in all_shifts:\n",
       "   505                                                               #     print('shift=%i' % shift, 'loss=%.4f' % loss)\n",
       "   506        14        740.0     52.9      0.0                      best = sorted(\n",
       "   507         7         22.0      3.1      0.0                          all_options + [[0, 0, 0, model, best_loss]],\n",
       "   508         7         19.0      2.7      0.0                          key=lambda x: x[-1],\n",
       "   509                                                               )\n",
       "   510         7         18.0      2.6      0.0                      if verbose != 0:\n",
       "   511         7         80.0     11.4      0.0                          print(\"sorted\")\n",
       "   512        14       4942.0    353.0      0.0                      best_df = pd.DataFrame(\n",
       "   513        14         51.0      3.6      0.0                          [\n",
       "   514                                                                       [expand_left, expand_right, shift, loss]\n",
       "   515         7         17.0      2.4      0.0                              for expand_left, expand_right, shift, model_shift, loss in best\n",
       "   516                                                                   ],\n",
       "   517         7         19.0      2.7      0.0                          columns=[\"expand.left\", \"expand.right\", \"shift\", \"loss\"],\n",
       "   518                                                               )\n",
       "   519         7         21.0      3.0      0.0                      if verbose != 0:\n",
       "   520         7      18017.0   2573.9      0.0                          print(best_df.sort_values(\"loss\"))\n",
       "   521                                                               # for shift, model_shift, loss in best:\n",
       "   522                                                               #     print('shift=%i' % shift, 'loss=%.4f' % loss)\n",
       "   523                                           \n",
       "   524                                                               # print('\\n history len')\n",
       "   525         7        251.0     35.9      0.0                      next_expand_left, next_expand_right, next_position, next_model, next_loss = best[0]\n",
       "   526         7         21.0      3.0      0.0                      if verbose != 0:\n",
       "   527         7        100.0     14.3      0.0                          print(\"action: %s\\n\" % str((next_expand_left, next_expand_right, next_position)))\n",
       "   528                                           \n",
       "   529         7         19.0      2.7      0.0                      if next_position != 0:\n",
       "   530                                                                   next_model.loss_history = model.loss_history + next_model.loss_history\n",
       "   531                                                                   next_model.loss_color = model.loss_color + next_model.loss_color\n",
       "   532                                           \n",
       "   533         7      40795.0   5827.9      0.0                      model = copy.deepcopy(next_model)\n",
       "   534                                           \n",
       "   535         4         10.0      2.5      0.0          if show_logo:\n",
       "   536                                                       if verbose != 0:\n",
       "   537                                                           print(\"after shift optimz model\")\n",
       "   538                                                       mb.pl.plot_activities(model, train)\n",
       "   539                                                       mb.pl.conv_mono(model)\n",
       "   540                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   541                                                       mb.pl.plot_loss(model)\n",
       "   542                                                       print(\"\")\n",
       "   543                                           \n",
       "   544                                                   # the first kernel does not require an additional fit.\n",
       "   545         4          8.0      2.0      0.0          if i == 0:\n",
       "   546         1          3.0      3.0      0.0              continue\n",
       "   547                                           \n",
       "   548         3          7.0      2.3      0.0          if verbose != 0:\n",
       "   549         3         33.0     11.0      0.0              print(\"\\n\\nfinal refinement step (after shift)...\")\n",
       "   550         3         32.0     10.7      0.0              print(\"\\nunfreezing all layers for final refinement\")\n",
       "   551                                           \n",
       "   552        15         39.0      2.6      0.0          for ki in range(n_kernels):\n",
       "   553        12         29.0      2.4      0.0              if verbose != 0:\n",
       "   554        12        139.0     11.6      0.0                  print(\"kernel grad (%i) = %i \\n\" % (ki, True), sep=\", \", end=\"\")\n",
       "   555        12        571.0     47.6      0.0              model.update_grad(ki, ki == i)\n",
       "   556         3          7.0      2.3      0.0          if verbose != 0:\n",
       "   557         3         31.0     10.3      0.0              print(\"\")\n",
       "   558                                           \n",
       "   559         3         44.0     14.7      0.0          next_optimiser = (\n",
       "   560         3        532.0    177.3      0.0              topti.Adam(model.parameters(), lr=next_lr, weight_decay=next_weight_decay)\n",
       "   561         3          7.0      2.3      0.0              if optimiser is None\n",
       "   562                                                       else optimiser(model.parameters(), lr=next_lr)\n",
       "   563                                                   )\n",
       "   564                                           \n",
       "   565                                                   # mask kernels to avoid using weights from further steps into early ones.\n",
       "   566         3          8.0      2.7      0.0          if ignore_kernel:\n",
       "   567                                                       model.set_ignore_kernel(np.array([0 for i in range(i + 1)] + [1 for i in range(i + 1, n_kernels)]))\n",
       "   568         3          7.0      2.3      0.0          if verbose != 0:\n",
       "   569         3         73.0     24.3      0.0              print(\"kernels mask\", model.get_ignore_kernel())\n",
       "   570                                                   # assert False\n",
       "   571         6   11323604.0 1887267.3      4.1          mb.tl.train_network(\n",
       "   572         3          7.0      2.3      0.0              model,\n",
       "   573         3          7.0      2.3      0.0              train,\n",
       "   574         3          6.0      2.0      0.0              device,\n",
       "   575         3          6.0      2.0      0.0              next_optimiser,\n",
       "   576         3          6.0      2.0      0.0              criterion,\n",
       "   577         3          7.0      2.3      0.0              num_epochs=num_epochs,\n",
       "   578         3          6.0      2.0      0.0              early_stopping=early_stopping,\n",
       "   579         3          7.0      2.3      0.0              log_each=log_each,\n",
       "   580         3          6.0      2.0      0.0              dirichlet_regularization=dirichlet_regularization,\n",
       "   581         3          6.0      2.0      0.0              verbose=verbose,\n",
       "   582                                                   )\n",
       "   583                                           \n",
       "   584                                                   # load the best model after the final refinement\n",
       "   585         3        504.0    168.0      0.0          model.loss_color += list(np.repeat(colors[i], len(model.loss_history) - len(model.loss_color)))\n",
       "   586         3       1394.0    464.7      0.0          model.load_state_dict(model.best_model_state)\n",
       "   587                                           \n",
       "   588         3          8.0      2.7      0.0          if stop_at_kernel is not None and stop_at_kernel == i:\n",
       "   589                                                       break\n",
       "   590                                           \n",
       "   591         3          7.0      2.3      0.0          if show_logo:\n",
       "   592                                                       print(\"\\n##final motif signal (after final refinement)\")\n",
       "   593                                                       mb.pl.plot_activities(model, train)\n",
       "   594                                                       mb.pl.conv_mono(model)\n",
       "   595                                                       mb.pl.conv_mono(model, flip=True, log=False)\n",
       "   596                                                       # mb.pl.plot_loss(model)\n",
       "   597                                           \n",
       "   598         3         68.0     22.7      0.0          print('best loss', model.best_loss)\n",
       "   599                                                   # if i == 1:\n",
       "   600                                                   #     assert False\n",
       "   601                                           \n",
       "   602                                               # r = [k_parms, w, n_feat, l_best]\n",
       "   603                                               # # print(r)\n",
       "   604                                               # res.append(r)\n",
       "   605                                           \n",
       "   606         1          3.0      3.0      0.0      return model, model.best_loss"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f mb.tl.train_iterative model, best_loss = mb.tl.train_iterative(train, device, num_epochs=500, show_logo=False, early_stopping=50, log_each=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6adf409a-cd38-47af-96ae-077bab7ceb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_rev = train.dataset.store_rev\n",
    "i, batch = enumerate(train).__next__()\n",
    "mononuc = batch[\"mononuc\"].to(device)\n",
    "b = batch[\"batch\"].to(device) if \"batch\" in batch else None\n",
    "rounds = batch[\"rounds\"].to(device) if \"rounds\" in batch else None\n",
    "countsum = batch[\"countsum\"].to(device) if \"countsum\" in batch else None\n",
    "residues = batch[\"residues\"].to(device) if \"residues\" in batch else None\n",
    "protein_id = batch[\"protein_id\"].to(device) if \"protein_id\" in batch else None\n",
    "inputs = {\"mono\": mononuc, \"batch\": b, \"countsum\": countsum}\n",
    "if store_rev:\n",
    "    mononuc_rev = batch[\"mononuc_rev\"].to(device)\n",
    "    inputs[\"mono_rev\"] = mononuc_rev\n",
    "if residues is not None:\n",
    "    inputs[\"residues\"] = residues\n",
    "if protein_id is not None:\n",
    "    inputs[\"protein_id\"] = protein_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b482d280-24d7-42af-b463-7cdcdfb361f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.002318 s\n",
       "File: /home/johanna/ICB/multibind/multibind/models/models.py\n",
       "Function: forward at line 81\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    81                                               def forward(self, mono, **kwargs):\n",
       "    82                                                   # mono_rev=None, di=None, di_rev=None, batch=None, countsum=None, residues=None, protein_id=None):\n",
       "    83         1          1.0      1.0      0.0          mono_rev = kwargs.get(\"mono_rev\", None)\n",
       "    84         1          0.0      0.0      0.0          di = kwargs.get(\"di\", None)\n",
       "    85         1          0.0      0.0      0.0          di_rev = kwargs.get(\"di_rev\", None)\n",
       "    86         1        165.0    165.0      7.1          mono = self.padding(mono)\n",
       "    87         1          1.0      1.0      0.0          if mono_rev is None:\n",
       "    88         1        175.0    175.0      7.5              mono_rev = mb.tl.mono2revmono(mono)\n",
       "    89                                                   else:\n",
       "    90                                                       mono_rev = self.padding(mono_rev)\n",
       "    91                                           \n",
       "    92                                                   # prepare the dinucleotide objects if we need them\n",
       "    93         1          2.0      2.0      0.1          if self.use_dinuc:\n",
       "    94                                                       if di is None:\n",
       "    95                                                           di = mb.tl.mono2dinuc(mono)\n",
       "    96                                                       if di_rev is None:\n",
       "    97                                                           di_rev = mb.tl.mono2dinuc(mono_rev)\n",
       "    98                                                       di = torch.unsqueeze(di, 1)\n",
       "    99                                                       di_rev = torch.unsqueeze(di_rev, 1)\n",
       "   100                                                       kwargs[\"di\"] = di\n",
       "   101                                                       kwargs[\"di_rev\"] = di_rev\n",
       "   102                                           \n",
       "   103                                                   # unsqueeze mono after preparing di and unsqueezing mono\n",
       "   104         1         18.0     18.0      0.8          mono_rev = torch.unsqueeze(mono_rev, 1)\n",
       "   105         1          2.0      2.0      0.1          mono = torch.unsqueeze(mono, 1)\n",
       "   106                                           \n",
       "   107                                                   # binding_per_mode: matrix of size [batchsize, number of binding modes]\n",
       "   108         1       1815.0   1815.0     78.3          binding_per_mode = self.binding_modes(mono=mono, mono_rev=mono_rev, **kwargs)\n",
       "   109         1        138.0    138.0      6.0          binding_scores = self.activities(binding_per_mode, **kwargs)\n",
       "   110                                           \n",
       "   111         1          1.0      1.0      0.0          if self.datatype == \"pbm\":\n",
       "   112         1          0.0      0.0      0.0              return binding_scores\n",
       "   113                                                   elif self.datatype == \"selex\":\n",
       "   114                                                       return self.selex_module(binding_scores, **kwargs)\n",
       "   115                                                   else:\n",
       "   116                                                       return None  # this line should never be called"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f model.forward model(**inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (multibind)",
   "language": "python",
   "name": "multibind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
